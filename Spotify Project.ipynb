{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c889fea5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c889fea5",
        "outputId": "a5b99c50-010f-476b-f882-b42035fcdfe4"
      },
      "outputs": [],
      "source": [
        "!pip install spotipy\n",
        "!pip install lyricsgenius\n",
        "!pip install langdetect\n",
        "!pip install pyLDAvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c4026ff",
      "metadata": {
        "id": "4c4026ff"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import spotipy \n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from spotipy.oauth2 import SpotifyClientCredentials\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import time\n",
        "from lyricsgenius import Genius\n",
        "from langdetect import detect\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.corpora import MmCorpus\n",
        "from gensim.models import LdaModel\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "from collections import Counter\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier\n",
        "from sklearn.linear_model import LinearRegression, Lasso, LogisticRegression\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from itertools import cycle\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a50779ec",
      "metadata": {
        "id": "a50779ec"
      },
      "source": [
        "# Data Collection"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42df167f",
      "metadata": {
        "id": "42df167f"
      },
      "source": [
        "The list of playlists to extract songs from has been defined.To obtain a relatively balanced dataset, it was tried to gather more and less popular songs. To do so, many different playlists were included. An example is the \"Top 50 Global\", which surely contains popular songs, and the longest playlist available, which surely contains less popular songs. The list of playlists is the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e400399",
      "metadata": {
        "id": "7e400399"
      },
      "outputs": [],
      "source": [
        "playlist_list = ['1q2ex5xXsRqgHZOAxIchOX','3dgpO6mDWzdpMhyttrVi9t','3etgFObeCKF3HqXEnOydIJ','1zAEw65yCDn5c1Tfm6bo2h','37i9dQZEVXbNG2KDcFcKOF', '37i9dQZF1DX9XIFQuFvzM4', '37i9dQZF1DX5Ejj0EkURtP', '37i9dQZF1DX1lVhptIYRda', '37i9dQZF1DX10zKzsJ2jva', '37i9dQZF1DWU8quswnFt3c', '37i9dQZF1DX873GaRGUmPl', '37i9dQZF1DX9XIFQuFvzM4', '37i9dQZF1DWYV7OOaGhoH0', '37i9dQZF1DWTcqUzwhNmKv', '37i9dQZF1DXbITWG1ZJKYt','6yPiKpy7evrwvZodByKvM9']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qrFfqybdeTy1",
      "metadata": {
        "id": "qrFfqybdeTy1"
      },
      "source": [
        "A function was then defined so as to be able to extract information concerning the songs included in the playlists. For the sake of this analysis, it was necessary to extract both basic informations about the song (e.g., title, album title, all artists involved...) and audio features (e.g., danceability, energy, loudness...). The function, after having performed again the authentication with the Spotify API,loops through all the playlists. Inside this loop, it identifies the total number of tracks in the playlist and then loops through each batch of tracks (up to 100 tracks per batch). Then, for each track in the identified results, it exctracts all the relevant informations for the analysis at hand. After having identified all the characteristics required for the analysis for each song, the function returns a dataframe containing all the information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fpgwiTKkne6U",
      "metadata": {
        "id": "fpgwiTKkne6U"
      },
      "outputs": [],
      "source": [
        "def get_songs_df(playlists):\n",
        "    client_credentials_manager = SpotifyClientCredentials(client_id = \"93dc1d8b704a44bbb7838960cab9e3f6\", client_secret=\"78ac04b521374940b4f1a7568ef440ac\")\n",
        "    sp = spotipy.Spotify(client_credentials_manager = client_credentials_manager)\n",
        "    song_list = []\n",
        "    i=0\n",
        "    for playlist in playlists:\n",
        "        results = sp.playlist_tracks(playlist)\n",
        "        total_tracks = results['total']\n",
        "        for offset in range(0, total_tracks, 100):\n",
        "            results = sp.playlist_tracks(playlist, offset=offset)\n",
        "            for item in results['items']:\n",
        "                artist_name = []\n",
        "                artist_id = []\n",
        "                try:\n",
        "                  print(i)  #print statement just to see the progress when runnning the code\n",
        "                  i+=1\n",
        "                  # Get the track information\n",
        "                  track = item['track']\n",
        "                  track_id = track['id']\n",
        "                  track_title = track['name']\n",
        "                  album_name = track[\"album\"][\"name\"]\n",
        "                  album_id = track[\"album\"][\"id\"]\n",
        "                  for artist in track['artists']:\n",
        "                    artist_name.append(artist['name'])\n",
        "                    artist_id.append(artist['id'])\n",
        "                  release_date = track.get('album', {}).get('release_date', '')\n",
        "                  popularity = track.get('popularity', '')\n",
        "\n",
        "                # Get the audio features of the track\n",
        "                  audio_features = sp.audio_features(track_id)[0]\n",
        "\n",
        "                # Create a dictionary with the track information\n",
        "                  song_dict = {'id': track_id,\n",
        "                              'title': track_title,\n",
        "                              'album_name': album_name,\n",
        "                              'all_artists': artist_name,\n",
        "                              'album_id': album_id,\n",
        "                              'artist_id': artist_id,\n",
        "                              'popularity': popularity,\n",
        "                              'release_date': release_date,\n",
        "                              'danceability': audio_features['danceability'],\n",
        "                              'energy': audio_features['energy'],\n",
        "                              'key': audio_features['key'],\n",
        "                              'loudness': audio_features['loudness'],\n",
        "                              'acousticness': audio_features['acousticness'],\n",
        "                              'instrumentalness': audio_features['instrumentalness'],\n",
        "                              'liveness': audio_features['liveness'],\n",
        "                              'valence': audio_features['valence'],\n",
        "                              'tempo': audio_features['tempo'],\n",
        "                              'duration_ms': audio_features['duration_ms'],\n",
        "                              'speechiness': audio_features['speechiness']}\n",
        "                except:\n",
        "                  continue\n",
        "                song_list.append(song_dict)\n",
        "    song_df = pd.DataFrame(song_list)\n",
        "    song_df.reset_index(drop=True, inplace=True)\n",
        "    return song_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "630218ae",
      "metadata": {
        "id": "630218ae"
      },
      "source": [
        "The function was than called to actually create the dataframes with all information concerning the songs in the playlists:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "r2cZmXWwnwsG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2cZmXWwnwsG",
        "outputId": "44a9ab66-aa00-49af-d676-05db384f6780"
      },
      "outputs": [],
      "source": [
        "songs_df = get_songs_df(playlist_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77a64f6c",
      "metadata": {
        "id": "77a64f6c"
      },
      "source": [
        "The record label that published the song was considered an additional useful information to determine song popolarity. Indeed, the most famous and important record labels likely have more economical resources to sponsor their artists and also better agreements with the overall music industry. Because of this, a function to extract the record label of the song has been defined in the following chunck of code. In particular, this function loops through all album_ids and identifies, where possible, the name of the record label that has published it. The function returns a dictionary containing the album_id as key and the record label name as value associated to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hYQmKB2sBxh9",
      "metadata": {
        "id": "hYQmKB2sBxh9"
      },
      "outputs": [],
      "source": [
        "def get_record_labels_from_spotify(album_ids, access_token = access_token):\n",
        "    headers = {\n",
        "        'Authorization': f'Bearer {access_token}'\n",
        "    }\n",
        "    record_labels = {}\n",
        "    i=0\n",
        "    for album_id in album_ids:\n",
        "        print(i)  #print statement just to see the progress when runnning the code\n",
        "        i+=1\n",
        "        url = f'https://api.spotify.com/v1/albums/{album_id}'\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            album_info = response.json()\n",
        "            if 'label' in album_info:\n",
        "                record_labels[album_id] = album_info['label']\n",
        "                print(album_info['label'])\n",
        "\n",
        "    return record_labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7a6ffea",
      "metadata": {
        "id": "c7a6ffea"
      },
      "source": [
        "Then, all necessary parameters to call the function have been defined and the function has been called to preduce the final dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RQb14N7BQg6o",
      "metadata": {
        "id": "RQb14N7BQg6o"
      },
      "outputs": [],
      "source": [
        "auth_url = 'https://accounts.spotify.com/api/token'\n",
        "\n",
        "client_id = \"93dc1d8b704a44bbb7838960cab9e3f6\"\n",
        "client_secret=\"78ac04b521374940b4f1a7568ef440ac\"\n",
        "data = {\n",
        "    'grant_type': 'client_credentials',\n",
        "    'client_id': client_id,\n",
        "    'client_secret': client_secret,\n",
        "}\n",
        "client_id = \"93dc1d8b704a44bbb7838960cab9e3f6\"\n",
        "client_secret=\"78ac04b521374940b4f1a7568ef440ac\"\n",
        "auth_response = requests.post(auth_url, data=data)\n",
        "access_token = auth_response.json().get('access_token')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Mu06GufZp5xu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mu06GufZp5xu",
        "outputId": "26f26921-0962-4cca-b808-e1bfcf353c07"
      },
      "outputs": [],
      "source": [
        "record_labels= get_record_labels_from_spotify(songs_df.album_id.to_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b3a74a8",
      "metadata": {
        "id": "3b3a74a8"
      },
      "source": [
        "At this point, it was still necessary to extract the information concerning the artists. Indeed, factors such as the number of followers of the artist, its popularity and the main genres might be very relevant in determining the final popularity of the song. To this end the sollowing function was defined. It mirrors a lot the initial function to get the dataframe of the songs, but it loops through all the unique ids of the artists in the playlists and extracts the relevant information (number of followers, genres, and popularity). The function returns a dataframe containing all the information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vysZKpE1vImL",
      "metadata": {
        "id": "vysZKpE1vImL"
      },
      "outputs": [],
      "source": [
        "def get_artist_info_from_songs_df(songs_df):\n",
        "    artist_ids = songs_df[\"artist_id\"].explode().unique()\n",
        "    client_credentials_manager = SpotifyClientCredentials(client_id= \"2d21ba072f2a428684313e9a35ca77e3\", client_secret= \"70d24fa4786f40a0a65be9df06986cd9\")\n",
        "    sp = spotipy.Spotify(client_credentials_manager = client_credentials_manager)\n",
        "    artist_list = []\n",
        "    i=0\n",
        "    for artist_id in artist_ids:\n",
        "        i+=1\n",
        "        print(i) #print statement just to see the progress when runnning the code\n",
        "        artist_info = sp.artist(artist_id)\n",
        "        followers = artist_info.get('followers', {}).get('total', '')\n",
        "        genres = ', '.join(artist_info.get('genres', []))\n",
        "        popularity = artist_info.get('popularity', '')\n",
        "        artist_dict = {'id': artist_id,\n",
        "                       'followers': followers,\n",
        "                       'genres': genres,\n",
        "                       'popularity': popularity}\n",
        "        artist_list.append(artist_dict)\n",
        "    artist_df = pd.DataFrame(artist_list)\n",
        "    artist_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    return artist_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7f41dfa",
      "metadata": {
        "id": "b7f41dfa"
      },
      "source": [
        "Then, the dataframes obtained were saved locally so as to have them safely stored and to avoid having to call the Spotify API again. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ef9ae40",
      "metadata": {
        "id": "8ef9ae40"
      },
      "outputs": [],
      "source": [
        "songs_df.to_csv(\"songs_new4.csv\", index = False)\n",
        "artists_df.to_csv(\"artists_new2.csv\", index = False)\n",
        "record_labels = pd.DataFrame(record_labels.items())\n",
        "record_labels.rename(columns={0: \"album_id\", 1: \"record_label\"}, inplace=True)\n",
        "record_labels.to_csv(\"record_labels1.csv\", index = False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e861864",
      "metadata": {
        "id": "9e861864"
      },
      "source": [
        "# Additional data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fd1504a",
      "metadata": {
        "id": "8fd1504a"
      },
      "source": [
        "At this point, it was necessary to merge the dataframes containing the information about the songs, the record labels and the artists. An import of the above saved dataframes has been included so that the code can be run without having to call the Spotify API again. The following code merges the dataframes (by performing a left join, so that the songs dataframe will have all its entries included, on the album_id column which is common between all dataframes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nszjjoqi0ipR",
      "metadata": {
        "id": "nszjjoqi0ipR"
      },
      "outputs": [],
      "source": [
        "songs_df = pd.read_csv(\"songs_new4.csv\").replace('\\'','', regex=True)\n",
        "artist_df = pd.read_csv(\"artists_new2.csv\").replace('\\'','', regex=True) \n",
        "labels_df = pd.read_csv(\"record_labels1.csv\").replace('\\'','', regex=True) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q0dAWzefM2mO",
      "metadata": {
        "id": "q0dAWzefM2mO"
      },
      "outputs": [],
      "source": [
        "merged1 = pd.merge(songs_df, labels_df, how=\"left\",on=\"album_id\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dXV5zKRVZ-yX",
      "metadata": {
        "id": "dXV5zKRVZ-yX"
      },
      "outputs": [],
      "source": [
        "songs_df = merged1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be6a131d",
      "metadata": {
        "id": "be6a131d"
      },
      "source": [
        "Given that several different playlists were considered, it might be the case that there are some repeated songs (as they might be present in more than one playlist). To avoid data duplication the followinng line of code drops all duplicates (if any):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "unFW4NOArQct",
      "metadata": {
        "id": "unFW4NOArQct"
      },
      "outputs": [],
      "source": [
        "songs_df = songs_df.drop_duplicates(subset = ['title', 'artist_id'],keep = 'last').reset_index(drop = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dde67e80",
      "metadata": {
        "id": "dde67e80"
      },
      "outputs": [],
      "source": [
        "songs_df.to_csv(\"final.csv\", index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NKa-HMcCrxD2",
      "metadata": {
        "id": "NKa-HMcCrxD2"
      },
      "source": [
        "### Lyrics extraction for topic modelling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26861bfd",
      "metadata": {
        "id": "26861bfd"
      },
      "source": [
        "At this point, an additional step was to extract the lyrics of the songs. In this way, an attempt to run some topic modelling to understand the main topics of a song could be run. Indeed, a factor influencing the popularity of the song could be the arguments treated in its lyrics. \\\n",
        "To be able to extract the lyrics of the songs, the Genius API was used (Genius provides the lyrics for songs on Spotify)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Bd0zPimTtPcv",
      "metadata": {
        "id": "Bd0zPimTtPcv"
      },
      "outputs": [],
      "source": [
        "genius = Genius(\"OzYrhuvvGAOIEH01wADfiCj-B6j_4m-3cAgmFVMmrXAycHoz0JWbXc9MBzDSmvSA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7808f61",
      "metadata": {
        "id": "a7808f61"
      },
      "source": [
        "A loop over all songs was then included so as to save the lyrics, where present, of the songs in a list, otherwise a NaN value was saved. \\\n",
        "Then starting from the list of lyrics, a dataframe was created containing the lyrics of the songs. This dataframe, as previously, was saved locally so as to avoid having to call the Genius API again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "au11hTozJP6K",
      "metadata": {
        "id": "au11hTozJP6K",
        "outputId": "2fbe13ee-1e22-4486-dc8b-5cdc123ffad1"
      },
      "outputs": [],
      "source": [
        "l=[]\n",
        "for i in range(len(songs_df)):\n",
        "    try:\n",
        "    # Search for the song lyrics using the Genius API\n",
        "        song = genius.search_song(songs_df[\"title\"][i], songs_df[\"all_artists\"][i][2:-2])\n",
        "        l.append(song.lyrics) \n",
        "    except:\n",
        "        l.append(None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4wBnMf2lJXmD",
      "metadata": {
        "id": "4wBnMf2lJXmD"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aErrApzWJXwb",
      "metadata": {
        "id": "aErrApzWJXwb"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"lyrics1.csv\", index = False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "063f18df",
      "metadata": {
        "id": "063f18df"
      },
      "source": [
        "The lyrics dataframe has then be imported and merged with songs dataframe so as to have all the information in a single dataframe and have all songs matched with their lyrics. Clearly, this operation reduced the dimension of the dataset as the lyrics were not available for all songs through the Genius API, though, for the sake of the topic modelling analysis, a suffiecient number of songs (both popular and not popular) remained available (indeed lyrics were available for almost two thirds of the songs)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QvzARRwWsamn",
      "metadata": {
        "id": "QvzARRwWsamn"
      },
      "outputs": [],
      "source": [
        "lyrics_df = pd.read_csv(\"lyrics1.csv\").replace('\\'','', regex=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M4PLxi5wxdR6",
      "metadata": {
        "id": "M4PLxi5wxdR6"
      },
      "outputs": [],
      "source": [
        "lyrics_df.drop(\"Unnamed: 0\", inplace=True, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "whQYaEeDXQbk",
      "metadata": {
        "id": "whQYaEeDXQbk"
      },
      "outputs": [],
      "source": [
        "merged2 = songs_df.join(lyrics_df)\n",
        "merged2.rename(columns={\"0\": \"lyrics\"}, inplace = True)\n",
        "merged2.dropna(axis=0, subset=['record_label', 'lyrics'], inplace = True)\n",
        "songs_df = merged2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3d3191b4",
      "metadata": {},
      "source": [
        "Once again the dataframe as been saved so as not to have to repeat this steps again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13f8f921",
      "metadata": {
        "id": "13f8f921"
      },
      "outputs": [],
      "source": [
        "songs_df.to_csv(\"train_df_final4.csv\", index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V4ZcS-Mxl1HX",
      "metadata": {
        "id": "V4ZcS-Mxl1HX"
      },
      "source": [
        "# Topic modeling\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "2fd178b7",
      "metadata": {},
      "source": [
        "Now, the complete dataframe containing the lyrics and the information about the songs was used to perform some topic modelling. In particular, the aim was to understand if there are some topics that are more likely to be present in popular songs than in not popular songs. To this end, the Latent Dirichlet Allocation (LDA) algorithm was used. But before applying it, the language of the songs was detected through the detect function of the langdetect library. This was done because texts in different languages can risk to confound the algorithm and different preprocessing procedures should be applied for each different language. Given that the greatest part of the songs included in the dataframe are in English (as it can be seen from the value counts), it was deemed appropriate and reasonable to continue the analysis only with English songs with the idea of pursuing the analysis also of the other languages in a further analysis if the topic was found of relevance in the one on English songs. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2687af7a",
      "metadata": {
        "id": "2687af7a"
      },
      "outputs": [],
      "source": [
        "train_df1 = pd.read_csv(\"train_df_final4.csv\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "40274ab2",
      "metadata": {},
      "source": [
        "The function to detect the language was defined and applied to the lyrics of the songs. The dataframe was then filtered to keep only the songs in English. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "O7lswD_7oPjr",
      "metadata": {
        "id": "O7lswD_7oPjr"
      },
      "outputs": [],
      "source": [
        "def detect_lang(text):\n",
        "    return detect(text)\n",
        "\n",
        "train_df1['lang'] = train_df1['lyrics'].apply(detect_lang)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IyLTtETqoIk_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyLTtETqoIk_",
        "outputId": "af523052-cc57-4213-ed6c-fabae35c28e7"
      },
      "outputs": [],
      "source": [
        "train_df1['lang'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fuyix7uEo9sq",
      "metadata": {
        "id": "fuyix7uEo9sq"
      },
      "outputs": [],
      "source": [
        "train_df1 = train_df1[train_df1['lang'] == 'en']\n",
        "train_df1.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "afeb464c",
      "metadata": {},
      "source": [
        "Then, a preprocessing of the lyrics was carried out before applying the LDA algorithm. This operation is necessary to reduce the variance of the input texts. In particular, in the preprocessing function, the texts were, to begin with, tokenized. Then, the tokens were kept only if they were not numbers and longer than 2 characters. Moreover, all stopwords have been removed. Stopwords are all those words that are very common in a language, English in this case, and therefore are not considere information carrying words. It has to be noted that the standard set of stop words included in the Natural Language Toolkit library was augmented with some words specific to this analysis. In particular, when obtaining the lyrics from the Genius API, the name of teh artist as well as the section of the song (e.g., chorus, verse, hook, pre-chorus...) are included. Though, this words are not really part of the lyrics of the song, and is for this reason that it was necessary to drop them along with the stopwords. Moroever, also some common words such as \"record\" or \"song\" were removed as they are not really informative.Finally, the surviving tokens have been lemmatized, which means that they have been reduced to their dictionary equivalent (e.g., was becomes be). \\\n",
        "The function than returns a list containing all the lemmatized tokens of the lyrics of the songs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b57cb23",
      "metadata": {
        "id": "1b57cb23",
        "outputId": "979a78f4-69d6-437c-fce6-d0273db2efa7"
      },
      "outputs": [],
      "source": [
        "stop_words = stopwords.words('english')\n",
        "new_stop_words = stop_words+['ooh','yeah','hey','whoa','woah', 'ohh', 'was', 'mmm', 'oooh','yah','yeh','mmm', 'hmm','deh','doh','jah','wa']+[\"chorus\", \"verse\", \"post-chorus\", \"hook\", \"pre-chorus\", \"lyrics\", \"bridge\", \"intro\",\"interlude\",\"remaster\",\"feat\",\"ft.\",\"recording\",\"song\",\"record\",\"records\"]\n",
        "aux=[]\n",
        "for i in range(len(train_df1)):\n",
        "    aux+=[j.lower() for j in word_tokenize(train_df1['all_artists'].iloc[i]) if j not in [\"[\",\",\",\"]\"]]\n",
        "names=list(set(aux))\n",
        "final_stop_words=new_stop_words+names\n",
        "def preprocessing(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [tok for tok in tokens if tok not in final_stop_words and tok.isalpha() and len(tok)>2]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(tok) for tok in tokens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13dec9e5",
      "metadata": {
        "id": "13dec9e5"
      },
      "outputs": [],
      "source": [
        "train_df1['tokenized_lyrics'] = train_df1.apply(lambda row: preprocessing(row['lyrics']), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cJ0Cax5qzN2",
      "metadata": {
        "id": "8cJ0Cax5qzN2"
      },
      "outputs": [],
      "source": [
        "tokenized_lyrics = train_df1['tokenized_lyrics'].to_list()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "db9f61c4",
      "metadata": {},
      "source": [
        "To then be able to apply the LDA algorithm, it was neessary to create a dictionary containing all the tokenized lyrics. This dictionary was also filtered so that only the words that appeared at least 8 times in all the lyrics were kept. This was done to avoid having words that appeared only once or twice in the lyrics and that would not be very informative for a topic modelling task. Moreover, also an upper bound was set (equal to 70%) so as to exclude all those words that appear in more than 70% of the lyrics. This was done to avoid having words that are too common and that, hence, would not be very informative, once again, for a topic modelling task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Sgftq4a39fh-",
      "metadata": {
        "id": "Sgftq4a39fh-"
      },
      "outputs": [],
      "source": [
        "dictionary = Dictionary(tokenized_lyrics)\n",
        "dictionary.filter_extremes(no_below = 8, no_above = 0.7)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "9ac2d381",
      "metadata": {},
      "source": [
        "A Bag-of-Words was then created from the dictionary. This procedure associates each word with the number of times that the word appears in the lyrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40F4de5S9sls",
      "metadata": {
        "id": "40F4de5S9sls"
      },
      "outputs": [],
      "source": [
        "gensim_corpus = [dictionary.doc2bow(song) for song in tokenized_lyrics]\n",
        "temp = dictionary[0]\n",
        "id2word = dictionary.id2token"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cef11c66",
      "metadata": {},
      "source": [
        "The necessary parameters were then defined (to identify the final set of parameters a trial and error procedure was conducted strating from the default parameters)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kbzLJhY49zIf",
      "metadata": {
        "id": "kbzLJhY49zIf"
      },
      "outputs": [],
      "source": [
        "chunksize = 2000\n",
        "passes = 20\n",
        "iterations = 400\n",
        "num_topics = 12"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "caf754d2",
      "metadata": {},
      "source": [
        "Then, a function to find the most appropriate number of topics was defined. This function loops through a range of number of topics and for each number of topics it runs the LDA algorithm and calculates the coherence score. The coherence score is a measure of how good the topics are. The higher the coherence score, the better the topics are. The fucntion was then applied to identify the most appropriate numebr of topics between 1 and 11 (which are prettu standard values for the number of topics in a topic modelling analysis). For easyness of evaluation, results have been plotted and the number of topics that maximizes the coherence score has been identified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ae5c710",
      "metadata": {
        "id": "9ae5c710"
      },
      "outputs": [],
      "source": [
        "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        lda_model=LdaModel(corpus=gensim_corpus, id2word=id2word, chunksize=chunksize, alpha='auto', eta='auto', iterations=iterations, num_topics=num_topics, passes=passes)\n",
        "        model_list.append(lda_model)\n",
        "        coherencemodel = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "\n",
        "    return model_list, coherence_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1124c510",
      "metadata": {
        "id": "1124c510",
        "outputId": "335f46e0-f3d8-4dbe-eb26-3c5432abf6fc"
      },
      "outputs": [],
      "source": [
        "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=gensim_corpus, texts=tokenized_lyrics, start=2, limit=12, step=1)\n",
        "\n",
        "limit=12\n",
        "start=2\n",
        "step=1\n",
        "x = range(start, limit, step)\n",
        "plt.plot(x, coherence_values)\n",
        "plt.xlabel(\"Num Topics\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.legend((\"coherence_values\"), loc='best')\n",
        "plt.show()\n",
        "\n",
        "for m, cv in zip(x, coherence_values):\n",
        "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "95896453",
      "metadata": {},
      "source": [
        "As it can be seen, the number of topics maximizing the coherence score seems to be either 5 or 8. In the sollowing, the LDA algorithm was run so as to identify 8 topics. An attempt was also done with 5 topics but results were less precise and so, for this reason, the final algorithms includes 8 topics overall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3fba690",
      "metadata": {
        "id": "f3fba690"
      },
      "outputs": [],
      "source": [
        "num_topics=8\n",
        "lda_model = LdaModel(\n",
        "corpus=gensim_corpus,\n",
        "id2word=id2word,\n",
        "chunksize=chunksize,\n",
        "alpha='auto',\n",
        "eta='auto',\n",
        "iterations=iterations,\n",
        "num_topics=num_topics,\n",
        "passes=passes\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ace0e0d9",
      "metadata": {},
      "source": [
        "The results have then been plotted so as to be able to visualize results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "859a05a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "vis_data = gensimvis.prepare(lda_model, gensim_corpus, dictionary)\n",
        "pyLDAvis.display(vis_data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "0fd64e57",
      "metadata": {
        "id": "0fd64e57"
      },
      "source": [
        "From the above, the following topics were identified:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "0a47886b",
      "metadata": {},
      "source": [
        "1: \"People and Thoughts\" (e.g., person, face, thought, moment, word)\\\n",
        "2: \"Legal\" (e.g., law, court, act, case, labour, part, right)\\\n",
        "3: \"States/Feelings\" (e.g., feel, well, ill, need)\\\n",
        "4: \"Profanity and Violence\" (e.g., sh\\*t, b\\*tch, f\\*cking, mean, battle)\\\n",
        "5: \"Body and Senses\" (e.g., arm, foot, hand, look, face, watch, stand, voice)\\\n",
        "6: \"Months and Food\" (e.g., january, march, debruary, water, tea, bread)\\\n",
        "7: \"People, Nature and the Unknown\" (e.g., human, brain, memory, tree, forest, pet, spider, bug, secret, shadow, space, box, ancient, anomaly, nightmare)\\\n",
        "8: \"Music and Rappers\" (e.g., artist, vol, acoustic, creator, remix, playboi, carti, nudy, yachty, ski, styrke)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "61905ea9",
      "metadata": {},
      "source": [
        "Now, to asses whether there was a difference or not between the topics of popular and non-popular songs, the most relevant topic for each song was calculated in the following chuncks of code. In particular, the most relevant topic for each song was calculated as the topic with the highest probability. (The corpus has been redefined just for extra safety, though it is not a mandatory step)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fd0a73f",
      "metadata": {
        "id": "7fd0a73f"
      },
      "outputs": [],
      "source": [
        "corpus = [dictionary.doc2bow(text) for text in tokenized_lyrics]\n",
        "doc_topics = lda_model[corpus]\n",
        "topics = [max(doc_topic, key=lambda x: x[1])[0]+1 for doc_topic in doc_topics]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e6145e9",
      "metadata": {
        "id": "1e6145e9",
        "outputId": "ea84860a-002c-46b9-e99e-38159cbee70e"
      },
      "outputs": [],
      "source": [
        "Counter(topics)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e18f5647",
      "metadata": {},
      "source": [
        "As it can be seen from the counter above, most of the songs belong, for their greatest part, to topic 7. Which makes sense since is the one about persons, nature and the unknown, which are very common subjects of songs worldwide."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "6ea981f2",
      "metadata": {},
      "source": [
        "Then, the distribution of topics was plotted for both popular and non popular songs. The threshold to distinguish popular and non popular songs was set to be 46% in popularity of the songs. This value was selected as it is approximately the mean of the popularity of the songs in the dataset excluding all the songs that have 0 as popularity. It was deemed appropriate to exclude the songs with zero popularity from the computation of the mean because of their great number and hence their great influence on the mean, which biases the results towards zero. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6641295",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df1[train_df1['popularity']>0][\"popularity\"].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26abfbf8",
      "metadata": {},
      "outputs": [],
      "source": [
        "indeces = train_df1[train_df1[\"popularity\"]>46].index\n",
        "a=[]\n",
        "for i in indeces:\n",
        "    a.append(topics[i])\n",
        "sns.histplot(a,color=\"green\",kde=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89e80150",
      "metadata": {},
      "outputs": [],
      "source": [
        "indeces = train_df1[train_df1[\"popularity\"]<46].index\n",
        "a=[]\n",
        "for i in indeces:\n",
        "    a.append(topics[i])\n",
        "sns.histplot(a,color=\"green\",kde=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4aeee9e8",
      "metadata": {},
      "source": [
        "As it can be seen form the plotetd histograms, there seems not to be any relevant difference. Indeed, the two distributions seem approximately the same. it is for this reason that it was decided to not pursue the analysis of the topic modelling for popular and non popular songs and hence not to include the results in the final dataframe. Given that the final goal is predicting whether a song will be popular or not based on previous evidence, there is no point in including an extra feature if this does not generate a distinction between the two groups. Given that this result was obtained for English lyrics, which represents the vast majority of the dataset, there was also no point in trying to perform the same analysis for the other languages, which likely would have not yielded different results. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "1617af1c",
      "metadata": {},
      "source": [
        "# Sentiment Analysis"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "72a95ed6",
      "metadata": {},
      "source": [
        "Another attempt to discover significant differences between popular and non popular songs was to perform sentiment analysis on the lyrics. Indeed, the overall mood of the song can considered to be a good candidate to drive the distinction between popular and unpopular songs. Once again, only lyrics in English have been considered given that sentiment analysis algorithms/transformes are often language specific. As of before, the idea was to run sentiment analysis on songs in English (which are the vast majority of the songs in the dataset) and then, if a significant distiction was to be discovered, also perform the same steps on other languages.\\\n",
        "In order to conduct Sentiment Analysis, a first attempt was to use SentimentIntensityAnalyzer from the Nltk library. This is a rather simple pretrained sentiment analysis algorithm that returns a score for each of the three main sentiments (positive, negative and neutral) and a compound score that is the sum of the three scores. In particular, this algorithm was used to identify these scores, then the functions defined below, also creates a new column in the dataframe corresponding to the dominant sentiment. \\\n",
        "The results were then plotted in a piechart to be able to visualize the distribution of the sentiment scores across popular and non popular songs. The threshold to distinguish popular and non popular songs was, as for topic modelling, set to be 46%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46c17346",
      "metadata": {
        "id": "46c17346"
      },
      "outputs": [],
      "source": [
        "def setimentanalyzer(df):\n",
        "    neg='Negative'\n",
        "    neu='Neutral'\n",
        "    pos='Positive'\n",
        "    negative = []\n",
        "    neutral = []\n",
        "    positive = []\n",
        "    dominant_sentiment=[]\n",
        "    dominant_sentiment_score=[]\n",
        "    sid = SentimentIntensityAnalyzer()\n",
        "    for i in df.index:\n",
        "       \n",
        "        scores = sid.polarity_scores(' '.join(df['tokenized_lyrics'].iloc[i]))\n",
        "        negative.append(scores['neg'])\n",
        "        neutral.append(scores['neu'])\n",
        "        positive.append(scores['pos'])\n",
        "        if scores['neg']>scores['pos'] and scores['neg']>0.3:\n",
        "            dominant_sentiment_score.append(scores['neg'])\n",
        "            dominant_sentiment.append(neg)\n",
        "        elif scores['neg']<scores['pos'] and scores['pos']>0.3:\n",
        "            dominant_sentiment_score.append(scores['pos'])\n",
        "            dominant_sentiment.append(pos)\n",
        "        else:\n",
        "            dominant_sentiment_score.append(scores['neu'])\n",
        "            dominant_sentiment.append(neu)\n",
        "    df['dominant_sentiment']=dominant_sentiment\n",
        "    return df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "bc6428cd",
      "metadata": {},
      "source": [
        "The function was then applied to the dataframe under analysis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77e8c320",
      "metadata": {
        "id": "77e8c320"
      },
      "outputs": [],
      "source": [
        "df_fin=setimentanalyzer(train_df1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "675d2995",
      "metadata": {},
      "source": [
        "And the piecharts were plotted starting from the one for popular songs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2931dc1b",
      "metadata": {
        "id": "2931dc1b",
        "outputId": "57f6344d-282d-4ae8-de48-af5a2522f710"
      },
      "outputs": [],
      "source": [
        "indeces = train_df1[train_df1[\"popularity\"]>46].index\n",
        "a=[]\n",
        "for i in indeces:\n",
        "    a.append(train_df1[\"dominant_sentiment\"].iloc[i])\n",
        "b=Counter(a)\n",
        "plt.pie(b.values(),labels=b.keys(),autopct='%1.1f%%', startangle=90,colors=[\"lightgreen\",\"darkgreen\",\"green\"],explode = (0.05,0.05,0.05))\n",
        "centre_circle = plt.Circle((0,0),0.70,fc='white')\n",
        "fig = plt.gcf()\n",
        "fig.gca().add_artist(centre_circle)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7028f58",
      "metadata": {
        "id": "b7028f58",
        "outputId": "2b401bd5-3a7b-4e6a-e527-1937c2cb50ad"
      },
      "outputs": [],
      "source": [
        "indeces = train_df1[train_df1[\"popularity\"]<46].index\n",
        "a_b=[]\n",
        "for i in indeces:\n",
        "    a_b.append(train_df1[\"dominant_sentiment\"].iloc[i])\n",
        "b_a=Counter(a_b)\n",
        "plt.pie(b_a.values(),labels=b_a.keys(),autopct='%1.1f%%', startangle=90,colors=[\"green\",\"lightgreen\",\"darkgreen\"],explode = (0.05,0.05,0.05))\n",
        "centre_circle = plt.Circle((0,0),0.70,fc='white')\n",
        "fig = plt.gcf()\n",
        "fig.gca().add_artist(centre_circle)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d36d43d7",
      "metadata": {},
      "source": [
        "As it can be seen from the graphs above, once again there does not seem to be any statistically significant difference among popular and unpopular songs when it comes to classifying them according to positive/nuetral/negative sentiment. Indeed, the percentages are almost the same both for popular and unpopular songs."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ab10457d",
      "metadata": {},
      "source": [
        "At this point, given that the SentimentIntensityAnalyzer pretrained model from Ntlk is quite simple and is most suited for short texts, it was deemed appropriate to try to re-run the analysis with a more complicated model. Thus, an attempt was made with the DistilBert pretrained model from HugginFace. This model is a pretrained model that has been trained on a large corpus of text and that can be used to perform a variety of NLP tasks. In particular, this model can be used to perform sentiment analysis. The model was loaded and then applied to the lyrics of the songs. As before, results were then stored in a new column of the dataframe. The results (0 for \"negative\" songs and 1 for \"positive\" songs) were, as in the case of SentimentIntensityAnalyzer, plotted in a piechart to be able to visualize the distribution of the sentiment scores across popular and non popular songs. The threshold to distinguish popular and non popular songs was, as for topic modelling and SentimentIntensityAnalyzer, set to be 46%.\\\n",
        "Notice that in the code below the tokens previously created for the topic modelling were rejoined into a single string so as then to apply the specific tokenizer for the DistilBert model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc45f03d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
        "\n",
        "def analyze_sentiment(song_lyrics):\n",
        "    inp = tokenizer(song_lyrics, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    out = model(**inp)\n",
        "    aux, predicted_label = torch.max(out[0], dim=1)\n",
        "    return predicted_label.item()\n",
        "\n",
        "sentiments = []\n",
        "for lyric in train_df1['tokenized_lyrics']:\n",
        "    song_lyrics = ' '.join(lyric)\n",
        "    sentiment_label = analyze_sentiment(song_lyrics)\n",
        "    sentiments.append(sentiment_label)\n",
        "\n",
        "train_df1['sentiment_label'] = sentiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cb152f7",
      "metadata": {},
      "outputs": [],
      "source": [
        "indeces = train_df1[train_df1[\"popularity\"]>46].index\n",
        "a=[]\n",
        "for i in indeces:\n",
        "    a.append(train_df1[\"sentiment_label\"].iloc[i])\n",
        "b=Counter(a)\n",
        "plt.pie(b.values(),labels=b.keys(),autopct='%1.1f%%', startangle=90,colors=[\"lightgreen\",\"darkgreen\"],explode = (0.05,0.05))\n",
        "centre_circle = plt.Circle((0,0),0.70,fc='white')\n",
        "fig = plt.gcf()\n",
        "fig.gca().add_artist(centre_circle)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0609aa15",
      "metadata": {},
      "outputs": [],
      "source": [
        "indeces = train_df1[train_df1[\"popularity\"]<46].index\n",
        "a_b=[]\n",
        "for i in indeces:\n",
        "    a_b.append(train_df1[\"sentiment_label\"].iloc[i])\n",
        "b_a=Counter(a_b)\n",
        "plt.pie(b_a.values(),labels=b_a.keys(),autopct='%1.1f%%', startangle=90,colors=[\"darkgreen\",\"lightgreen\"],explode = (0.05,0.05))\n",
        "centre_circle = plt.Circle((0,0),0.70,fc='white')\n",
        "fig = plt.gcf()\n",
        "fig.gca().add_artist(centre_circle)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d001064a",
      "metadata": {},
      "source": [
        "As it can be seen from the graphs above, once again there does not seem to be any statistically significant difference among popular and unpopular songs when it comes to classifying them according to positive/negative sentiment.\\\n",
        "It is for this reason that, following the same logic as the one used for topic modelling, it was decided not to include the sentiment analysis results in the final dataframe. As a matter of fact, the results of the sentiment analysis do not seem to be able to distinguish between popular and unpopular songs. Logically, this can also make sense. Indeed, very popular songs as well as non popular songs exist for all these three possible sentiments. Being that the distributions are equal, it would not be meaningful to use the sentiment analysis results to predict whether a song will be popular or not."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "65a149b5",
      "metadata": {
        "id": "65a149b5"
      },
      "source": [
        "# Actual Analysis"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "7002bfcc",
      "metadata": {
        "id": "7002bfcc"
      },
      "source": [
        "### Data pre-processing and feature engineering"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "348ca54e",
      "metadata": {},
      "source": [
        "At this point, given that the analysis of the lyrics did not yield any significant difference between popular and non popular songs, the analysis was focused on the other features of the dataset. In particular, to begin with, the old database containing all the songs in all the languages has been loaded:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bed8f010",
      "metadata": {
        "id": "bed8f010"
      },
      "outputs": [],
      "source": [
        "songs_df = pd.read_csv(\"final.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06fbf480",
      "metadata": {
        "id": "06fbf480"
      },
      "outputs": [],
      "source": [
        "songs_df.rename(columns={\"title\": \"name\", \"artist_id\": \"id_artists\"}, inplace= True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c9d993cf",
      "metadata": {},
      "source": [
        "Then, some dictiories were created. In particular, dictionaries matching artists woth their popularity, followers and genres were created:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0170a38",
      "metadata": {
        "id": "b0170a38"
      },
      "outputs": [],
      "source": [
        "artist_popularity_dic = dict(zip(artist_df[\"id\"], artist_df[\"popularity\"]))\n",
        "artist_followers_dic = dict(zip(artist_df[\"id\"], artist_df[\"followers\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "053e5e6f",
      "metadata": {
        "id": "053e5e6f"
      },
      "outputs": [],
      "source": [
        "genre_dic = defaultdict(list)\n",
        "id_genre_dic = dict()\n",
        "\n",
        "for i, row in artist_df.iterrows():\n",
        "    genre_list = str(row[\"genres\"]).replace(\"[\", \"\").replace(\"]\",\"\").replace(\"'\", \"\").split()\n",
        "    for entry in genre_list:\n",
        "        genre_dic[entry].append(row[\"popularity\"])\n",
        "    id_genre_dic[row[\"id\"]] = genre_list"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "7a8c232c",
      "metadata": {},
      "source": [
        "Lists containing the highest occuring genres as well as the most popular ones have also been created:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cbaa133",
      "metadata": {
        "id": "9cbaa133"
      },
      "outputs": [],
      "source": [
        "genre_counts = sorted([(p, len(genre_dic[p])) for p in genre_dic], key=lambda x:x[1], reverse=True)\n",
        "valid_genres = [g[0] for g in genre_counts[0:1000]]\n",
        "top_genres = [g[0] for g in genre_counts[0:200]]\n",
        "popularity_sorted = sorted([(p, np.mean(genre_dic[p])) for p in genre_dic if p in valid_genres], key=lambda x:x[1], reverse=True)\n",
        "popular_genres = [p[0] for p in popularity_sorted[0:200]]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "84d3e4fc",
      "metadata": {},
      "source": [
        "Since many of the songs are features or collaborations, it was deemed useful to compute the mean and max for popularity and followers of all artists involved in each song:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "492b3426",
      "metadata": {
        "id": "492b3426"
      },
      "outputs": [],
      "source": [
        "songs_df[\"artist_list\"] = songs_df[\"id_artists\"].map(lambda x: str(x).replace(\"[\", \"\").replace(\"]\",\"\").replace(\"'\", \"\").split())\n",
        "\n",
        "for i, row in songs_df.iterrows():\n",
        "    mod_string = row[\"artist_list\"]\n",
        "    if len(row[\"artist_list\"])>1:\n",
        "        l=[]\n",
        "        t=0\n",
        "        for element in row[\"artist_list\"]:\n",
        "            if t==len(row[\"artist_list\"])-1:\n",
        "                l.append(row[\"artist_list\"][len(row[\"artist_list\"])-1])\n",
        "                break\n",
        "            size = len(row[\"artist_list\"][t])\n",
        "            mod_string = element[:size - 1]\n",
        "            l.append(mod_string)\n",
        "            t+=1\n",
        "        songs_df.at[i,'artist_list'] = l"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "92a1c8c6",
      "metadata": {},
      "source": [
        "Two functions to calculate the popularity and the followers for each artist were then defined. These function will be then applied to the artist list column so as to obtain the required new features for the dataframe for further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f3cea1d",
      "metadata": {
        "id": "1f3cea1d"
      },
      "outputs": [],
      "source": [
        "def calc_popularity_for_artist(id_list, flag):\n",
        "    popularity_scores = list()\n",
        "    for entry in id_list:\n",
        "        if entry in artist_popularity_dic:\n",
        "            popularity_scores.append(artist_popularity_dic[entry])\n",
        "        else:\n",
        "            popularity_scores.append(0)\n",
        "    if flag == \"max\":\n",
        "        return max(popularity_scores)\n",
        "    else:\n",
        "        return np.mean(popularity_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "601a6271",
      "metadata": {
        "id": "601a6271"
      },
      "outputs": [],
      "source": [
        "def calc_followers_for_artist(id_list, flag):\n",
        "    follower_scores = list()\n",
        "    for entry in id_list:\n",
        "        if entry in artist_followers_dic:\n",
        "            follower_scores.append(artist_followers_dic[entry])\n",
        "        else:\n",
        "            follower_scores.append(0)\n",
        "    if flag == \"max\":\n",
        "        return max(follower_scores)\n",
        "    else:\n",
        "        return np.mean(follower_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce3a76a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "songs_df[\"feature_max_artist_popularity\"] = songs_df[\"artist_list\"].map(lambda x: calc_popularity_for_artist(x, flag=\"max\"))\n",
        "songs_df[\"feature_mean_artist_popularity\"] = songs_df[\"artist_list\"].map(lambda x: calc_popularity_for_artist(x, flag=\"mean\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69c42388",
      "metadata": {
        "id": "69c42388"
      },
      "outputs": [],
      "source": [
        "songs_df[\"feature_max_artist_followers\"] = songs_df[\"artist_list\"].map(lambda x: calc_followers_for_artist(x, flag=\"max\"))\n",
        "songs_df[\"feature_mean_artist_followers\"] = songs_df[\"artist_list\"].map(lambda x: calc_followers_for_artist(x, flag=\"mean\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d1f40684",
      "metadata": {},
      "source": [
        "A function was then created to map genres to each of the songs. In particular, the genres associated to all the artists involved in the song will be associated to each individual song. Moroever, some flags will be used. In particular, valid genres indicate genres applicable to the song, top genres are the ones occurring with highest frequency, and popular genres are the ones associated with the most popular songs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68098c01",
      "metadata": {
        "id": "68098c01"
      },
      "outputs": [],
      "source": [
        "def map_genres(id_list, flag):\n",
        "    genres = list()\n",
        "    for entry in id_list:\n",
        "        if entry in id_genre_dic:\n",
        "            if flag == \"valid\":\n",
        "                genres += [i for i in id_genre_dic[entry] if i in valid_genres]\n",
        "            elif flag == \"popular\":\n",
        "                genres += [i for i in id_genre_dic[entry] if i in popular_genres]\n",
        "            else:\n",
        "                genres += [i for i in id_genre_dic[entry] if i in top_genres]\n",
        "    genres = list(set(genres))\n",
        "    if len(genres) > 0:\n",
        "        return \"_\".join(sorted(genres))\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94191ebf",
      "metadata": {
        "id": "94191ebf"
      },
      "outputs": [],
      "source": [
        "songs_df[\"genre_flag_valid\"] = songs_df[\"artist_list\"].map(lambda x: map_genres(x, flag=\"valid\"))\n",
        "songs_df[\"genre_flag_top\"] = songs_df[\"artist_list\"].map(lambda x: map_genres(x, flag=\"top\"))\n",
        "songs_df[\"genre_flag_popular\"] = songs_df[\"artist_list\"].map(lambda x: map_genres(x, flag=\"popular\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e59e1500",
      "metadata": {},
      "source": [
        "Then, the id of the artists were stripped from the square brackets and then split:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e2da864",
      "metadata": {
        "id": "5e2da864"
      },
      "outputs": [],
      "source": [
        "songs_df['id_artists'] = songs_df['id_artists'].str.strip('[]').str.split(',')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e4d7b6fb",
      "metadata": {},
      "source": [
        "And a new dataframe was created with this new information:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b33b831",
      "metadata": {
        "id": "2b33b831"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(songs_df['id_artists'].values.tolist()).add_prefix('artist_')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a375c76e",
      "metadata": {
        "id": "a375c76e"
      },
      "outputs": [],
      "source": [
        "songs_df = pd.merge(songs_df, df['artist_0'], left_index=True, right_index=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "69c706ad",
      "metadata": {},
      "source": [
        "Then, the artists and songs dataframes were merged:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56bf2748",
      "metadata": {
        "id": "56bf2748"
      },
      "outputs": [],
      "source": [
        "train_df = songs_df.merge(artist_df,left_on=['artist_0'],right_on=['id'],how='inner')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "646ef32a",
      "metadata": {},
      "source": [
        "Then, the duration was transform from millisecond to minutes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8d05690",
      "metadata": {
        "id": "d8d05690"
      },
      "outputs": [],
      "source": [
        "train_df['duration_ms'] = train_df['duration_ms']/1000/60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49eacd17",
      "metadata": {
        "id": "49eacd17"
      },
      "outputs": [],
      "source": [
        "train_df.rename(columns={\"duration_ms\": \"duration\", \"popularity_x\": \"popularity_song\",\"popularity_y\":\"popularity_artist\"}, inplace=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c6a415fb",
      "metadata": {},
      "source": [
        "And the genres were split:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42bbdff4",
      "metadata": {
        "id": "42bbdff4"
      },
      "outputs": [],
      "source": [
        "train_df['genres'] = train_df['genres'].str.split(',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "754fb086",
      "metadata": {
        "id": "754fb086"
      },
      "outputs": [],
      "source": [
        "train_df['genres'] = train_df['genres'].apply(lambda d: d if isinstance(d, list) else [])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e8d6665c",
      "metadata": {},
      "source": [
        "And a new genres dataframe was created with this new information and then merged to the existing training dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd7a2886",
      "metadata": {
        "id": "bd7a2886"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(train_df['genres'].values.tolist()).add_prefix('genre_')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79a4204e",
      "metadata": {
        "id": "79a4204e"
      },
      "outputs": [],
      "source": [
        "train_df = pd.merge(train_df, df, left_index=True, right_index=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "2953464e",
      "metadata": {},
      "source": [
        "Then, it was deemed appropriate to create a dummy variable for the most important genres, clearly the dum,my would be 1 if the song is associated with the genre and 0 otherwise. The code used was the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfd8db6d",
      "metadata": {
        "id": "dfd8db6d"
      },
      "outputs": [],
      "source": [
        "cols=[]\n",
        "for i in range(15):\n",
        "    cols.append(f\"genre_{i}\")\n",
        "train_df[\"all\"] = train_df[cols].apply(lambda x: ','.join(x.dropna()), axis=1)\n",
        "train_df['all'] = train_df['all'].str.split(',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7209669f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7209669f",
        "outputId": "906e86ff-219c-4af6-bfb4-81f42f47350e"
      },
      "outputs": [],
      "source": [
        "genres = [\"pop\", \"r&b\", \"trap\", \"rap\", \"edm\", \"adult standards\", \"folk\", \"classical\", \"reggae\", \"jazz\", \"rock\", \"latin\", \"soul\"]\n",
        "train_df1 = train_df.copy()\n",
        "for g in genres:\n",
        "    train_df[g] = 0\n",
        "    i=0\n",
        "    for index,row in train_df1.iterrows():\n",
        "        print(i)\n",
        "        i+=1\n",
        "        t = False\n",
        "        nn=len(row[\"all\"])\n",
        "        end=19+nn\n",
        "        for col in train_df1.iloc[:,34:49]:\n",
        "            if g is not None and isinstance(row[col], str):\n",
        "                one = g.split()\n",
        "                two = row[col].split()\n",
        "                common = set(one).intersection(set(two))\n",
        "                unique = set(one).symmetric_difference(set(two))\n",
        "            if len(common)>0:\n",
        "                t = True\n",
        "        if t == True:\n",
        "            train_df1.loc[index, g ] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c007d97",
      "metadata": {
        "id": "1c007d97"
      },
      "outputs": [],
      "source": [
        "train_df1[[\"pop\", \"r&b\", \"trap\", \"rap\", \"edm\", \"adult standards\", \"folk\", \"classical\", \"reggae\", \"jazz\", \"rock\", \"latin\", \"soul\", 'feature_max_artist_popularity',\n",
        "       'feature_mean_artist_popularity', 'feature_max_artist_followers',\n",
        "       'feature_mean_artist_followers']] = train_df1[[\"pop\", \"r&b\", \"trap\", \"rap\", \"edm\", \"adult standards\", \"folk\", \"classical\", \"reggae\", \"jazz\", \"rock\", \"latin\", \"soul\", 'feature_max_artist_popularity',\n",
        "       'feature_mean_artist_popularity', 'feature_max_artist_followers',\n",
        "       'feature_mean_artist_followers']].fillna(0).astype(int)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "45880cf3",
      "metadata": {},
      "source": [
        "Now that the dummy about the genres has been created, it is posisble to proceed with the dropping of the genres columns as well as of other columns with no information of use (e.g., the id and the name of the artist are of no use given that the popularity and the followers of the artist are known, indeed these are the main characteristics that are likely to differentiate among popular and non popular songs and tell much more about an artist than its name per se). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89d5d677",
      "metadata": {
        "id": "89d5d677"
      },
      "outputs": [],
      "source": [
        "for i in range(15):\n",
        "    train_df1.drop(columns = [f\"genre_{i}\"], inplace = True)\n",
        "train_df1.drop (columns = [\"name\", \"id_artists\", \"id_x\", \"genres\", \"artist_0\", \"all\"], inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d8c4f73",
      "metadata": {
        "id": "0d8c4f73"
      },
      "outputs": [],
      "source": [
        "train_df1.drop(['album_name', 'album_id','artist_list','genre_flag_valid'],axis=1,inplace=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "1af19c9c",
      "metadata": {},
      "source": [
        "Then, it was also deemed appropriate to create a dummy variable, building upon the genres' flags created above, to indicate whether the particular song under analysis belongs or not to the top genres and/or belongs to the popular genres. The code to do so is as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcdcd0d4",
      "metadata": {
        "id": "dcdcd0d4"
      },
      "outputs": [],
      "source": [
        "train_df1[\"genre_flag_top\"]=[0 if x is None else 1 for x in train_df1[\"genre_flag_top\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac62114a",
      "metadata": {
        "id": "ac62114a"
      },
      "outputs": [],
      "source": [
        "train_df1[\"genre_flag_popular\"]=[0 if x is None else 1 for x in train_df1[\"genre_flag_popular\"]]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "403f78e8",
      "metadata": {},
      "source": [
        "Then, some preprocessing for what concerns the date of the songs' release was performed. In particular, the date was formatted with the to_datetime function of pandas and then the weekday and the month were extracted. Indeed, realising songs on a particular day of the week or month might impact their popularity significantly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "epcTMtYeBujh",
      "metadata": {
        "id": "epcTMtYeBujh"
      },
      "outputs": [],
      "source": [
        "train_df1['release_date_format'] = pd.to_datetime(train_df1['release_date'], errors='coerce')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XORatiOUBQjF",
      "metadata": {
        "id": "XORatiOUBQjF"
      },
      "outputs": [],
      "source": [
        "train_df1['weekday'] = train_df1['release_date_format'].dt.dayofweek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HWlOSacEFAAP",
      "metadata": {
        "id": "HWlOSacEFAAP"
      },
      "outputs": [],
      "source": [
        "train_df1 = train_df1.join(train_df1['release_date'].str.split('-', expand=True).rename(columns={0:'release_year', 1:'release_month', 2:'release_day'}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "673ac038",
      "metadata": {
        "id": "673ac038"
      },
      "outputs": [],
      "source": [
        "train_df1[\"release_month\"] = train_df1[\"release_month\"].astype(\"float\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "0c4981bf",
      "metadata": {},
      "source": [
        "Then, superflous columns were dropped (notice that the year has been dropped since it was deemed less relevant that the day of the week and the month of the release, as a matter of fact the task is predicting future popularity and not past popularity, hence having, for instance, the informtion that songs released in 2010 are more popular than those published in 2009 does not provide actionable insights for record labels):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63a87d42",
      "metadata": {
        "id": "63a87d42"
      },
      "outputs": [],
      "source": [
        "train_df1.drop(['release_date', 'release_year','release_day','release_date_format','id_y'], axis=1, inplace = True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5a5c7eda",
      "metadata": {},
      "source": [
        "Then, **One-Hot Encoding** was applied to the categorical variables of the dataframe so that they can be used in the machine learning models. In particular, the categorical variables transformed are the following: the day of the week and the month. The record label was left as an object and not included in the One-Hot encoding as in the dataset there are more than 1600 record labels and, hence, creating a dummy variable for each of them would have resulted in a very sparse dataframe. This would have made the training of the models very slow and would have also made the models less precise with high probability. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19cb3422",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19cb3422",
        "outputId": "6d4b8125-4a26-477a-9da3-6ea2e0e38082"
      },
      "outputs": [],
      "source": [
        "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "columns_encoded=OH_encoder.fit_transform(train_df1[['release_month','weekday']])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d8b968d0",
      "metadata": {},
      "source": [
        "Thenm the new encoded columns were transformed into a dataframe and merged to the training dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21a6a955",
      "metadata": {
        "id": "21a6a955"
      },
      "outputs": [],
      "source": [
        "columns_encoded = pd.DataFrame(columns_encoded, columns=OH_encoder.get_feature_names_out(['release_month','weekday']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b8ebedc",
      "metadata": {
        "id": "3b8ebedc"
      },
      "outputs": [],
      "source": [
        "train_df1=pd.merge(train_df1,columns_encoded,left_index=True,right_index=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "626aa472",
      "metadata": {},
      "source": [
        "And the old, not encoded, versions of the variables were dropped:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5C2Gh9krGZSs",
      "metadata": {
        "id": "5C2Gh9krGZSs"
      },
      "outputs": [],
      "source": [
        "train_df1.drop(\"release_month\", axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PiMl_KccKYt6",
      "metadata": {
        "id": "PiMl_KccKYt6"
      },
      "outputs": [],
      "source": [
        "train_df1.drop(\"weekday\", axis=1, inplace=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5436c978",
      "metadata": {},
      "source": [
        "Then, as a checkpoint, the resulting dataframe was saved as a csv file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mD7q0vKmHFVB",
      "metadata": {
        "id": "mD7q0vKmHFVB"
      },
      "outputs": [],
      "source": [
        "train_df1.to_csv('final4 (1).csv', index=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "94012584",
      "metadata": {
        "id": "94012584"
      },
      "source": [
        "### Artists analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aerUH_S11EuL",
      "metadata": {
        "id": "aerUH_S11EuL"
      },
      "outputs": [],
      "source": [
        "train_df1 = pd.read_csv(\"final4 (1).csv\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "271524a0",
      "metadata": {},
      "source": [
        "Now, a closer look at three very important artists have been taken. In particular, Meghan Trainor, Ariana Grande, and Taylor Swift were analysed more in detail to understand if there are some distinguishable patterns that differentiate their popular songs from their non popular ones. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c35e06c5",
      "metadata": {},
      "source": [
        "#### Meghan Trainor"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "dff25343",
      "metadata": {},
      "source": [
        "A sub-dataframe containing only songs by Meghan Trainor was created (notice that only the songs in which the artist was singing alone were considered, indeed havig a feature or a collaboration with another artist might have impacted the popularity of the song beyond the artist's popularity and hence bias results):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d6d0ad4",
      "metadata": {
        "id": "7d6d0ad4"
      },
      "outputs": [],
      "source": [
        "indices=train_df1[train_df1[\"all_artists\"]==\"[Meghan Trainor]\"].index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26247e35",
      "metadata": {
        "id": "26247e35"
      },
      "outputs": [],
      "source": [
        "Meghan=train_df1.loc[indices]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "31464a58",
      "metadata": {},
      "source": [
        "Once again, songs were splitt into popular and non popular as was done for topic modelling and sentiment analysis so as to be able to detect any difference, if present. This time, the threshold used was 45% as the mean popularity of the songs in the dataframe (as before excluding all those songs with 0 popularity) was approximately 45%:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33f961be",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df1[train_df1['popularity_song']>0][\"popularity_song\"].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vIDALwSr3LEK",
      "metadata": {
        "id": "vIDALwSr3LEK"
      },
      "outputs": [],
      "source": [
        "Meghan.loc[(Meghan['popularity_song'] > 45), 'popular'] = 'Y'\n",
        "Meghan['popular'] = Meghan['popular'].fillna('N')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eDp2mCihC0bw",
      "metadata": {
        "id": "eDp2mCihC0bw"
      },
      "outputs": [],
      "source": [
        "Meghan.reset_index(inplace=True,drop=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "a9513952",
      "metadata": {},
      "source": [
        "Then, indices of popular and non popular songs were obtained:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JjgroszkC15U",
      "metadata": {
        "id": "JjgroszkC15U"
      },
      "outputs": [],
      "source": [
        "indices_pop = Meghan[Meghan[\"popular\"]==\"Y\"].index\n",
        "indices_nonpop = Meghan[Meghan[\"popular\"]==\"N\"].index"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "10536482",
      "metadata": {},
      "source": [
        "And non numeric features were dropped so as to be able to run a Pricipal Component Analysis (PCA) on the sub-dataframe to try and detect what are the most relevant dimensions to be considered. More in depth, Principal Component Analysis is an unsupervised learning technique that helps in finding a sequence of linear combinations of variables so as to be able to increases interpretability yet, at the same time, minimizing information loss. This last point is achieved by preserving maximum variation. As a matter of fact, dimensions on which more variation is included are likely those that carry most of the information that can possibly lead to the distiction between popular and non popular songs. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71d3fe83",
      "metadata": {
        "id": "71d3fe83"
      },
      "outputs": [],
      "source": [
        "Meghan_numeric=Meghan.drop([\"record_label\", \"popular\"],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00248173",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00248173",
        "outputId": "336ec134-5d94-4964-fd4e-450cf173b9bf"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=2)\n",
        "features_pca =pca.fit_transform(Meghan_numeric.iloc[:,4:])\n",
        "print(pca.explained_variance_ratio_)\n",
        "print(pca.singular_values_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hK7rjzSx7AOu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "hK7rjzSx7AOu",
        "outputId": "0e95fff0-9e90-4123-9976-17dfeb07b4fd"
      },
      "outputs": [],
      "source": [
        "plt.scatter(features_pca[indices_pop,0], features_pca[indices_pop,1])\n",
        "plt.scatter(features_pca[indices_nonpop,0], features_pca[indices_nonpop,1])\n",
        "plt.legend([\"pop\" , \"non_pop\"], bbox_to_anchor = (1 , 1))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "78110c7b",
      "metadata": {},
      "source": [
        "From the plotting of the songs after applying PCA, it is possible to see that no clear separation between popular and non-popular songs is distinguishable. This is a hint that, in classifying songs as popular or non-popular, a linear classification boundary is likely not ideal, even though this might change in a higher dimensionality space.\n",
        "In particular, though, it is possible to see that the top 3 features along which more variance is associated are, for the first component, respectively the dummy about the genre trap, the number of followers and the popularity of the artist:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9ba66b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "np.argsort(abs(pca.components_[0]))[0:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3af29d08",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df1.iloc[:,25]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2f6e1c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df1.iloc[:,21]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a9f3aaa",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df1.iloc[:,22]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "iNEqz1Ds-gwu",
      "metadata": {
        "id": "iNEqz1Ds-gwu"
      },
      "source": [
        "#### Ariana Grande\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f91607a0",
      "metadata": {},
      "source": [
        "The exact same procedures were then carried out also for Ariana Grande:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7TSRPqnH-kI0",
      "metadata": {
        "id": "7TSRPqnH-kI0"
      },
      "outputs": [],
      "source": [
        "indices=train_df1[train_df1[\"all_artists\"]==\"[Ariana Grande]\"].index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TJeXHA7u-pbs",
      "metadata": {
        "id": "TJeXHA7u-pbs"
      },
      "outputs": [],
      "source": [
        "Ariana=train_df1.loc[indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nOUT_DTe-0i9",
      "metadata": {
        "id": "nOUT_DTe-0i9"
      },
      "outputs": [],
      "source": [
        "Ariana.loc[(Ariana['popularity_song'] > 45), 'popular'] = 'Y'\n",
        "Ariana['popular'] = Ariana['popular'].fillna('N')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7zh48YC_Bcov",
      "metadata": {
        "id": "7zh48YC_Bcov"
      },
      "outputs": [],
      "source": [
        "Ariana.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CnnEG3xrBA4k",
      "metadata": {
        "id": "CnnEG3xrBA4k"
      },
      "outputs": [],
      "source": [
        "indices_pop = Ariana[Ariana[\"popular\"]==\"Y\"].index\n",
        "indices_nonpop = Ariana[Ariana[\"popular\"]==\"N\"].index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jYLwae-v-9Lk",
      "metadata": {
        "id": "jYLwae-v-9Lk"
      },
      "outputs": [],
      "source": [
        "Ariana_numeric=Ariana.drop([\"record_label\", \"popular\"],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pI3P_Sad_C2M",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pI3P_Sad_C2M",
        "outputId": "3a1c5a70-00c7-4ff1-c35d-1d51484f8eda"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=2)\n",
        "features_pca =pca.fit_transform(Ariana_numeric.iloc[:,4:])\n",
        "print(pca.explained_variance_ratio_)\n",
        "print(pca.singular_values_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "td_Vzhtb_Kae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "td_Vzhtb_Kae",
        "outputId": "7c0a68a9-9b28-4f54-94ec-b9c56163a0eb"
      },
      "outputs": [],
      "source": [
        "plt.scatter(features_pca[indices_pop,0], features_pca[indices_pop,1])\n",
        "plt.scatter(features_pca[indices_nonpop,0], features_pca[indices_nonpop,1])\n",
        "plt.legend([\"pop\" , \"non_pop\"], bbox_to_anchor = (1 , 1))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d4539999",
      "metadata": {},
      "source": [
        "Here, the same reasoning conducted for Meghan Trainor can be applied. In particular, it is possible to see that the top 3 features along which more variance is associated are, for the first component, once again respectively the dummy about the genre trap, the number of followers and the popularity of the artist:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b3451b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "np.argsort(abs(pca.components_[0]))[0:3]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "DGfCGOLF_uuD",
      "metadata": {
        "id": "DGfCGOLF_uuD"
      },
      "source": [
        "#### Taylor Swift"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5364a3f8",
      "metadata": {},
      "source": [
        "Once again, also for Taylor Swift, the same procedures were carried out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7iFof2-c_wp6",
      "metadata": {
        "id": "7iFof2-c_wp6"
      },
      "outputs": [],
      "source": [
        "indices=train_df1[train_df1[\"all_artists\"]==\"[Taylor Swift]\"].index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tPVpV-MyD7Lj",
      "metadata": {
        "id": "tPVpV-MyD7Lj"
      },
      "outputs": [],
      "source": [
        "Taylor=train_df1.loc[indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JLITm5PVD7Rn",
      "metadata": {
        "id": "JLITm5PVD7Rn"
      },
      "outputs": [],
      "source": [
        "Taylor.loc[(Taylor['popularity_song'] > 45), 'popular'] = 'Y'\n",
        "Taylor['popular'] = Taylor['popular'].fillna('N')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FngDw6fYD7Ug",
      "metadata": {
        "id": "FngDw6fYD7Ug"
      },
      "outputs": [],
      "source": [
        "Taylor.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LSfPnEYUD7W9",
      "metadata": {
        "id": "LSfPnEYUD7W9"
      },
      "outputs": [],
      "source": [
        "indices_pop = Taylor[Taylor[\"popular\"]==\"Y\"].index\n",
        "indices_nonpop = Taylor[Taylor[\"popular\"]==\"N\"].index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JDMU7ik1D7Zy",
      "metadata": {
        "id": "JDMU7ik1D7Zy"
      },
      "outputs": [],
      "source": [
        "Taylor_numeric=Taylor.drop([\"record_label\", \"popular\"],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kF2ns-0wD7c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kF2ns-0wD7c4",
        "outputId": "5cf77d93-cb31-4788-d4f1-1e15e05d9cd2"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=2)\n",
        "features_pca =pca.fit_transform(Taylor_numeric.iloc[:,4:])\n",
        "print(pca.explained_variance_ratio_)\n",
        "print(pca.singular_values_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2axwq8sgEG58",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "2axwq8sgEG58",
        "outputId": "410b3ba5-1138-406d-bc89-19dfea696f7e"
      },
      "outputs": [],
      "source": [
        "plt.scatter(features_pca[indices_pop,0], features_pca[indices_pop,1])\n",
        "plt.scatter(features_pca[indices_nonpop,0], features_pca[indices_nonpop,1])\n",
        "plt.legend([\"pop\" , \"non_pop\"], bbox_to_anchor = (1 , 1))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "2d519dd6",
      "metadata": {},
      "source": [
        "By the looks of this graphs, it seems that Taylor Swift has just one non popular song, thus probably it is not the best choice for the artist to identify significant distinguishing factors. Still, the top 3 features along which more variance is associated can be obtained as before and these are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1e59826",
      "metadata": {},
      "outputs": [],
      "source": [
        "np.argsort(abs(pca.components_[0]))[0:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33d5c109",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df1.iloc[:,25]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a84bad41",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df1.iloc[:,19]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f18631b",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df1.iloc[:,20]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ae63d02",
      "metadata": {
        "id": "1ae63d02"
      },
      "source": [
        "# Exploratory data analysis and more feature engineering"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "23cb5ad9",
      "metadata": {},
      "source": [
        "At this point, before running the final models, some visualizations and data exploration analysis were carried out.\\\n",
        "To begin with, the relationship between the popularity of the song, the popularity of the artist, and technical charactiristics of the songs was explored via the usage of a pairplot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "face04d4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 893
        },
        "id": "face04d4",
        "outputId": "2bb048e3-aeb5-4239-9232-bfe0a24bbd46"
      },
      "outputs": [],
      "source": [
        "sampled_data = train_df1.sample(n=1000, random_state=42)\n",
        "sns.pairplot(sampled_data[['popularity_song', 'danceability', 'energy', 'key', 'loudness',\n",
        "       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n",
        "       'duration', 'speechiness', 'followers', 'popularity_artist']])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5ea3a6b1",
      "metadata": {},
      "source": [
        "From this, and in particular from the histograms on the diagonal, it can be seen that some variables are right skewed. This could potentially be a concern for certain types of models that are not robust to skewedness of data. So as to mitigate this issue, the logarithm of the variables was taken. Indeed, among the usual techniques to deal with skewedness (another one is for example taking the square root), the logarithm is the one that was found to work best on the available data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c197dcf1",
      "metadata": {
        "id": "c197dcf1"
      },
      "outputs": [],
      "source": [
        "train_df1[\"log_duration\"] = train_df1[\"duration\"].map(lambda x: np.log(x) if x > 0 else 0)\n",
        "train_df1[\"log_speechiness\"] = train_df1[\"speechiness\"].map(lambda x: np.log(x) if x > 0 else 0)\n",
        "train_df1[\"log_instrumentalness\"] = train_df1[\"instrumentalness\"].map(lambda x: np.log(x) if x > 0 else 0)\n",
        "train_df1[\"log_liveness\"] = train_df1[\"liveness\"].map(lambda x: np.log(x) if x > 0 else 0)\n",
        "train_df1[\"log_followers\"] = train_df1[\"followers\"].map(lambda x: np.log(x) if x > 0 else 0)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "78155a0d",
      "metadata": {},
      "source": [
        "Then, the boxplots of the technical features of the songs were plotted as a way to check for the presence of outliers, which might, once again, negatively impact the performance of certain models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc5ea7d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "bc5ea7d1",
        "outputId": "86f2f0a6-c22f-4df0-d8ce-bbef2523a74d"
      },
      "outputs": [],
      "source": [
        "cols = ['danceability', 'energy', 'acousticness', 'log_liveness', 'valence',  'log_speechiness']\n",
        "sns.boxplot(data=train_df1[cols], orient=\"h\")\n",
        "plt.title('Boxplot of technical characteristics')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a64629c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "9a64629c",
        "outputId": "51e755eb-de79-43cd-fb34-426851aaf554"
      },
      "outputs": [],
      "source": [
        "cols = ['loudness','log_instrumentalness']\n",
        "sns.boxplot(data=train_df1[cols], orient=\"h\")\n",
        "plt.title('Boxplot of technical characteristics')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc3ce3a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "bc3ce3a0",
        "outputId": "72c4c13d-ffd5-4e2c-89b2-af0efac14455"
      },
      "outputs": [],
      "source": [
        "cols = ['log_duration']\n",
        "sns.boxplot(data=train_df1[cols], orient=\"h\")\n",
        "plt.title('Boxplot of technical characteristics')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "a9e2745b",
      "metadata": {},
      "source": [
        "As it can be seen, there seems to be some outliers in the log_duration, log_speechiness, and loudness variables. However, since the outliers are not unrealistic values, they will not be removed from the dataset for the time being. If then some particular model will show to be particularly sensitive to outliers, the outliers will be removed and the model will be retrained."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d37113b3",
      "metadata": {},
      "source": [
        "Then, the key of the song was analysed and plotted against the popularity of the song:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TIQ5JQGiKKwy",
      "metadata": {
        "id": "TIQ5JQGiKKwy"
      },
      "outputs": [],
      "source": [
        "train_df1['key'] = train_df1['key'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f63e7f9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "6f63e7f9",
        "outputId": "5e8ff9b7-63cf-4ea4-c710-6077b7c52a99"
      },
      "outputs": [],
      "source": [
        "sns.barplot(x = 'key', y = 'popularity_song', data = train_df1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3e0888d2",
      "metadata": {},
      "source": [
        "As it can be seen, there seems not to be a great difference in the popularity of the songs depending on the key."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "eca5aaa8",
      "metadata": {},
      "source": [
        "Then, the correlations of the features with the variable of interest were obtained. Correlation indeed might be a sign of what are the variables that are more likely to impact the popularity of the song. It has though to be remembered that, in general, correlation does NOT imply causation. Still this could be an interesting starting point and several useful informations can be deduced. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hiLJgsW0JIhh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiLJgsW0JIhh",
        "outputId": "368b468a-b604-400f-9184-c95813c73e09"
      },
      "outputs": [],
      "source": [
        "train_df1.corrwith(train_df1['popularity_song']).sort_values(ascending=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "fe028ff2",
      "metadata": {},
      "source": [
        "From this, it seems that indicators of popularity of the artists involved in the songs are very correlated with the final popularity of the song, which also makes sense logically. Moreover, instrumentalness seems to be the varibale with the lowest correlation, even a negative one. This might also make sense since , in general, songs with lyrics are more popular than instrumental ones. \\\n",
        "Now, the correlation matrix has been plotted for the variables that risk to be highly correlated withing each other. As a matter of fact, high correlation within variables might severely impact the performance of some models, one amongst them is linear regression. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7322028",
      "metadata": {},
      "outputs": [],
      "source": [
        "corr_matrix = train_df1[['feature_max_artist_popularity',\n",
        "       'feature_mean_artist_popularity', 'feature_max_artist_followers',\n",
        "       'feature_mean_artist_followers', 'genre_flag_top', 'genre_flag_popular',\n",
        "       'followers', 'popularity_artist']].corr()\n",
        "sns.heatmap(corr_matrix, annot=True,cmap=\"Greens\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ff2ab4fc",
      "metadata": {},
      "source": [
        "Clearly, there is a very high correlation between an artist popularity and the max and mean popularity of the artists included in the song. This was highly foreseable and will surely be taken under consideration when applying models.\\\n",
        "The correlation between days of the week and months of the year was also explored (also the overall popularity of the song was included in this case to get extra information): "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bae9978",
      "metadata": {},
      "outputs": [],
      "source": [
        "corr_matrix = train_df1[['popularity_song','release_month_1.0', 'release_month_2.0',\n",
        "       'release_month_3.0', 'release_month_4.0', 'release_month_5.0',\n",
        "       'release_month_6.0', 'release_month_7.0', 'release_month_8.0',\n",
        "       'release_month_9.0', 'release_month_10.0', 'release_month_11.0',\n",
        "       'release_month_12.0', 'release_month_nan', 'weekday_0.0', 'weekday_1.0',\n",
        "       'weekday_2.0', 'weekday_3.0', 'weekday_4.0', 'weekday_5.0',\n",
        "       'weekday_6.0']].corr()\n",
        "sns.heatmap(corr_matrix,cmap=\"Greens\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3ca174d0",
      "metadata": {},
      "source": [
        "In this case, there seems not to be any issue concerning multicollinearity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb81caa0",
      "metadata": {
        "id": "fb81caa0"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4842dca2",
      "metadata": {},
      "source": [
        "After having conducted some exploratory data analysis and some more feature engineering, some machine learning models were applied to actually predict the popularity of songs and effectively answer the reasearch question that has guided this whole analysis. In particular, the problem was addressed both from a regression and from a classification perspective.  "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "79778656",
      "metadata": {},
      "source": [
        "### Regression Perspective"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "89d2f538",
      "metadata": {},
      "source": [
        "To begin with, the problem was addressed from a regression perspective. Hence, an attampt was made in predicting exactly the popularity of the song. To this end, after having performed a train test split and a Standard Scaling (which is not required for every model, indeed some of them are insensitive to the scale of the data, but still is for some others), different machine learning models were tested so as to identify the one working best on the avalilable data. It has to be noted how the few categorical features that were still present in the dataframe (e.g., the record label) were excluded from this part of the analysis as the following algorithms are not able to deal with categorical features. They were then included when an attempt was done with the CatBoost algorithm. Though, results were worst, even after a cross validated grid search, than the ones of the best performing regression algorithm included below and, hence, it was not included in the final analysis. More specifically, to evaluate the results, both the Root Mean Squared Error and the R-squared were used. The RMSE, in particular, represents the square root of the second sample moment of the differences between predicted values and observed values. While the R-squared (coefficient of determination) is the proportion of the variation in the dependent variable that is predictable from the independent variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24c449a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "X = train_df1[[ 'danceability', 'energy', 'loudness',\n",
        "       'acousticness', 'valence', 'tempo',\n",
        "       'feature_max_artist_popularity', 'feature_mean_artist_popularity',\n",
        "       'feature_max_artist_followers', 'feature_mean_artist_followers',\n",
        "       'genre_flag_top', 'genre_flag_popular',\n",
        "       'popularity_artist', 'pop', 'r&b', 'trap', 'rap', 'edm',\n",
        "       'adult standards', 'folk', 'classical', 'reggae', 'jazz', 'rock',\n",
        "       'latin', 'soul', 'release_month_1.0', 'release_month_2.0',\n",
        "       'release_month_3.0', 'release_month_4.0', 'release_month_5.0',\n",
        "       'release_month_6.0', 'release_month_7.0', 'release_month_8.0',\n",
        "       'release_month_9.0', 'release_month_10.0', 'release_month_11.0',\n",
        "       'release_month_12.0', 'release_month_nan', 'weekday_0.0', 'weekday_1.0',\n",
        "       'weekday_2.0', 'weekday_3.0', 'weekday_4.0', 'weekday_5.0',\n",
        "       'weekday_6.0', 'weekday_nan', 'log_duration', 'log_speechiness',\n",
        "       'log_instrumentalness', 'log_liveness', 'log_followers']]\n",
        "y = train_df1['popularity_song']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "regressors = [\n",
        "    Lasso(),\n",
        "    SVR(kernel='poly'),\n",
        "    KNeighborsRegressor(),\n",
        "    RandomForestRegressor(),\n",
        "    GradientBoostingRegressor(),\n",
        "]\n",
        "\n",
        "for regressor in regressors:\n",
        "    regressor.fit(X_train, y_train)\n",
        "    y_pred = regressor.predict(X_test)\n",
        "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    print(f\"{type(regressor).__name__}: RMSE = {rmse:.2f}, R^2 = {r2:.2f}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ba09a150",
      "metadata": {},
      "source": [
        "Amongst the attempted models, Random forest seems the best performing one when it comes to both the root mean squared error and the R-squared. This might be due to the fact that random forest is a powerful algorithm and that it is robust to outliers since they get averaged out by the aggregation of multiple tree outputs. Moreover, it is also robust to the scale of the data."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "669cf505",
      "metadata": {},
      "source": [
        "Having obtained this result, it was deemed appropriate run a RandomizedSearchCV to tune the hyperparameters model to possibly improve the performance. In particular, RandomizedSearchCV implements a “fit” and a “score” method and the parameters of the estimator used to apply these methods are optimized by cross-validated search over parameter settings. A RandomizedSearch rather than a standard GridSearch was used as a great amount of parameters were included to be tested and hence using all possible combinations would have led to a enourmous running time. Instead, RondomizedSearch tries random combinations of a range of values and, hence, can be faster if many parameters have been included. \\\n",
        "Thus, a set for the parameters of the forest have been defined and the RandomizedSearchCV has been applied:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f056f381",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 10)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt',1.0]\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]\n",
        "# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap,\n",
        "               'verbose': [1]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "075ad3d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "rf = RandomForestRegressor()\n",
        "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
        "rf_random.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7363a253",
      "metadata": {},
      "outputs": [],
      "source": [
        "rf_random.best_params_"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d9bf0805",
      "metadata": {},
      "source": [
        "And the forest has been retrained with the best parameters found and the measures for the performance of the model have been computed once again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be324c17",
      "metadata": {},
      "outputs": [],
      "source": [
        "rf_best = RandomForestRegressor(n_estimators=1366, min_samples_split=2, min_samples_leaf= 2, max_features='sqrt', max_depth=None, bootstrap= False, random_state=42)\n",
        "rf_best.fit(X_train, y_train)\n",
        "y_pred = rf_best.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"Root Mean squared error: {np.sqrt(mse)}\")\n",
        "print(f\"R-squared: {r2}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e1313923",
      "metadata": {},
      "source": [
        "As it can be seen, the result improved both in terms of RMSE and R-squared. In particular, the RMSE decreased by 1 while the R-squared increased by 5%. Clearly, several other grid searches were attempted, though, here only the best performing final result is presented. \\\n",
        "As a way to additionally try to impove the predictions, a basic Neural Network was also built and applied. Though, even with the tuning of the parameters was underperforming the results obtained by the random forest and, hence, is not included in this final report. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "7a21074e",
      "metadata": {},
      "source": [
        "### Classification Perspective"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "041b5963",
      "metadata": {},
      "source": [
        "An attempt was also done with a classification perspective. Indeed, it might also be the case that some record labels are not strictly interested in the precise number of the predicted popularity but rather in a simple indication of class consisting in a range of popularities. \\\n",
        "In particular, the problem was addressed as a multi-class classification problem both with 4 and 5 classes. Clearly, when dealing with 5 classes the range for each class is 20 meaning the the classes are 0-19, 20-39, 40-59, 60-79, 80-100. This is the first approach tried. Then also a 4 classes approach was tried, in which the classes are 0-24, 25-49, 50-74, 75-100. \\\n",
        "Before applying some classification algorithms, the distribution of the classes was plotted so as to be able to understand if the classes were balanced or not. In particular, because of the nature of the database at hand which includes a lot of songs with low popularity, the classes were indeed not balanced. To account for this, in the final algorithm, appropriate measures were taken so to reduce that the negative impact of unbalanced classes could have. In particular, as it will be seen later, in the random forest classifier the class weights parameter was used with this aim. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0ec9753",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df1[\"bracket\"]=[0 if 0<=x<20 else 1 for x in train_df1[\"popularity_song\"]]     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65a7fcef",
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(len(train_df1[\"bracket\"])):\n",
        "    if 40<=train_df1[\"popularity_song\"][i] < 60:\n",
        "        train_df1[\"bracket\"][i] = 2\n",
        "    elif 60<=train_df1[\"popularity_song\"][i] < 80:\n",
        "        train_df1[\"bracket\"][i] = 3\n",
        "    elif 80<=train_df1[\"popularity_song\"][i] <= 100:\n",
        "        train_df1[\"bracket\"][i] = 4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bdbbabb",
      "metadata": {},
      "outputs": [],
      "source": [
        "aux=Counter(train_df1[\"bracket\"].tolist())\n",
        "plt.pie(aux.values(),labels=aux.keys(),autopct='%1.1f%%', startangle=90)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "1dbb8891",
      "metadata": {},
      "source": [
        "As in the regression setting, different algorithms were tried to asses which is the most suited for the data at hand. This time though, the measurement used to perform the model evaluation was a simple accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9af3f96",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_class = train_df1[[ 'danceability', 'energy', 'loudness',\n",
        "       'acousticness', 'valence', 'tempo',\n",
        "       'feature_max_artist_popularity', 'feature_mean_artist_popularity',\n",
        "       'feature_max_artist_followers', 'feature_mean_artist_followers',\n",
        "       'genre_flag_top', 'genre_flag_popular',\n",
        "       'popularity_artist', 'pop', 'r&b', 'trap', 'rap', 'edm',\n",
        "       'adult standards', 'folk', 'classical', 'reggae', 'jazz', 'rock',\n",
        "       'latin', 'soul', 'release_month_1.0', 'release_month_2.0',\n",
        "       'release_month_3.0', 'release_month_4.0', 'release_month_5.0',\n",
        "       'release_month_6.0', 'release_month_7.0', 'release_month_8.0',\n",
        "       'release_month_9.0', 'release_month_10.0', 'release_month_11.0',\n",
        "       'release_month_12.0', 'release_month_nan', 'weekday_0.0', 'weekday_1.0',\n",
        "       'weekday_2.0', 'weekday_3.0', 'weekday_4.0', 'weekday_5.0',\n",
        "       'weekday_6.0', 'weekday_nan', 'log_duration', 'log_speechiness',\n",
        "       'log_instrumentalness', 'log_liveness', 'log_followers']]\n",
        "y_class = train_df1['bracket']\n",
        "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(X_class, y_class, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train_class)\n",
        "X_test = scaler.transform(X_test_class)\n",
        "classifiers = [\n",
        "    SVC(),\n",
        "    RandomForestClassifier(),\n",
        "    LogisticRegression()\n",
        "]\n",
        "\n",
        "for classifier in classifiers:\n",
        "    classifier.fit(X_train_class, y_train_class)\n",
        "    y_pred_class = classifier.predict(X_test_class)\n",
        "    acc = accuracy_score(y_test_class, y_pred_class)\n",
        "    print(f\"{type(classifier).__name__}: Accuracy = {acc:.2f}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "de114bee",
      "metadata": {},
      "source": [
        "Once again, Random Forest seems to be the best performing algortihm on our data also in the classification setting. As of before, a GridSearchCV was applied to tune the hyperparameters of the model. In particular, this time a grid search was used instead of a RamdomizedSearchCV as the number of parameters to be tested was not so high and it was deemed appropriate, thus, to try all possible combinations. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "370d84de",
      "metadata": {},
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    'n_estimators': [200,300,400,500],\n",
        "    'max_depth': [None, 20, 25],\n",
        "    'min_samples_split': [3, 4, 5],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(rfc, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train_class, y_train_class)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a175eb15",
      "metadata": {},
      "outputs": [],
      "source": [
        "rfc=RandomForestClassifier(**grid_search.best_params_, class_weight = {0: 1/0.417, 1: 1/0.16, 2: 1/0.242, 3: 1/0.154, 4: 1/0.027},random_state=42)\n",
        "rfc.fit(X_train_class, y_train_class)\n",
        "y_pred_class = rfc.predict(X_test_class)\n",
        "acc = accuracy_score(y_test_class, y_pred_class)\n",
        "print(f\"Accuracy = {acc:.2f}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "75b9ae05",
      "metadata": {},
      "source": [
        "Even after a 207 minutes grid search, the result did not improve much. Though, it has to be noted that accuracy is not always a proper measure to evaluate the performance pf a classification algorithm. Indeed, when unbalanced classes are present, a more suitable measure could be the ROC-AUC, which is obtained and plotted below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d58542a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict the probabilities for the testing set and binarize them\n",
        "y_prob = rfc.predict_proba(X_test_class)\n",
        "y_test_bin = label_binarize(y_test_class, classes=[0, 1, 2, 3, 4])\n",
        "n_classes = y_test_bin.shape[1]\n",
        "\n",
        "# Then the false positive rate, true positive rate, and AUC was calculated for each class\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_prob[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# The micro-average ROC curve and ROC area were then computed\n",
        "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_bin.ravel(), y_prob.ravel())\n",
        "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "lw = 2\n",
        "colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red', 'green'])\n",
        "for i, color in zip(range(n_classes), colors):\n",
        "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
        "             label=f'ROC curve of class {i} (area = {roc_auc[i]:.2f})')\n",
        "\n",
        "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
        "         label=f'micro-average ROC curve (area = {roc_auc[\"micro\"]:.2f})',\n",
        "         color='deeppink', linestyle=':', linewidth=4)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic for multi-class')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c29ff353",
      "metadata": {},
      "source": [
        "A ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. In the ROC curve, the diagonal line represents a random classifier that makes predictions at random. A classifier that performs better than random will have an ROC curve that is above the diagonal line. The farther the ROC curve is from the diagonal line, the better the classifier is performing. Then the Area Under Curve (AUC) provides an aggregate measure of performance across all possible classification thresholds. In particular, a random classifier would achieve an AUC of 0.5 while a perfect one an AUC of 1, thus the closer the AUC is to 1 the better the classifier is. In the case of multi-class classification, it is appropriate to plot the ROC curves for each class and also a micro-average ROC curve that aggregates the results across all classes so as to be able to evaluate the performance of the classifier not only on specific classes but also more in general. This way of assessing models performance might be more suitable for imbalanced classification problems as it does not only considers the basic accuracy of the algorithm but also the ability of the algorithm to correctly classify the minority classese. \\\n",
        "According to this measurement, the random forest seems to perform quite well, with an average AUC of 0.87 (remembering that 0.5 is the AUC of a random classifier while 1 is the AUC of a perfect classifier)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "735f9ff8",
      "metadata": {},
      "source": [
        "The same identical procedures have been applied for the case with only 4 classes. As it will be seen in the following, the results are quite similar. Indeed, raw accuracy increases by 3% but the AUC remains, on average, the same. Thus, classifying the songs into either 4 or 5 classes does not seem to make a big difference in terms of performance. Though, using 4 classes might be better for balance reasons. It has to be noted that attempts were made only for 4 and 5 classes as these are the ones that allow for an appropriate range of popularity scores to be included in each class. Indeed, if more classes were used, the range of popularity scores would have been too small and, hence, the classification would have been too difficult. On the other hand, using too few classes would have not yielded economically significant results that the record labels could possibly use before deciding into which songs to invest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9edfe2ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df1[\"bracket\"]=[0 if 0<=x<25 else 1 for x in train_df1[\"popularity_song\"]]     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c84df5ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(len(train_df1[\"bracket\"])):\n",
        "    if 50<=train_df1[\"popularity_song\"][i] < 75:\n",
        "        train_df1[\"bracket\"][i] = 2\n",
        "    elif 75<=train_df1[\"popularity_song\"][i] <= 100:\n",
        "        train_df1[\"bracket\"][i] = 3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd8a673b",
      "metadata": {},
      "outputs": [],
      "source": [
        "aux=Counter(train_df1[\"bracket\"].tolist())\n",
        "plt.pie(aux.values(),labels=aux.keys(),autopct='%1.1f%%', startangle=90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a14d093e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "X_class = train_df1[[ 'danceability', 'energy', 'loudness',\n",
        "       'acousticness', 'valence', 'tempo',\n",
        "       'feature_max_artist_popularity', 'feature_mean_artist_popularity',\n",
        "       'feature_max_artist_followers', 'feature_mean_artist_followers',\n",
        "       'genre_flag_top', 'genre_flag_popular',\n",
        "       'popularity_artist', 'pop', 'r&b', 'trap', 'rap', 'edm',\n",
        "       'adult standards', 'folk', 'classical', 'reggae', 'jazz', 'rock',\n",
        "       'latin', 'soul', 'release_month_1.0', 'release_month_2.0',\n",
        "       'release_month_3.0', 'release_month_4.0', 'release_month_5.0',\n",
        "       'release_month_6.0', 'release_month_7.0', 'release_month_8.0',\n",
        "       'release_month_9.0', 'release_month_10.0', 'release_month_11.0',\n",
        "       'release_month_12.0', 'release_month_nan', 'weekday_0.0', 'weekday_1.0',\n",
        "       'weekday_2.0', 'weekday_3.0', 'weekday_4.0', 'weekday_5.0',\n",
        "       'weekday_6.0', 'weekday_nan', 'log_duration', 'log_speechiness',\n",
        "       'log_instrumentalness', 'log_liveness', 'log_followers']]\n",
        "y_class = train_df1['bracket']\n",
        "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(X_class, y_class, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train_class)\n",
        "X_test = scaler.transform(X_test_class)\n",
        "classifiers = [\n",
        "    SVC(),\n",
        "    RandomForestClassifier(),\n",
        "    LogisticRegression(),\n",
        "    SGDClassifier(),\n",
        "    GaussianNB()\n",
        "]\n",
        "\n",
        "for classifier in classifiers:\n",
        "    classifier.fit(X_train_class, y_train_class)\n",
        "    y_pred_class = classifier.predict(X_test_class)\n",
        "    acc = accuracy_score(y_test_class, y_pred_class)\n",
        "    print(f\"{type(classifier).__name__}: Accuracy = {acc:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68bfa78f",
      "metadata": {},
      "outputs": [],
      "source": [
        "rfc=RandomForestClassifier(n_estimators=400, class_weight = {0: 1/0.445, 1: 1/0.251, 2: 1/0.255, 3: 1/0.049},random_state=42)\n",
        "rfc.fit(X_train_class, y_train_class)\n",
        "y_pred_class = rfc.predict(X_test_class)\n",
        "acc = accuracy_score(y_test_class, y_pred_class)\n",
        "print(f\"Accuracy = {acc:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d22f63de",
      "metadata": {},
      "outputs": [],
      "source": [
        "y_prob = rfc.predict_proba(X_test_class)\n",
        "y_test_bin = label_binarize(y_test_class, classes=[0, 1, 2, 3])\n",
        "n_classes = y_test_bin.shape[1]\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_prob[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_bin.ravel(), y_prob.ravel())\n",
        "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "plt.figure()\n",
        "lw = 2\n",
        "colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red'])\n",
        "for i, color in zip(range(n_classes), colors):\n",
        "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
        "             label=f'ROC curve of class {i} (area = {roc_auc[i]:.2f})')\n",
        "\n",
        "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
        "         label=f'micro-average ROC curve (area = {roc_auc[\"micro\"]:.2f})',\n",
        "         color='deeppink', linestyle=':', linewidth=4)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic for multi-class')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b11cd991",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
