{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following procedures are aimed at exploring and analyzing different databases with the end goal of extracting valuable information and identifying patterns. In particular, the first part focuses on the dataset \"vending.csv\", provided for the assignment, while the following two parts are centered on datasets obtained from the surveys run on Qualtrics both for the general testing of the proposed hypothesis and for the A/B testing of our final proposals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART A (Analyses of provided dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index:    <a class=\"anchor\" id=\"index\"></a>\n",
    "* [Index](#index)\n",
    "* [Import of relevant libraries](#zero-bullet)\n",
    "* [Import of data](#first-bullet)\n",
    "* [Missing Values and Type Checks](#second-bullet)\n",
    "* [Correlation Checks](#corr-bullet)\n",
    "* [Dealing with Excess Columns](#third-bullet)\n",
    "* [Data Visualization and Outliers Checks](#fourth-bullet)\n",
    "* [Feature Engineering](#fifth-bullet)\n",
    "* [Exploratory Data Analysis](#sixth-bullet)\n",
    "* [First t-test](#seventh-bullet)\n",
    "* [Cash vs Credit Comparison](#eighth-bullet)\n",
    "* [Second t-test](#ninth-bullet)\n",
    "* [Cash-Carbonated vs Credit-Carbonated](#tenth-bullet)\n",
    "* [Location](#eleventh-bullet)\n",
    "* [RCoil](#twelfth-bullet)\n",
    "* [CatBoost](#thirhtenth-bullet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import of relevant libraries <a class=\"anchor\" id=\"zero-bullet\"></a>\n",
    "[Index](#index) <a class=\"anchor\" id=\"index\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "%matplotlib inline\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import of data <a class=\"anchor\" id=\"first-bullet\"></a>\n",
    "[Index](#index) <a class=\"anchor\" id=\"index\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db=pd.read_csv('vending.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brief vizualization the head of the dataset to have a grasp at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values and Type Checks <a class=\"anchor\" id=\"second-bullet\"></a>\n",
    "[Index](#index) <a class=\"anchor\" id=\"index\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A check for missing values has then been conducted: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further analyses, the index of the missing values is retrieved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db[db['MPrice'].isnull()].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db[db['Product'].isnull()].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "mis=db[db['Category'].isnull()].index.tolist()\n",
    "mis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing that most of the missing values are in the attribute \"Category\" but not in the attribute \"Product\", it is possible to retreive the name of the product so that the missing value can be replaced manually leading to no data loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing=set(db['Product'][mis])\n",
    "missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling up of NaNs can is done with the following loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in mis:\n",
    "    if db['Product'][i]=='Canada Dry - Ginger Ale':\n",
    "        db['Category'][i]='Carbonated'\n",
    "    if db['Product'][i]=='Canada Dry - Ginger Ale & Lemonde':\n",
    "        db['Category'][i]='Carbonated'\n",
    "    if db['Product'][i]=='Doritos Dinamita Chile Lemon':\n",
    "        db['Category'][i]='Food'\n",
    "    if db['Product'][i]=='Doritos Spicy Nacho':\n",
    "        db['Category'][i]='Food'\n",
    "    if db['Product'][i]=='Mini Chips Ahoy - Go Paks':\n",
    "        db['Category'][i]='Food'\n",
    "    if db['Product'][i]=='Oreo Mini - Go Paks':\n",
    "        db['Category'][i]='Food'\n",
    "    if db['Product'][i]=='Starbucks Doubleshot Energy - Coffee':\n",
    "        db['Category'][i]='Non Carbonated'\n",
    "    if db['Product'][i]=='Teddy Grahams - Go Paks':\n",
    "        db['Category'][i]='Food'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should still remain 4 missing values in the feature \"Category\" corresponding to those entries that also have a misisng value in the \"Product\" feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mis=db[db['Category'].isnull()].index.tolist()\n",
    "mis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdb=db.drop(mis,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As no further operation is possibile to retrive the 4 missing values left, they have been dropped. Being that they are only 4, out of a total of 6445 observations, the risk of data loss is fairly limited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checks of the types of inputs data are then performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdb.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional check to see whether the feature \"Status\" presents a unique value or more than one has then been performed with the result that the column contains only one value (namely \"Processed\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdb[\"Status\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Checks <a class=\"anchor\" id=\"corr-bullet\"></a>\n",
    "[Index](#index) <a class=\"anchor\" id=\"index\"></a>\n",
    "<br> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix=newdb.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,15))\n",
    "sns.heatmap(corr_matrix,annot=True,cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the correlation matrix it can be seen that some variables present really high (even perfect in some case) correlation with each other. Correlation can cause issues in several algorithms and models, for this reason, this highly correlated variables will be better explored in the subsequent section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Excess Columns <a class=\"anchor\" id=\"third-bullet\"></a>\n",
    "[Index](#index) <a class=\"anchor\" id=\"index\"></a>\n",
    "<br> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this subsection, all excess columns will be dropped.\n",
    "To begin with, it can be seen that Coil, Qty, and Price are present in two differnt variations inside the provided dataset. Given that the correlation between the variations is perfect (Pearson's correlation coefficient=1), as highlighted by the correlation matrix, it is very likely that the variations of the aforementioned columns are identical. An assesment of whether they contain the same information is therefore run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(newdb[\"RCoil\"]==newdb[\"MCoil\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(newdb[\"RQty\"]==newdb[\"MQty\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(newdb[\"RPrice\"]==newdb[\"MPrice\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assesment conducted, yielded the result that the information contained in the columns compared are the same. Now the same procedure is also run for \"RPrice\" and \"LineTotal\" that, in theory, should present at least some different values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(newdb[\"RPrice\"]==newdb[\"LineTotal\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And indeed they do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a check to see whether there are some Transaction ID that are repeated in several dates has been conducted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = newdb[newdb.groupby('Transaction')['TransDate'].transform('nunique').ne(1)]\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the new dataframe is empty, there are no repeated Transaction ID in different days. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now possible to drop the excess columns, meanining those that do not provide additional information to then be used for prediction or data analysis in general:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_db=newdb.drop([\"Status\",\"Device ID\",\"MCoil\",\"MPrice\",\"MQty\",\"Transaction\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, \"MPrice\", \"MQty\", and \"MCoil\" have been dropped as they provide the same information of \"RCoil\", \"RQty\", and \"RPrice\", while \"Status\" has been dropped because it presents a single values, as showed in the section above. \"Devide ID\" was dropped as the name of the vending machine is present in the dataset, and so having a feature containing the ID would just be a duplicate information. Last but not least, \"Transaction\" was dropped aswell since it is just a number indexing transactions, it does not provide additional useful information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization and Outliers Checks <a class=\"anchor\" id=\"fourth-bullet\"></a>\n",
    "[Index](#index) <a class=\"anchor\" id=\"index\"></a>\n",
    "<br> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histograms of remaining data have been plotted to see their approximate distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_db.hist(bins=50, figsize=(30,30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data seem in line with the expectationa and do not seem to be very unbalanced. Given that no detailed information about the nature of the data are available, no further action has been taken to rescale/normalize the data for the time being. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A check for the presence of outliers has then been performed. The check has been, to begin with, done graphically with boxplots. Progressively, columns have been removed so that also visualisation of features with smaller magnitude was possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdb_no_trans=newdb.copy()\n",
    "newdb_no_trans=newdb_no_trans.drop(\"Transaction\",axis=1)\n",
    "plt.figure(figsize=(30, 20))\n",
    "sns.boxplot(data=newdb_no_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdb_no_big=newdb.copy()\n",
    "newdb_no_big=newdb_no_big.drop([\"Transaction\",\"RCoil\",\"MCoil\"],axis=1)\n",
    "plt.figure(figsize=(30, 20))\n",
    "sns.boxplot(data=newdb_no_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdb_no_big2=newdb.copy()\n",
    "newdb_no_big2=newdb_no_big2.drop([\"Transaction\",\"RCoil\",\"MCoil\",\"RQty\",\"MQty\"],axis=1)\n",
    "plt.figure(figsize=(30, 20))\n",
    "sns.boxplot(data=newdb_no_big2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems not to be any absurd value in any of the feature. To double check this, the feature \"TransTotal\", which will also be the target variable of the analysis, is analysed more in depth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_db[n_db[\"TransTotal\"]>6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to notice that the values greter than 6 are still plausible values, as a consequence, no row has been dropped to prevent information loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering <a class=\"anchor\" id=\"fifth-bullet\"></a>\n",
    "[Index](#index) <a class=\"anchor\" id=\"index\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some feature engineering was then perform to prepare the data to be fed to algotithms and models for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, the date feature was split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_db[\"Date_Split\"]=n_db[\"TransDate\"].str.split(\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_db[[\"Day_of_Week\",\"Month+Day\",\"Year\"]]=pd.DataFrame(n_db.Date_Split.tolist(),index=n_db.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, some columns contain same information about the data as others. A drop is therefore necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_db=n_db.drop([\"TransDate\",\"Date_Split\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Days were then split between Festive and Working days. Days were classified as festive if they were either a saturday, a sunday, or a New Jersey State holiday (https://www.state.nj.us/nj/about/facts/holidays/). Holidays were considered up to July, month of the latest observation in the dataset. The code to perform such operation is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday=[\"January 1\",\"January 17\",\"February 21\",\"April 15\",\"May 30\",\"June 17\",\"July 4\"]\n",
    "n=(n_db[\"Day_of_Week\"]==\"Saturday\") | (n_db[\"Day_of_Week\"]==\"Sunday\") \n",
    "m=n_db[\"Month+Day\"].isin(holiday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_db[\"Festive\"]=np.where(n|m,1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column containing the year was also dropped as it contained do additional information given that all observation in the dataset are from 2022:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_db=n_db.drop([\"Year\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the days were classified as festive or nor, a column containing the month was created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "for i in range(len(n_db[\"Month+Day\"])):\n",
    "    l.append(n_db[\"Month+Day\"].iloc[i].split(\" \")[0])\n",
    "n_db[\"Month\"]=l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, once again, columns containing duplicate information were dropped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_db = n_db.drop([\"Month+Day\",\"Prcd Date\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precise day date of the month was dropped as it was deemed not that relevant once the indication of the month, of the week day, and of whether that day was festive or not was included. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis <a class=\"anchor\" id=\"sixth-bullet\"></a>\n",
    "[Index](#index) <a class=\"anchor\" id=\"index\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, some exploratory data analysis has been conducted in order to deepen the knowledge of the data at hand and to try to identify patterns or charcateristics linking different purchases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"TransTotal\" has been analysed as it is the variable of interest of the analysis. The means has been plotted according to different weekdays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_db.groupby('Day_of_Week')[\"TransTotal\"].mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a difference in means between different days. To see whether this difference is really due to \"Day_of_Week\" or rather to other factors, the \"Type\" of the transaction was also included: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_db.groupby(['Type', 'Day_of_Week'])[\"TransTotal\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a relevant difference between the average transaction total for purchases made with card and the ones made with cash. In particular:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_db.groupby('Type')[\"TransTotal\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First t-test <a class=\"anchor\" id=\"seventh-bullet\"></a>\n",
    "[Index](#index) <a class=\"anchor\" id=\"index\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see whether the empirically discovered difference is statistically significant, a t-test has been run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "stats.ttest_ind(a=n_db[n_db['Type'] == 'Cash'].TransTotal, b=n_db[n_db['Type']== 'Credit'].TransTotal, equal_var=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the p-value is approximately 0, the null hypothesis of non-significance of the difference has been rejected and so there is a statistically significant difference between the transactions done with card and with cash. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, since there is a statistically significant difference, a deeper comparison has been run on Cash VS Credit in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cash vs Credit Comparison <a class=\"anchor\" id=\"eighth-bullet\"></a>\n",
    "[Index](#index) <a class=\"anchor\" id=\"index\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, a deeper analysis has been run to understand whether other features can be the main cause in the differnce between average transaction total when paying with card with respect to when paying with cash."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this point there isn't any operation on the original ready n_db dataset, just some exploratory analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, a comparison within categories has been carried out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_cash = n_db[n_db['Type'] == 'Cash']\n",
    "d_credit = n_db[n_db['Type'] == 'Credit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_cash['TransTotal'].hist(), d_credit['TransTotal'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_credit.groupby('Category')[\"TransTotal\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_credit.groupby('Day_of_Week')[\"TransTotal\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_cash.groupby('Day_of_Week')[\"TransTotal\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_cash.groupby('Category')[\"TransTotal\"].mean(), d_credit.groupby('Category')[\"TransTotal\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a difference between the average transtotal with cash and with credit in all categories of products, let us start by seeing whether the one in the \"Carbonated\" category is indeed statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second T-test <a class=\"anchor\" id=\"ninth-bullet\"></a>\n",
    "[Index](#index) <a class=\"anchor\" id=\"index\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "stats.ttest_ind(a=d_cash[d_cash['Category'] == 'Carbonated'].TransTotal, b=d_credit[d_credit['Category'] == 'Carbonated'].TransTotal, equal_var=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The t-test yield a p-value of 3.392838283865919e-38, meaning that the null hypothesis of non-significance is rejected. Given that there is a significant difference between the average transaction total with cash and credit card when it comes to \"Carbonated\" drinks, a further analysis is conducted to deepen the understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cash-Carbonated vs Credit-Carbonated <a class=\"anchor\" id=\"tenth-bullet\"></a>\n",
    "[Index](#index) <a class=\"anchor\" id=\"index\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here follows some comparisons between the carbonated drinks that are bought with cash and those that are bought with credit card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carbonated_cash = d_cash[d_cash['Category'] == 'Carbonated']\n",
    "carbonated_credit =d_credit[d_credit['Category'] == 'Carbonated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = carbonated_cash.groupby('Product')['RPrice'].mean()-carbonated_credit.groupby('Product')['RPrice'].mean() == 0\n",
    "#carbonated[\"Product\"][l]\n",
    "l = l[l==False]\n",
    "la = l[l==False].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carbonated_differ = n_db.loc[n_db['Product'].isin(la)]\n",
    "carbonated_differ['Product'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carbonated_differ_g = carbonated_differ.drop(['Month','TransTotal','Festive','Location','Type','Machine','Category','RQty','LineTotal','Day_of_Week'],axis=1)\n",
    "carbonated_differ_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carbonated_differ_g.groupby('Product')['RPrice'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carbonated_differ_g.groupby('Product')['RPrice'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cola_zero = carbonated_differ.loc[carbonated_differ['Product'] == 'Coca Cola - Zero Sugar']\n",
    "cola_zero.groupby('RPrice')['Location'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cola_zero.groupby('RPrice')['Month'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carbonated = n_db[n_db['Category']=='Carbonated']\n",
    "carbonated.groupby('Product')['RPrice'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carbonated = n_db[n_db['Category']=='Carbonated']\n",
    "carbonated.groupby('RPrice')['Product'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carbonated.groupby('Product')['RPrice'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the analyses carried out above, what was discovered is that different products have different prices for unknown reasons. Therefore, it is not possible to conclude whether carbonated drinks are those really driving the difference between cash and credit or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same holds true also for the other categories of products, for easiness of presentation codes are not reported in the final version but the results are in line with what happened with carbonated drinks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location  <a class=\"anchor\" id=\"eleventh-bullet\"></a>\n",
    "[Index](#index) <a class=\"anchor\" id=\"index\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, an analysis of how the location might have had an impact of the transaction total has been carried out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_db.Location.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_db.groupby('Location')['TransTotal'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the variation seems to be way smaller that the one obtained for the cash vs credit comparison performed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_db.groupby(['Location','Day_of_Week'])['TransTotal'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the analysis above, it is possible to confirm what was already foreseeable with logic: transaction totals are higher in the mall during weekends and in the offices/library during weekdays. This was imaginable since the library and the offices are working venues while the mall is mainly a leisure one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RCoil  <a class=\"anchor\" id=\"twelfth-bullet\"></a>\n",
    "[Index](#index) <a class=\"anchor\" id=\"index\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we tried to analyse whether the position of the product in the vending machine has some relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_db.groupby('RCoil')['TransTotal'].sum().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this representation it seems that the most used coils are the ones approximately at the middle. Let us try to gain some more information by grouping the coils and plotting an heatmap. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variable summarizing the coils has been created. This variable, in particular, contains the number of the row of the coil used (here we assumed that the vending machines at hand have a standard configuration: 10 coils per row):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_db_c=n_db.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_db_c['hor_eye'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(n_db_c)):\n",
    "    n_db_c['hor_eye'].loc[n_db_c.index[i]] = int(str(n_db_c['RCoil'].loc[n_db_c.index[i]])[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new variable is than use to plot an heatmap to use the usage of coils:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = n_db_c.groupby('RCoil')['TransTotal'].sum()\n",
    "b = a.to_frame()\n",
    "data = pd.DataFrame({'RCoil': range(110,170),\n",
    "                   'Total': 0},\n",
    "                   columns = ['RCoil', 'Total'])\n",
    "data = pd.merge(b, data, left_on=\"RCoil\", right_on=\"RCoil\")\n",
    "data.drop(['Total'], axis=1, inplace= True)\n",
    "data['Row'] = np.nan\n",
    "data['Column'] = np.nan\n",
    "for i in range(len(data)):\n",
    "    data['Row'].loc[data.index[i]] = int(str(data['RCoil'].loc[data.index[i]])[1])\n",
    "    data['Column'].loc[data.index[i]] = int(str(data['RCoil'].loc[data.index[i]])[2])\n",
    "data.drop(['RCoil'], axis=1, inplace = True)\n",
    "datas = pd.DataFrame(np.nan, index=[i for i in range(1,7)], columns=[i for i in range(0,10)])\n",
    "for i in range(len(data)):\n",
    "    a = data['Row'].loc[data.index[i]]\n",
    "    b = data['Column'].loc[data.index[i]]\n",
    "    c = data['TransTotal'].loc[data.index[i]]\n",
    "    datas[a][b] = c\n",
    "datas.drop([0,6,7,8,9], axis =1, inplace = True)\n",
    "datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(datas, cmap = 'YlOrRd')\n",
    "ax.set(xlabel=\"Columns\", ylabel=\"Rows\")\n",
    "ax.xaxis.tick_top()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This heatmap seems to verify what was also seen before. Though, given that no extra information is provided about the configurations of the machines, we decided not to further investigate in this respect as it might be the case that the central coils are the most used because some machines might have only them, for instance. Still, these results as well as the ones obtained for the previous features, can be useful to develop new strategies and theories if combined with further information.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost  <a class=\"anchor\" id=\"thirteenth-bullet\"></a>\n",
    "[Index](#index) <a class=\"anchor\" id=\"index\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "To verify what has been previously discovered through data exploration procedures, a CatBoost has been run. CatBoost is a machine learning algorithm that uses gradient boosting on decision trees. This procedure was selected as clustering algorithms did not perform well given the huge number of categorical varibales present in the dataset. For this reason, a tree based approach was selected in order to identify what are the features generating the greatest difference between observations and so granting an higher information gain once split on those variables has been performed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, the target feature is isolated from the rest of the dataset and the train/test split is performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = n_db[\"TransTotal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_db=n_db.drop([\"TransTotal\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_db=n_db.drop([\"LineTotal\", \"RPrice\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(n_db, target, test_size = 0.2, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = cb.Pool(X_train, y_train, cat_features = [\"Location\", \"Machine\", \"Product\", \"Category\",\"Type\", \"Day_of_Week\", \"Festive\", \"Month\", \"RCoil\"])\n",
    "test_dataset = cb.Pool(X_test, y_test, cat_features = [\"Location\", \"Machine\", \"Product\", \"Category\",\"Type\", \"Day_of_Week\", \"Festive\", \"Month\", \"RCoil\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cb.CatBoostRegressor(loss_function=\"RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A grid-search has then been run to identify the optimal parameters among the ones fed to the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "grid = {'iterations': [100, 150, 200],\n",
    "        'learning_rate': [0.025,0.05,0.075, 0.1],\n",
    "        'depth': [2, 4, 6, 8, 10],\n",
    "        'l2_leaf_reg': [0.2, 0.5, 1, 3]}\n",
    "model.grid_search(grid,train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The moel has the been fit on the optimal hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model = cb.CatBoostRegressor(depth= 10,l2_leaf_reg= 1,iterations=200,learning_rate= 0.075,loss_function=\"RMSE\")\n",
    "model.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the feature importances have been obtained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_feature_importance = model.feature_importances_.argsort()\n",
    "plt.barh(n_db.columns[sorted_feature_importance], model.feature_importances_[sorted_feature_importance], \n",
    "        color='turquoise')\n",
    "plt.xlabel(\"CatBoost Feature Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the importances have been devised, let us asses the performance of the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)\n",
    "rmse = (np.sqrt(mean_squared_error(y_test, pred)))\n",
    "r2 = r2_score(y_test, pred)\n",
    "print(\"Testing performance\")\n",
    "print(\"RMSE: {:.2f}\".format(rmse))\n",
    "print(\"R2: {:.2f}\".format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART B (Preparation of the datasets to be used on STATA to analyze survey results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The starting point is the import of the survey dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"H-FARM INNOVATION business case_November 19, 2022_11.57.csv\", sep = \";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where the column concerning the card is null, we associated a 1 in the newly created \"Non_user\" column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = np.where(df[\"Q10\"].isnull())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Non_user\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in l:\n",
    "    df.at[i, \"Non_user\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar procedure was carried out for the credit card users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Card_y\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.where(df[\"Q10\"]==1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in n:\n",
    "    df.at[i, \"Card_y\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and also for the cash users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Card_n\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.where(df[\"Q10\"]==0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in m:\n",
    "    df.at[i, \"Card_n\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the column concerning card ussage has been dropped since now its information is stored in the previously created columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"Q10\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created the df1 dataset containing only student observations and the df2 containing only worker observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.drop(\"Usage Workplace\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.drop(\"Usage Library\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"df_stud.csv\")\n",
    "df2.to_csv(\"df_work.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART C (Preparation of the datasets to be used on STATA to analyze A/B survey's results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the relevent datasets and remove excess rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"A_B test_November 26, 2022_07.00.csv\", sep = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop([0,1,2], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"A_B test 2_November 26, 2022_07.02.csv\", sep = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.drop([0,1,2], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We concatenate the two datasets to build a new one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df1.append(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we create a new column containing the offer type showed to the respondent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [0]*100+[1]*103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final[\"offer_type\"] = l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then renamed the columns for easiness of procedures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.rename(columns={\"Q3\":\"age\", \"Q4\":\"NJ resident\", \"Q5\":\"occupation\", \"Q7_1\": \"vending_user\", \"Q9\": \"nonuser_to_app\", \"Q10_1\":\"card_yn\", \"Unnamed: 6\":\"cash_to_app\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and dropped rows corresponding to those who pay with credit card:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.dropna(axis=0, subset=[\"nonuser_to_app\", \"cash_to_app\"], how=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before exporting the csv to be used on STATA, we combined the observations for non users with the ones for cash users in a single column named \"propensity\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final[\"propensity\"] = df_final[\"nonuser_to_app\"].fillna(df_final[\"cash_to_app\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(\"df_final_ab.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
