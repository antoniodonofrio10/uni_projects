{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d906833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import time \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Optional, Union, List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "###############################################################################\n",
    "# 1) Exponential Schedules for VESDE\n",
    "###############################################################################\n",
    "def exponential_schedule_get_value(t, min_val, max_val):\n",
    "    return min_val * (max_val / min_val) ** t\n",
    "\n",
    "def exponential_schedule_get_derivative(t, min_val, max_val):\n",
    "    val = exponential_schedule_get_value(t, min_val, max_val)\n",
    "    return val * np.log(max_val / min_val)\n",
    "\n",
    "def linear_schedule_get_value(t, min_val, max_val):\n",
    "    return min_val + (max_val - min_val) * t\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 2) VESDE (Forward)\n",
    "###############################################################################\n",
    "def create_vpsde(hyperparam_min=0.01, hyperparam_max=20.0):\n",
    "    def drift_and_diffusion_vpsde(y, t):\n",
    "        beta_t = linear_schedule_get_value(t, hyperparam_min, hyperparam_max)\n",
    "        drift = -0.5 * beta_t * y\n",
    "        diffusion = np.sqrt(beta_t)\n",
    "        return drift, diffusion\n",
    "\n",
    "    def sample_prior_vpsde(shape, seed=None):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        return rng.normal(0.0, 1.0, size=shape)\n",
    "\n",
    "    def get_mean_std_vpsde(y0, t):\n",
    "        a = hyperparam_min\n",
    "        b = hyperparam_max - hyperparam_min\n",
    "        beta_integral = a*t + 0.5*b*(t**2)\n",
    "        mean = y0 * np.exp(-0.5 * beta_integral)\n",
    "        std = np.sqrt(1 - np.exp(-beta_integral))\n",
    "        std = np.broadcast_to(std, y0.shape)\n",
    "        return mean, std\n",
    "\n",
    "    return {\n",
    "        \"name\": \"vpsde\",\n",
    "        \"hyperparam_min\": hyperparam_min,\n",
    "        \"hyperparam_max\": hyperparam_max,\n",
    "        \"drift_and_diffusion\": drift_and_diffusion_vpsde,\n",
    "        \"sample_prior\": sample_prior_vpsde,\n",
    "        \"get_mean_std\": get_mean_std_vpsde\n",
    "    }\n",
    "\n",
    "\n",
    "def create_vesde(hyperparam_min=0.01, hyperparam_max=20.0):\n",
    "    def drift_and_diffusion_vesde(y, t):\n",
    "        sigma = exponential_schedule_get_value(t, hyperparam_min, hyperparam_max)\n",
    "        sigma_prime = exponential_schedule_get_derivative(t, hyperparam_min, hyperparam_max)\n",
    "        drift = np.zeros_like(y)\n",
    "        diffusion = np.sqrt(2.0 * sigma * sigma_prime)\n",
    "        return drift, diffusion\n",
    "\n",
    "    def sample_prior_vesde(shape, seed=None):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        return rng.normal(0.0, hyperparam_max, size=shape)\n",
    "\n",
    "    def get_mean_std_vesde(y0, t):\n",
    "        sigma_t = exponential_schedule_get_value(t, hyperparam_min, hyperparam_max)\n",
    "        sigma_0 = exponential_schedule_get_value(0.0, hyperparam_min, hyperparam_max)\n",
    "        std = np.sqrt(sigma_t**2 - sigma_0**2)\n",
    "        std = np.broadcast_to(std, y0.shape)\n",
    "        return y0, std\n",
    "\n",
    "    def initialize_from_data(y0):\n",
    "        hyperparam_min_ = 0.01\n",
    "        if y0.shape[1] == 1:\n",
    "            max_diff = y0.max() - y0.min()\n",
    "        else:\n",
    "            y0_aug = y0[:, np.newaxis, :]\n",
    "            max_diff = np.max(np.sqrt(np.sum((y0_aug - y0)**2, axis=-1)))\n",
    "        return hyperparam_min_, max_diff\n",
    "\n",
    "    return {\n",
    "        \"name\": \"vesde\",\n",
    "        \"hyperparam_min\": hyperparam_min,\n",
    "        \"hyperparam_max\": hyperparam_max,\n",
    "        \"drift_and_diffusion\": drift_and_diffusion_vesde,\n",
    "        \"sample_prior\": sample_prior_vesde,\n",
    "        \"get_mean_std\": get_mean_std_vesde,\n",
    "        \"init_from_data\": initialize_from_data\n",
    "    }\n",
    "\n",
    "###############################################################################\n",
    "# 3) VESDE Reverse: Euler-Maruyama\n",
    "###############################################################################\n",
    "def euler_maruyama_update(y, drift, diffusion, dt, rng):\n",
    "    dW = rng.normal(size=y.shape)\n",
    "    return y + drift * dt + diffusion * np.sqrt(dt) * dW\n",
    "\n",
    "def euler_maruyama_integration(sde_dict, y0, t0, t1, n_steps, rng):\n",
    "    dt = (t1 - t0) / n_steps\n",
    "    y = y0.copy()\n",
    "    t = t0\n",
    "    for _ in range(n_steps):\n",
    "        t_arr = np.full((y.shape[0], 1), t)\n",
    "        drift, diffusion = sde_dict[\"drift_and_diffusion\"](y, t_arr)\n",
    "        y = euler_maruyama_update(y, drift, diffusion, dt, rng)\n",
    "        t += dt\n",
    "    return y\n",
    "\n",
    "def euler_maruyama_reverse_integration(sde_dict, y0, original_t0, rev_total, n_steps, rng, score_fn):\n",
    "    dt = rev_total / n_steps\n",
    "    y = y0.copy()\n",
    "    t_rev = 0.0\n",
    "    for _ in range(n_steps):\n",
    "        forward_t = original_t0 - t_rev\n",
    "        t_arr = np.full((y.shape[0], 1), forward_t)\n",
    "        drift_fwd, diff_fwd = sde_dict[\"drift_and_diffusion\"](y, t_arr)\n",
    "        drift_rev = -drift_fwd + (diff_fwd**2) * score_fn(y, t_arr)\n",
    "        y = euler_maruyama_update(y, drift_rev, diff_fwd, dt, rng)\n",
    "        t_rev += dt\n",
    "    return y\n",
    "\n",
    "def _pndm_step_vpsde(x_t, noise_t, t_now, t_next, hyperparam_min, hyperparam_max):\n",
    "    def compute_alpha_bar_lin(u):\n",
    "        a = hyperparam_min\n",
    "        b = hyperparam_max - hyperparam_min\n",
    "        val = a*u + 0.5*b*(u**2)\n",
    "        return np.exp(-val)\n",
    "    \n",
    "    ab_t = compute_alpha_bar_lin(t_now)\n",
    "    ab_s = compute_alpha_bar_lin(t_next)\n",
    "\n",
    "    def s_sqrt(x):\n",
    "        return np.sqrt(np.maximum(x, 1e-40))\n",
    "    \n",
    "    sqrt_ab_t = s_sqrt(ab_t)\n",
    "    sqrt_ab_s = s_sqrt(ab_s)\n",
    "    diff_ab   = ab_s - ab_t\n",
    "\n",
    "    denom1 = sqrt_ab_t * (sqrt_ab_s + sqrt_ab_t)\n",
    "    denom2 = sqrt_ab_t * (s_sqrt((1.0-ab_s)*ab_t) + s_sqrt((1.0-ab_t)*ab_s))\n",
    "\n",
    "    bracket = (x_t / denom1) - (noise_t / denom2)\n",
    "    return x_t + diff_ab * bracket\n",
    "\n",
    "###############################################################################\n",
    "# 4) VESDE Reverse: DPM Solver\n",
    "###############################################################################\n",
    "def vesde_reverse_finalDPM(\n",
    "    sde_dict, \n",
    "    y0, \n",
    "    original_t0, \n",
    "    rev_total, \n",
    "    n_steps, \n",
    "    rng, \n",
    "    noise_fn\n",
    "):\n",
    "    y = y0.copy()\n",
    "    t_rev = 0.0\n",
    "    dt = rev_total / n_steps\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "        forward_t = original_t0 - t_rev\n",
    "        t_arr = np.full((y.shape[0], 1), forward_t)\n",
    "        \n",
    "        sigma_t = exponential_schedule_get_value(forward_t, \n",
    "                                                 sde_dict[\"hyperparam_min\"], \n",
    "                                                 sde_dict[\"hyperparam_max\"])\n",
    "        \n",
    "        next_t = forward_t - dt\n",
    "        sigma_next = exponential_schedule_get_value(next_t, \n",
    "                                                    sde_dict[\"hyperparam_min\"], \n",
    "                                                    sde_dict[\"hyperparam_max\"])\n",
    "        \n",
    "        h_i = np.log(sigma_next/sigma_t + 1e-40)\n",
    "        \n",
    "        epsilon = noise_fn(y, t_arr)\n",
    "        \n",
    "        y = y + epsilon * sigma_t * (np.exp(h_i) - 1)\n",
    "        t_rev += dt\n",
    "        \n",
    "    return y  \n",
    "\n",
    "###############################################################################\n",
    "# A) Helper to choose sigma(t) from either \"karras_linear\" or \"exponential\"\n",
    "###############################################################################\n",
    "def get_sigma_value(\n",
    "    t_value: float,\n",
    "    schedule: str,\n",
    "    hyperparam_min: float,\n",
    "    hyperparam_max: float\n",
    ") -> float:\n",
    "    if schedule == \"karras_linear\":\n",
    "        return t_value\n",
    "    else:\n",
    "        return exponential_schedule_get_value(t_value, hyperparam_min, hyperparam_max)\n",
    "\n",
    "###############################################################################\n",
    "# B) Modified DPM sampler that includes Karras-churn\n",
    "###############################################################################\n",
    "def vesde_reverse_finalDPM_stoch(\n",
    "    sde_dict,\n",
    "    y0,\n",
    "    original_t0,\n",
    "    rev_total,\n",
    "    n_steps,\n",
    "    rng,\n",
    "    noise_fn,\n",
    "    S_churn=10.0,\n",
    "    S_min=0.05,\n",
    "    S_max=50.0,\n",
    "    S_noise=1.0,\n",
    "    schedule=\"karras_linear\"\n",
    "):\n",
    "    # Create descending array of time steps, e.g. from 1.0 down to 0.0\n",
    "    t_array = np.linspace(original_t0, original_t0 - rev_total, n_steps + 1)\n",
    "    x = y0.copy()\n",
    "\n",
    "    hp_min = sde_dict[\"hyperparam_min\"]\n",
    "    hp_max = sde_dict[\"hyperparam_max\"]\n",
    "\n",
    "    for i in range(n_steps):\n",
    "        t_i   = t_array[i]\n",
    "        t_ip1 = t_array[i+1]\n",
    "\n",
    "        # 1) Current sigma_i\n",
    "        sigma_i = get_sigma_value(t_i, schedule, hp_min, hp_max)\n",
    "\n",
    "        # 2) Possibly churn\n",
    "        if (sigma_i >= S_min) and (sigma_i <= S_max):\n",
    "            gamma_i = min(S_churn / n_steps, np.sqrt(2.0) - 1.0)\n",
    "        else:\n",
    "            gamma_i = 0.0\n",
    "\n",
    "        sigma_hat_i = sigma_i * (1.0 + gamma_i)\n",
    "\n",
    "        # If increasing sigma, add noise\n",
    "        if sigma_hat_i > sigma_i:\n",
    "            delta_sig_sq = sigma_hat_i**2 - sigma_i**2\n",
    "            delta_sig_sq = max(delta_sig_sq, 0.0)\n",
    "            eps_i = rng.normal(scale=S_noise, size=x.shape)\n",
    "            x = x + np.sqrt(delta_sig_sq) * eps_i\n",
    "\n",
    "        # 3) DPM step from sigma_hat_i -> sigma_{i+1}\n",
    "        sigma_ip1 = get_sigma_value(t_ip1, schedule, hp_min, hp_max)\n",
    "\n",
    "        epsilon_i = noise_fn(x, np.full((x.shape[0], 1), sigma_hat_i))\n",
    "        \n",
    "        # First-order DPM update\n",
    "        x = x + epsilon_i * (sigma_ip1 - sigma_hat_i)\n",
    "\n",
    "    return x\n",
    "\n",
    "def inverse_lambda_mapping(lambda_value, sigma_min, sigma_max):\n",
    "    return -np.log(lambda_value * sigma_min) / np.log(sigma_max / sigma_min)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def vesde_reverse_finalDPM_solver2_stoch(\n",
    "    sde_dict,\n",
    "    y0,\n",
    "    original_t0,\n",
    "    rev_total,\n",
    "    n_steps,\n",
    "    rng,\n",
    "    noise_fn,\n",
    "    S_churn=10.0,\n",
    "    S_min=0.05,\n",
    "    S_max=50.0,\n",
    "    S_noise=1.0,\n",
    "    schedule=\"karras_linear\"\n",
    "):\n",
    "    hp_min = sde_dict[\"hyperparam_min\"]\n",
    "    hp_max = sde_dict[\"hyperparam_max\"]\n",
    "\n",
    "    t_array = np.linspace(original_t0, original_t0 - rev_total, n_steps + 1)\n",
    "    x = y0.copy()\n",
    "\n",
    "    for i in range(n_steps):\n",
    "        t_i   = t_array[i]\n",
    "        t_ip1 = t_array[i+1]\n",
    "\n",
    "        \n",
    "        sigma_i = get_sigma_value(t_i, schedule, hp_min, hp_max)\n",
    "        if (sigma_i >= S_min) and (sigma_i <= S_max):\n",
    "            gamma_i = min(S_churn / n_steps, np.sqrt(2.0) - 1.0)\n",
    "        else:\n",
    "            gamma_i = 0.0\n",
    "\n",
    "        sigma_hat_i = sigma_i * (1.0 + gamma_i)\n",
    "\n",
    "        if sigma_hat_i > sigma_i:\n",
    "            delta_sig_sq = sigma_hat_i**2 - sigma_i**2\n",
    "            eps_i = rng.normal(scale=S_noise, size=x.shape)\n",
    "            x = x + np.sqrt(delta_sig_sq) * eps_i\n",
    "\n",
    "    \n",
    "        sigma_ip1 = get_sigma_value(t_ip1, schedule, hp_min, hp_max)\n",
    "\n",
    "        e_i = noise_fn(x, np.full((x.shape[0], 1), sigma_hat_i))\n",
    "\n",
    "        half_step = 0.5 * (sigma_ip1 - sigma_hat_i)\n",
    "        x_mid = x + half_step * e_i\n",
    "\n",
    "        t_mid = 0.5*(t_i + t_ip1)\n",
    "        sigma_mid = get_sigma_value(t_mid, schedule, hp_min, hp_max)\n",
    "\n",
    "        e_mid = noise_fn(x_mid, np.full((x.shape[0], 1), sigma_mid))\n",
    "\n",
    "        x = x + (sigma_ip1 - sigma_hat_i) * e_mid\n",
    "\n",
    "    return x\n",
    "\n",
    "def vesde_reverse_finalDPM_solver3_stoch(\n",
    "    sde_dict,\n",
    "    y0,\n",
    "    original_t0=1.0,\n",
    "    rev_total=1.0,\n",
    "    n_steps=20,\n",
    "    rng=None,\n",
    "    noise_fn=None,\n",
    "    S_churn=10.0,\n",
    "    S_min=0.05,\n",
    "    S_max=50.0,\n",
    "    S_noise=1.0,\n",
    "    schedule=\"karras_linear\"\n",
    "):\n",
    "    hp_min = sde_dict[\"hyperparam_min\"]\n",
    "    hp_max = sde_dict[\"hyperparam_max\"]\n",
    "\n",
    "    t_array = np.linspace(original_t0, original_t0 - rev_total, n_steps + 1)\n",
    "    x = y0.copy()\n",
    "\n",
    "    r1 = 1.0/3.0\n",
    "    r2 = 2.0/3.0\n",
    "\n",
    "    for i in range(n_steps):\n",
    "        t_i   = t_array[i]\n",
    "        t_ip1 = t_array[i+1]\n",
    "\n",
    "    \n",
    "        sigma_i = get_sigma_value(t_i, schedule, hp_min, hp_max)\n",
    "        if (sigma_i >= S_min) and (sigma_i <= S_max):\n",
    "            gamma_i = min(S_churn / n_steps, np.sqrt(2.0) - 1.0)\n",
    "        else:\n",
    "            gamma_i = 0.0\n",
    "\n",
    "        sigma_hat_i = sigma_i * (1.0 + gamma_i)\n",
    "\n",
    "        if sigma_hat_i > sigma_i:\n",
    "            delta_sig_sq = sigma_hat_i**2 - sigma_i**2\n",
    "            eps_i = rng.normal(scale=S_noise, size=x.shape)\n",
    "            x = x + np.sqrt(delta_sig_sq) * eps_i\n",
    "\n",
    "        \n",
    "        sigma_ip1 = get_sigma_value(t_ip1, schedule, hp_min, hp_max)\n",
    "        delta_sigma = (sigma_ip1 - sigma_hat_i)\n",
    "\n",
    "        e0 = noise_fn(x, np.full((x.shape[0], 1), sigma_hat_i))\n",
    "\n",
    "    \n",
    "        sigma_1 = sigma_hat_i + r1*delta_sigma  # partial sigma\n",
    "        half_step1 = r1 * delta_sigma\n",
    "        x1 = x + half_step1 * e0\n",
    "\n",
    "        e1 = noise_fn(x1, np.full((x1.shape[0], 1), sigma_1))\n",
    "\n",
    "        sigma_2 = sigma_hat_i + r2*delta_sigma\n",
    "        half_step2 = r2 * delta_sigma\n",
    "        x2 = x + half_step2 * e1\n",
    "\n",
    "        e2 = noise_fn(x2, np.full((x2.shape[0], 1), sigma_2))\n",
    "\n",
    "    \n",
    "        x = x + delta_sigma * e2\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 5) VESDE Reverse: Exponential Integration with Polynomial Noise\n",
    "###############################################################################\n",
    "def _trapezoid_integration(func, a, b, n=32):\n",
    "    flipped = False\n",
    "    if a > b:\n",
    "        a, b = b, a\n",
    "        flipped = True\n",
    "\n",
    "    x = np.linspace(a, b, n+1)\n",
    "    y = np.array([func(xi) for xi in x])\n",
    "    h = (b - a)/n\n",
    "    integral = 0.5 * h * (y[0] + 2.0*np.sum(y[1:-1]) + y[-1])\n",
    "    if flipped:\n",
    "        integral = -integral\n",
    "    return integral\n",
    "\n",
    "def _lagrange_basis_poly(tau, node_times, j):\n",
    "    num = 1.0\n",
    "    den = 1.0\n",
    "    t_j = node_times[j]\n",
    "    for m, t_m in enumerate(node_times):\n",
    "        if m == j:\n",
    "            continue\n",
    "        num *= (tau - t_m)\n",
    "        den *= (t_j - t_m)\n",
    "    return num / (den + 1e-40)\n",
    "\n",
    "def _precompute_coeffs_expint_poly(t_points, sde_dict, poly_order=1, n_substeps=64):\n",
    "    def trapz_integrate(func, a, b, n):\n",
    "        xs = np.linspace(a, b, n+1)\n",
    "        vals = [func(xv) for xv in xs]\n",
    "        h = (b - a)/n\n",
    "        return 0.5 * h * (vals[0] + 2.0*sum(vals[1:-1]) + vals[-1])\n",
    "\n",
    "    N = len(t_points) - 1\n",
    "    C_list = []\n",
    "\n",
    "    for i in range(N):\n",
    "        t_i  = t_points[i]\n",
    "        t_i1 = t_points[i+1]\n",
    "\n",
    "        node_times = np.linspace(t_i, t_i1, poly_order+1)\n",
    "\n",
    "        def sigma_tau(tau):\n",
    "            return exponential_schedule_get_value(tau,\n",
    "                                                  sde_dict[\"hyperparam_min\"],\n",
    "                                                  sde_dict[\"hyperparam_max\"])\n",
    "        def sigma_prime_tau(tau):\n",
    "            return exponential_schedule_get_derivative(tau,\n",
    "                                                       sde_dict[\"hyperparam_min\"],\n",
    "                                                       sde_dict[\"hyperparam_max\"])\n",
    "        def g_sq_over_sigma(tau):\n",
    "            s_val = sigma_tau(tau)\n",
    "            s_p   = sigma_prime_tau(tau)\n",
    "            g_sq  = 2.0 * s_val * s_p\n",
    "            return 0.5 * g_sq * (1.0/(s_val + 1e-40))\n",
    "\n",
    "        C_i = []\n",
    "        for j in range(poly_order+1):\n",
    "            def integrand_j(xv):\n",
    "                Lj_tau = _lagrange_basis_poly(xv, node_times, j)\n",
    "                return g_sq_over_sigma(xv) * Lj_tau\n",
    "            val_j = trapz_integrate(integrand_j, t_i, t_i1, n_substeps)\n",
    "            C_i.append(val_j)\n",
    "\n",
    "        C_list.append(C_i)\n",
    "\n",
    "    return C_list\n",
    "\n",
    "def vesde_reverse_exponential_integration_polynomial_noise(\n",
    "    sde_dict, \n",
    "    y0,          \n",
    "    original_t0, \n",
    "    rev_total,   \n",
    "    n_steps,  \n",
    "    noise_fn,    \n",
    "    poly_order=1,\n",
    "    seed=None,\n",
    "    y_scaler=None\n",
    "):\n",
    "    rng = np.random.default_rng(seed if seed is not None else 42)\n",
    "    if y0.shape[1] != 1:\n",
    "        raise ValueError(\"Exponential Integrator (poly) is univariate-only in this demo.\")\n",
    "\n",
    "    t_array = np.linspace(original_t0, original_t0 - rev_total, n_steps+1)\n",
    "    C_all = _precompute_coeffs_expint_poly(t_array, sde_dict, poly_order=poly_order, n_substeps=64)\n",
    "\n",
    "    y_values = np.empty((n_steps+1, y0.shape[0], 1))\n",
    "    y_values[0] = y0.copy()\n",
    "\n",
    "    eps_buffer = {}\n",
    "    x_init = y_values[0]\n",
    "    t_init_val = np.full((x_init.shape[0],1), t_array[0])\n",
    "    eps_buffer[0] = noise_fn(x_init, t_init_val)\n",
    "\n",
    "    for i in range(n_steps):\n",
    "        x_i = y_values[i]\n",
    "        t_i = t_array[i]\n",
    "        t_ip1 = t_array[i+1]\n",
    "        coeffs_i = C_all[i]\n",
    "        x_new = x_i.copy()\n",
    "\n",
    "        if (i+1) not in eps_buffer:\n",
    "            t_next_val = np.full((x_i.shape[0],1), t_ip1)\n",
    "            eps_buffer[i+1] = noise_fn(x_i, t_next_val)  \n",
    "\n",
    "        if poly_order == 1:\n",
    "            if (i+1) not in eps_buffer:\n",
    "                C0 = coeffs_i[0] + coeffs_i[1]\n",
    "                eps_i_ = eps_buffer[i]\n",
    "                x_new += C0 * eps_i_\n",
    "            else:\n",
    "                C0, C1 = coeffs_i[:2]\n",
    "                eps_i_ = eps_buffer[i]\n",
    "                eps_ip1_ = eps_buffer[i+1]\n",
    "                x_new += (C0 * eps_i_ + C1 * eps_ip1_)\n",
    "\n",
    "        elif poly_order == 2:\n",
    "            if i == 0:\n",
    "                if (i+1) not in eps_buffer:\n",
    "                    C0 = coeffs_i[0] + coeffs_i[1]\n",
    "                    eps_i_ = eps_buffer[i]\n",
    "                    x_new += C0 * eps_i_\n",
    "                else:\n",
    "                    C0, C1 = coeffs_i[:2]\n",
    "                    eps_i_ = eps_buffer[i]\n",
    "                    eps_ip1_ = eps_buffer[i+1]\n",
    "                    x_new += (C0 * eps_i_ + C1 * eps_ip1_)\n",
    "            else:\n",
    "                if (i+1) not in eps_buffer:\n",
    "                    C0 = coeffs_i[0] + coeffs_i[1]\n",
    "                    eps_i_ = eps_buffer[i]\n",
    "                    x_new += C0 * eps_i_\n",
    "                else:\n",
    "                    C0, C1, C2 = coeffs_i[:3]\n",
    "                    eps_im1_ = eps_buffer[i-1]\n",
    "                    eps_i_ = eps_buffer[i]\n",
    "                    eps_ip1_ = eps_buffer[i+1]\n",
    "                    x_new += (C0 * eps_i_ + C1 * eps_ip1_ + C2 * eps_im1_)\n",
    "\n",
    "        else:  \n",
    "            if i < 2:\n",
    "                if i == 1:\n",
    "                    if (i+1) not in eps_buffer:\n",
    "                        C0 = coeffs_i[0] + coeffs_i[1]\n",
    "                        eps_i_ = eps_buffer[i]\n",
    "                        x_new += C0 * eps_i_\n",
    "                    else:\n",
    "                        if len(coeffs_i) < 3:\n",
    "                            C0 = coeffs_i[0] + coeffs_i[1]\n",
    "                            eps_i_ = eps_buffer[i]\n",
    "                            x_new += C0 * eps_i_\n",
    "                        else:\n",
    "                            C0, C1, C2 = coeffs_i[:3]\n",
    "                            eps_im1_ = eps_buffer[i-1]\n",
    "                            eps_i_ = eps_buffer[i]\n",
    "                            eps_ip1_ = eps_buffer[i+1]\n",
    "                            x_new += (C0*eps_i_ + C1*eps_ip1_ + C2*eps_im1_)\n",
    "                else:  \n",
    "                    if (i+1) not in eps_buffer:\n",
    "                        C0 = coeffs_i[0] + coeffs_i[1]\n",
    "                        eps_i_ = eps_buffer[i]\n",
    "                        x_new += C0 * eps_i_\n",
    "                    else:\n",
    "                        C0, C1 = coeffs_i[:2]\n",
    "                        eps_i_ = eps_buffer[i]\n",
    "                        eps_ip1_ = eps_buffer[i+1]\n",
    "                        x_new += (C0*eps_i_ + C1*eps_ip1_)\n",
    "            else:\n",
    "                if (i+1) not in eps_buffer:\n",
    "                    if len(coeffs_i) < 3:\n",
    "                        C0 = coeffs_i[0] + coeffs_i[1]\n",
    "                        eps_i_ = eps_buffer[i]\n",
    "                        x_new += C0 * eps_i_\n",
    "                    else:\n",
    "                        C0, C1, C2 = coeffs_i[:3]\n",
    "                        eps_im1_ = eps_buffer[i-1]\n",
    "                        eps_i_ = eps_buffer[i]\n",
    "                        x_new += (C0*eps_i_ + C1*eps_i_ + C2*eps_im1_)\n",
    "                else:\n",
    "                    if len(coeffs_i) < 4:\n",
    "                        C0, C1, C2 = coeffs_i[:3]\n",
    "                        eps_im1_ = eps_buffer[i-1]\n",
    "                        eps_i_ = eps_buffer[i]\n",
    "                        eps_ip1_ = eps_buffer[i+1]\n",
    "                        x_new += (C0*eps_i_ + C1*eps_ip1_ + C2*eps_im1_)\n",
    "                    else:\n",
    "                        C0, C1, C2, C3 = coeffs_i[:4]\n",
    "                        eps_im2_ = eps_buffer[i-2]\n",
    "                        eps_im1_ = eps_buffer[i-1]\n",
    "                        eps_i_ = eps_buffer[i]\n",
    "                        eps_ip1_ = eps_buffer[i+1]\n",
    "                        x_new += (\n",
    "                            C0*eps_i_\n",
    "                            + C1*eps_ip1_\n",
    "                            + C2*eps_im1_\n",
    "                            + C3*eps_im2_\n",
    "                        )\n",
    "\n",
    "        y_values[i+1] = x_new\n",
    "        t_next_arr = np.full((x_new.shape[0],1), t_ip1)\n",
    "        eps_buffer[i+1] = noise_fn(x_new, t_next_arr)\n",
    "\n",
    "    result = y_values[-1]\n",
    "    \n",
    "    if y_scaler is not None:\n",
    "        result_flat = result.reshape(-1, 1)\n",
    "        result_original = y_scaler.inverse_transform(result_flat)\n",
    "        result = result_original.reshape(result.shape)\n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 7) Stochastic sampler (k-diff approach)\n",
    "###############################################################################\n",
    "def vesde_reverse_stochsampler_once(\n",
    "    sde_dict,\n",
    "    y0,\n",
    "    n_steps=50,\n",
    "    noise_fn=None,\n",
    "    S_churn=10.0,\n",
    "    S_min=0.05,\n",
    "    S_max=50.0,\n",
    "    S_noise=1.0,\n",
    "):\n",
    "    if noise_fn is None:\n",
    "        raise ValueError(\"noise_fn must be provided.\")\n",
    "    \n",
    "    rng = np.random.default_rng()\n",
    "    t_array = np.linspace(1.0, 0.0, n_steps + 1)\n",
    "    x = y0.copy()\n",
    "\n",
    "    def denoiser_fn(x_, t_):\n",
    "        pred_noise = noise_fn(x_, t_)\n",
    "        return x_ - t_ * pred_noise\n",
    "\n",
    "    for i in range(n_steps):\n",
    "        t_i = t_array[i]\n",
    "        t_ip1 = t_array[i + 1]\n",
    "\n",
    "        if S_min <= t_i <= S_max:\n",
    "            gamma_i = min(S_churn / n_steps, np.sqrt(2.0) - 1.0)\n",
    "        else:\n",
    "            gamma_i = 0.0\n",
    "\n",
    "        eps_i = rng.normal(size=x.shape, scale=S_noise)\n",
    "        t_hat_i = t_i * (1.0 + gamma_i)\n",
    "        sigma_delta = np.sqrt(np.maximum(t_hat_i**2 - t_i**2, 0.0))\n",
    "        hat_x_i = x + sigma_delta * eps_i\n",
    "\n",
    "        t_hat_arr = np.full((hat_x_i.shape[0],1), t_hat_i)\n",
    "        d_i = (hat_x_i - denoiser_fn(hat_x_i, t_hat_arr)) / (t_hat_i+1e-40)\n",
    "\n",
    "        x_next = hat_x_i + (t_ip1 - t_hat_i) * d_i\n",
    "        if t_ip1 > 1e-14:\n",
    "            t_ip1_arr = np.full((hat_x_i.shape[0],1), t_ip1)\n",
    "            d_i_prime = (x_next - denoiser_fn(x_next, t_ip1_arr)) / (t_ip1 + 1e-40)\n",
    "            x_next = hat_x_i + (t_ip1 - t_hat_i) * (0.5 * d_i + 0.5 * d_i_prime)\n",
    "\n",
    "        x = x_next\n",
    "    \n",
    "    return x\n",
    "\n",
    "###############################################################################\n",
    "# 8) Unified SDE Integration Interface (modified to remove n_samples)\n",
    "###############################################################################\n",
    "def sdeint(\n",
    "    sde_dict, \n",
    "    y0, \n",
    "    t0=0.0, \n",
    "    t1=1.0, \n",
    "    method=\"euler\", \n",
    "    n_steps=20, \n",
    "    score_fn=None, \n",
    "    seed=None,\n",
    "    y_scaler=None,\n",
    "    time_budget=10.0,\n",
    "    S_churn=10.0,\n",
    "    S_min=0.05,\n",
    "    S_max=50.0,\n",
    "    S_noise=1.0,\n",
    "    schedule=\"exponential\"\n",
    "):\n",
    "  \n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    if t1 >= t0:\n",
    "        # forward pass\n",
    "        if method == \"euler\":\n",
    "            samples = euler_maruyama_integration(sde_dict, y0, t0, t1, n_steps, rng) \n",
    "        elif method == \"expint\":\n",
    "            raise ValueError(\"'expint' is only for reverse-time in this demo.\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method {method} for forward SDE.\")\n",
    "    else:\n",
    "        # reverse pass\n",
    "        if score_fn is None:\n",
    "            raise ValueError(\"score_fn required for reverse pass.\")\n",
    "        rev_total = t0 - t1\n",
    "        if method==\"euler\":\n",
    "            samples = euler_maruyama_reverse_integration(sde_dict, y0, t0, rev_total, n_steps, rng, score_fn)\n",
    "        elif method==\"dpm\":\n",
    "            samples = vesde_reverse_finalDPM(sde_dict, y0, t0, rev_total, n_steps, rng, score_fn)\n",
    "        elif method==\"pndm\":\n",
    "            samples = pndm_vpsde_integration(sde_dict, y0, n_steps, score_fn, rng, y_scaler=y_scaler)\n",
    "        elif method==\"expint\":\n",
    "            samples = vesde_reverse_exponential_integration_polynomial_noise(\n",
    "                sde_dict, y0, t0, rev_total, n_steps, score_fn, poly_order=3, y_scaler=y_scaler\n",
    "            )\n",
    "        elif method == 'unipc':\n",
    "            samples = unipc_vpsde_integration(\n",
    "                sde_dict=sde_dict,\n",
    "                y0=y0,\n",
    "                n_steps=n_steps,\n",
    "                score_fn=score_fn,\n",
    "                rng=rng,\n",
    "                p=2\n",
    "            )\n",
    "        elif method == \"stochsampler\":\n",
    "            samples = vesde_reverse_stochsampler_once(\n",
    "                sde_dict=sde_dict,\n",
    "                y0=y0,\n",
    "                n_steps=n_steps,\n",
    "                noise_fn=score_fn,\n",
    "                y_scaler=y_scaler,\n",
    "                seed=seed\n",
    "            )\n",
    "        elif method==\"dpm_stoch\":\n",
    "            samples = vesde_reverse_finalDPM_stoch(\n",
    "                sde_dict,\n",
    "                y0,\n",
    "                original_t0=t0,\n",
    "                rev_total=rev_total,\n",
    "                n_steps=n_steps,\n",
    "                rng=rng,\n",
    "                noise_fn=score_fn,\n",
    "                S_churn=S_churn,\n",
    "                S_min=S_min,\n",
    "                S_max=S_max,\n",
    "                S_noise=S_noise,\n",
    "                schedule=schedule\n",
    "            )\n",
    "        elif method==\"dpm_solver_2_stoch\":\n",
    "            samples = vesde_reverse_finalDPM_solver2_stoch(\n",
    "                sde_dict,\n",
    "                y0,\n",
    "                original_t0=t0,\n",
    "                rev_total=rev_total,\n",
    "                n_steps=n_steps,\n",
    "                rng=rng,\n",
    "                noise_fn=score_fn,\n",
    "                S_churn=S_churn,\n",
    "                S_min=S_min,\n",
    "                S_max=S_max,\n",
    "                S_noise=S_noise,\n",
    "                schedule=schedule\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method {method} for reverse SDE\")\n",
    "    \n",
    "    return samples\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 9) OU Process\n",
    "###############################################################################\n",
    "def create_ou_sde():\n",
    "    \n",
    "    def drift_and_diffusion_ou(y, t):\n",
    "        drift = -y\n",
    "        diffusion = np.sqrt(2.0)*np.ones_like(y)\n",
    "        return drift, diffusion\n",
    "    \n",
    "    def sample_prior_ou(shape, seed=None):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        return rng.normal(0,1,size=shape)\n",
    "\n",
    "    def get_mean_std_ou(y0,t):\n",
    "        t_ = t.reshape(-1)\n",
    "        e_neg_t=np.exp(-t_)\n",
    "        e_neg_2t=np.exp(-2*t_)\n",
    "        mean=(y0.T*e_neg_t).T\n",
    "        std = np.sqrt(1.0 - e_neg_2t)\n",
    "        std = np.broadcast_to(std.reshape(-1,1), y0.shape)\n",
    "        return mean,std\n",
    "\n",
    "    def init_from_data(y0):\n",
    "        return 0.0,1.0\n",
    "\n",
    "    return {\n",
    "        \"name\":\"ou\",\n",
    "        \"drift_and_diffusion\":drift_and_diffusion_ou,\n",
    "        \"sample_prior\":sample_prior_ou,\n",
    "        \"get_mean_std\":get_mean_std_ou,\n",
    "        \"init_from_data\":init_from_data\n",
    "    }\n",
    "\n",
    "###############################################################################\n",
    "# 10) Underdamped Langevin (used as corrector step in OU)\n",
    "###############################################################################\n",
    "def underdamped_langevin_step(y, v, dt, gamma, rng, score_fn, t_local):\n",
    "    y_new = y + dt*v\n",
    "    s_val = score_fn(y_new, t_local)\n",
    "    noise = rng.normal(size=v.shape)\n",
    "    v_new = v + dt*(-gamma*v - s_val) + np.sqrt(2.0*gamma*dt)*noise\n",
    "    return y_new, v_new\n",
    "\n",
    "###############################################################################\n",
    "# 11) OU Reverse Solvers\n",
    "###############################################################################\n",
    "def ou_reverse_step_exponential(y, dt, rng, score_fn, t_local):\n",
    "    exp_dt=np.exp(dt)\n",
    "    s_val=score_fn(y, t_local)\n",
    "    return exp_dt*y + (exp_dt-1.0)*s_val\n",
    "\n",
    "def ou_reverse_step_midpoint(y, dt, rng, alpha, score_fn, t_n, t_n_alpha):\n",
    "    exp_alpha_dt=np.exp(alpha*dt)\n",
    "    s_tn=score_fn(y, t_n)\n",
    "    y_half=exp_alpha_dt*y + (exp_alpha_dt-1.0)*s_tn\n",
    "\n",
    "    exp_dt=np.exp(dt)\n",
    "    s_tn_alpha = score_fn(y_half, t_n_alpha)\n",
    "    factor= dt*np.exp((1.0-alpha)*dt)\n",
    "    return exp_dt*y + factor*s_tn_alpha\n",
    "\n",
    "def ou_reverse_integration_once(\n",
    "    sde_dict,\n",
    "    y0,\n",
    "    original_t0,\n",
    "    rev_total,\n",
    "    n_steps,\n",
    "    rng,\n",
    "    score_fn,\n",
    "    gamma,\n",
    "    corrector_steps,\n",
    "    dt_corr,\n",
    "    predictor_method=\"exponential\",\n",
    "    y_scaler=None \n",
    "):\n",
    "    dt = rev_total/n_steps\n",
    "    y = y0.copy()\n",
    "    v = np.zeros_like(y)\n",
    "    t_rev = 0.0\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "        forward_t = original_t0 - t_rev\n",
    "        t_arr = np.full((y.shape[0], 1), forward_t)\n",
    "\n",
    "        if predictor_method == \"exponential\":\n",
    "            y = ou_reverse_step_exponential(y, dt, rng, score_fn, t_arr)\n",
    "        elif predictor_method == \"random_midpoint\":\n",
    "            alpha = rng.uniform()\n",
    "            t_n_alpha = np.full((y.shape[0], 1), forward_t + alpha*dt)\n",
    "            y = ou_reverse_step_midpoint(y, dt, rng, alpha, score_fn, t_arr, t_n_alpha)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown predictor method '{predictor_method}'\")\n",
    "\n",
    "        t_rev += dt\n",
    "        for _corr in range(corrector_steps):\n",
    "            y, v = underdamped_langevin_step(y, v, dt_corr, gamma, rng, score_fn, t_arr)\n",
    "    \n",
    "    \n",
    "    if y_scaler is not None:\n",
    "        y_flat = y.reshape(-1, 1)\n",
    "        y_original = y_scaler.inverse_transform(y_flat)\n",
    "        y = y_original.reshape(y.shape)\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "def sdeint_ou_reverse(\n",
    "    sde_dict,\n",
    "    y0,\n",
    "    t0=1.0,\n",
    "    t1=0.0,\n",
    "    n_steps=20,\n",
    "    predictor_method=\"exponential\",\n",
    "    gamma=0.1,\n",
    "    corrector_steps=1,\n",
    "    dt_corr=0.001,\n",
    "    score_fn=None,\n",
    "    seed=None,\n",
    "    y_scaler=None\n",
    "):\n",
    "   \n",
    "    if score_fn is None:\n",
    "        raise ValueError(\"score_fn required for reverse OU pass.\")\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rev_total = t0 - t1\n",
    "    return ou_reverse_integration_once(\n",
    "        sde_dict,\n",
    "        y0,\n",
    "        t0,\n",
    "        rev_total,\n",
    "        n_steps,\n",
    "        rng,\n",
    "        score_fn,\n",
    "        gamma,\n",
    "        corrector_steps,\n",
    "        dt_corr,\n",
    "        predictor_method\n",
    "    )\n",
    "\n",
    "###############################################################################\n",
    "# 12) Data Scaling and Sample Classes\n",
    "###############################################################################\n",
    "class ScalerMixedTypes:\n",
    "    def __init__(self, categorical_columns=None):\n",
    "        self._scaler = StandardScaler()\n",
    "        self._cat_cols = categorical_columns if categorical_columns is not None else []\n",
    "        self._label_encoders = {}\n",
    "        self._category_maps = {}\n",
    "        self._is_fitted = False\n",
    "\n",
    "    def fit(self, X):\n",
    "        if self._cat_cols is None:\n",
    "            self._cat_cols = []\n",
    "        self._num_cols = [col for col in X.columns if col not in self._cat_cols]\n",
    "        \n",
    "        if len(self._num_cols) > 0:\n",
    "            self._scaler.fit(X[self._num_cols])\n",
    "            \n",
    "        for col in self._cat_cols:\n",
    "            unique_cats = X[col].unique()\n",
    "            self._category_maps[col] = {cat: idx for idx,cat in enumerate(unique_cats)}\n",
    "            \n",
    "        self._is_fitted = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if not self._is_fitted:\n",
    "            raise ValueError(\"Scaler not fitted.\")\n",
    "        X_out = X.copy()\n",
    "        if len(self._num_cols) > 0:\n",
    "            X_out[self._num_cols] = self._scaler.transform(X[self._num_cols])\n",
    "        for col in self._cat_cols:\n",
    "            X_out[col] = X[col].map(lambda x: self._category_maps[col].get(x,-1))\n",
    "        return X_out\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        return self.fit(X).transform(X)\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        if not self._is_fitted:\n",
    "            raise ValueError(\"Scaler not fitted.\")\n",
    "        X_out = X.copy()\n",
    "        if len(self._num_cols) > 0:\n",
    "            X_out[self._num_cols] = self._scaler.inverse_transform(X[self._num_cols])\n",
    "            \n",
    "        for col in self._cat_cols:\n",
    "            reverse_map = {v: k for k,v in self._category_maps[col].items()}\n",
    "            X_out[col] = X[col].map(lambda x: reverse_map.get(x, 'unknown'))\n",
    "        return X_out\n",
    "\n",
    "class Samples:\n",
    "    def __init__(self, arr):\n",
    "        if arr.ndim < 2 or arr.ndim > 3:\n",
    "            raise ValueError(\"Samples must have 2 or 3 dims.\")\n",
    "        self._samples = arr\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self._samples.shape\n",
    "\n",
    "    def to_numpy(self):\n",
    "        return self._samples\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 13) Multi-step UniPC for VP-SDE (one sample version)\n",
    "###############################################################################\n",
    "def _compute_alpha_bar_lin(u, hyper_min, hyper_max):\n",
    "    a = hyper_min\n",
    "    b = hyper_max - hyper_min\n",
    "    val = a*u + 0.5*b*(u**2)\n",
    "    return np.exp(-val)\n",
    "\n",
    "def unipc_vpsde_integration(\n",
    "    sde_dict,\n",
    "    y0,\n",
    "    n_steps,\n",
    "    score_fn,\n",
    "    rng=None,\n",
    "    p=2,\n",
    "    y_scaler=None\n",
    "):\n",
    "    \n",
    "    hyper_min = sde_dict[\"hyperparam_min\"]\n",
    "    hyper_max = sde_dict[\"hyperparam_max\"]\n",
    "    \n",
    "    current_y0 = y0.copy()\n",
    "    \n",
    "    dt = 1.0\n",
    "    t_now = float(n_steps)\n",
    "\n",
    "    def alpha_bar_fn(step_idx):\n",
    "        return _compute_alpha_bar_lin(step_idx / n_steps, hyper_min, hyper_max)\n",
    "\n",
    "    def alpha_fn(step_idx):\n",
    "        return np.sqrt(alpha_bar_fn(step_idx))\n",
    "\n",
    "    def sigma_fn(step_idx):\n",
    "        return np.sqrt(1.0 - alpha_bar_fn(step_idx))\n",
    "\n",
    "    def lambda_fn(step_idx):\n",
    "        return -np.log(alpha_bar_fn(step_idx) + 1e-40)\n",
    "\n",
    "    x = current_y0.copy()\n",
    "    eps_buffer = {}\n",
    "\n",
    "    ab_init = alpha_bar_fn(t_now)\n",
    "    score_init = score_fn(x, np.full((x.shape[0],1), t_now/n_steps))\n",
    "    eps_buffer[t_now] = score_init\n",
    "\n",
    "    for step_i in range(n_steps):\n",
    "        t_im1 = t_now\n",
    "        t_i = t_now - dt\n",
    "\n",
    "        p_i = min(p, step_i+1)\n",
    "        \n",
    "        lam_i = lambda_fn(t_i)\n",
    "        lam_im1 = lambda_fn(t_im1)\n",
    "        h_i = lam_i - lam_im1\n",
    "\n",
    "        alpha_im1 = alpha_fn(t_im1)\n",
    "        alpha_i = alpha_fn(t_i)\n",
    "        sigma_i = sigma_fn(t_i)\n",
    "        e_hi_minus_1 = np.exp(h_i) - 1.0\n",
    "        eps_im1 = eps_buffer[t_im1]\n",
    "        \n",
    "        x_tilde_1 = (alpha_i/alpha_im1)*x - sigma_i*e_hi_minus_1*eps_im1\n",
    "\n",
    "        if p_i == 1:\n",
    "            x_c = x_tilde_1\n",
    "        else:\n",
    "            t_im2 = t_im1 + dt\n",
    "            \n",
    "            if t_im2 in eps_buffer:\n",
    "                eps_im2 = eps_buffer[t_im2]\n",
    "            else:\n",
    "                eps_im2 = eps_im1\n",
    "\n",
    "            x_tilde = x_tilde_1\n",
    "            eps_i_val = score_fn(x_tilde, np.full((x.shape[0],1), t_i/n_steps))\n",
    "\n",
    "            D2 = eps_i_val - eps_im1\n",
    "            lam_im2 = lambda_fn(t_im2)\n",
    "            r_1 = (lam_im2 - lam_i)/ (h_i + 1e-40)\n",
    "            D1 = eps_im2 - eps_im1\n",
    "\n",
    "            B_h = e_hi_minus_1\n",
    "            R_mat = np.array([\n",
    "                [1, 1],\n",
    "                [r_1*h_i, h_i]\n",
    "            ], dtype=np.float64)\n",
    "\n",
    "            phi_vec = compute_phi_p(2, h_i)\n",
    "            inv_R = np.linalg.inv(R_mat)\n",
    "            a_vec = (inv_R @ phi_vec) / B_h\n",
    "\n",
    "            c_term = a_vec[0]*(D1/(r_1+1e-40)) + a_vec[1]*D2\n",
    "            x_c = x_tilde_1 - sigma_i*B_h*c_term\n",
    "\n",
    "        x = x_c\n",
    "\n",
    "        eps_i_corrected = score_fn(x, np.full((x.shape[0],1), t_i/n_steps))\n",
    "        eps_buffer[t_i] = eps_i_corrected\n",
    "        t_now = t_i\n",
    "\n",
    "    return x.reshape(-1, 1)\n",
    "\n",
    "def compute_phi_p(p, z):\n",
    "    def varphi_0(z): \n",
    "        return np.exp(z)\n",
    "    \n",
    "    def varphi_next(vn, n, z):\n",
    "        return (vn - 1.0/math.factorial(n))/z\n",
    "    \n",
    "    def compute_varphis_up_to(p, z):\n",
    "        out = [varphi_0(z)]\n",
    "        for i in range(p):\n",
    "            out.append(varphi_next(out[-1], i, z))\n",
    "        return out\n",
    "\n",
    "    v_all = compute_varphis_up_to(p+1, z)\n",
    "    phi = []\n",
    "    for n in range(1, p+1):\n",
    "        f_n = (z**n)*math.factorial(n)*v_all[n] \n",
    "        phi.append(f_n)\n",
    "    return np.array(phi)\n",
    "\n",
    "###############################################################################\n",
    "# 14) LightGBM-based Score/Noise Model\n",
    "###############################################################################\n",
    "def make_training_data(X, y, sde_dict, n_repeats=10, eval_percent=0.1, seed=None):\n",
    "    if eval_percent>0:\n",
    "        X_tr, X_val, y_tr, y_val = train_test_split(X,y, test_size=eval_percent, random_state=seed)\n",
    "    else:\n",
    "        X_tr, y_tr = X,y\n",
    "        X_val, y_val = None, None\n",
    "\n",
    "    X_tr = np.tile(X_tr, (n_repeats,1))\n",
    "    y_tr = np.tile(y_tr, (n_repeats,1))\n",
    "\n",
    "    rng= np.random.default_rng(seed)\n",
    "    t_tr= rng.uniform(0,1,size=(X_tr.shape[0],1))\n",
    "    z_tr= rng.normal(size=y_tr.shape)\n",
    "\n",
    "    m_tr, s_tr= sde_dict[\"get_mean_std\"](y_tr, t_tr)\n",
    "    pert_y_tr= m_tr+ s_tr*z_tr\n",
    "    feats_tr= np.hstack([ pert_y_tr, X_tr, t_tr])\n",
    "    labels_tr= -z_tr\n",
    "\n",
    "    feats_val, labels_val= None, None\n",
    "    if X_val is not None and y_val is not None:\n",
    "        t_val= rng.uniform(0,1,size=(X_val.shape[0],1))\n",
    "        z_val= rng.normal(size=y_val.shape)\n",
    "        m_val, s_val= sde_dict[\"get_mean_std\"](y_val, t_val)\n",
    "        pert_y_val= m_val+ s_val*z_val\n",
    "        feats_val= np.hstack([ pert_y_val, X_val, t_val])\n",
    "        labels_val= -z_val\n",
    "    return feats_tr, labels_tr, feats_val, labels_val\n",
    "\n",
    "def train_lightgbm_model(X, y, X_val=None, y_val=None, seed=None, **lgbm_args):\n",
    "    model = lgb.LGBMRegressor(random_state=seed, verbose=-1, **lgbm_args)\n",
    "    if X_val is not None and y_val is not None:\n",
    "        model.fit(X, y, eval_set=[(X_val,y_val)])\n",
    "    else:\n",
    "        model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "def predict_score_lightgbm(models, sde_dict, y, X, t):\n",
    "\n",
    "    if y.ndim == 1:\n",
    "        y = y.reshape(-1, 1)\n",
    "    \n",
    "    if X.shape[0] != y.shape[0]:\n",
    "        factor = y.shape[0] // X.shape[0]\n",
    "        X = np.tile(X, (factor, 1))\n",
    "\n",
    "    feats = np.hstack([y, X, t])\n",
    "    _, s = sde_dict[\"get_mean_std\"](y, t)\n",
    "    s = np.maximum(s, 1e-6)\n",
    "    \n",
    "    scores = []\n",
    "    for i, mdl in enumerate(models):\n",
    "        pred_i = mdl.predict(feats)\n",
    "        scores.append(pred_i / s[:, i])\n",
    "    return np.column_stack(scores)\n",
    "\n",
    "def predict_noise_lightgbm_batched(models, sde_dict, y, X, t, batch_size=1000):\n",
    "   \n",
    "    if y.ndim > 2:\n",
    "        y = y.reshape(-1, y.shape[-1])\n",
    "    if t.ndim == 1:\n",
    "        t = t.reshape(-1, 1)\n",
    "        \n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = X.to_numpy()\n",
    "    \n",
    "    if X.shape[0] != y.shape[0]:\n",
    "        repeat_factor = y.shape[0] // X.shape[0]\n",
    "        if y.shape[0] % X.shape[0] != 0:\n",
    "            repeat_factor += 1\n",
    "        X = np.tile(X, (repeat_factor, 1))[:y.shape[0]]\n",
    "    \n",
    "    if t.shape[0] != y.shape[0]:\n",
    "        t = np.broadcast_to(t, (y.shape[0], t.shape[1]))\n",
    "\n",
    "    n_samples = y.shape[0]\n",
    "    n_outputs = len(models)\n",
    "    noises = np.zeros((n_samples, n_outputs))\n",
    "    \n",
    "    for i in range(0, n_samples, batch_size):\n",
    "        batch_end = min(i + batch_size, n_samples)\n",
    "        batch_y = y[i:batch_end]\n",
    "        batch_X = X[i:batch_end]\n",
    "        batch_t = t[i:batch_end]\n",
    "        \n",
    "        feats = np.hstack([batch_y, batch_X, batch_t])\n",
    "        \n",
    "        for j, mdl in enumerate(models):\n",
    "            pred_j = mdl.predict(feats)\n",
    "            noises[i:batch_end, j] = -pred_j\n",
    "            \n",
    "    return noises\n",
    "\n",
    "def fit_treeffuser(X, y, sde_dict, n_repeats=10, eval_percent=0.1, seed=None, **lgbm_args):\n",
    "    feats_tr, labels_tr, feats_val, labels_val= make_training_data(\n",
    "        X,y, sde_dict, n_repeats, eval_percent, seed\n",
    "    )\n",
    "    y_dim= y.shape[1]\n",
    "    models=[]\n",
    "    for i in range(y_dim):\n",
    "        y_val_i= labels_val[:,i] if labels_val is not None else None\n",
    "        model_i= train_lightgbm_model(\n",
    "            feats_tr, labels_tr[:,i],\n",
    "            feats_val, y_val_i,\n",
    "            seed=seed,\n",
    "            **lgbm_args\n",
    "        )\n",
    "        models.append(model_i)\n",
    "    return models\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 15) The 8 sampling functions (now time-based instead of n_samples)\n",
    "###############################################################################\n",
    "\n",
    "def sample_treeffuser(\n",
    "    X,\n",
    "    sde_dict,\n",
    "    models,\n",
    "    x_scaler,\n",
    "    y_scaler,\n",
    "    time_budget=10.0, \n",
    "    n_steps=50,\n",
    "    seed=None\n",
    "):\n",
    "    start_t = time.time()\n",
    "    rng = np.random.default_rng(seed)\n",
    "    all_samples = []\n",
    "\n",
    "    def score_fn(y_local, t_local):\n",
    "        return predict_score_lightgbm(models, sde_dict, y_local, X, t_local)\n",
    "\n",
    "    sample_idx = 0\n",
    "    while True:\n",
    "        elapsed = time.time() - start_t\n",
    "        if elapsed >= time_budget:\n",
    "            break\n",
    "\n",
    "        shape = (1, X.shape[0], 1)\n",
    "        y0 = sde_dict[\"sample_prior\"](shape, seed=None)  # next random draw\n",
    "        y0_flat = y0.reshape(shape[0]*shape[1], shape[2])\n",
    "\n",
    "        single_sample = sdeint(\n",
    "            sde_dict,\n",
    "            y0_flat,\n",
    "            t0=1.0,\n",
    "            t1=0.0,\n",
    "            method=\"euler\",\n",
    "            n_steps=n_steps,\n",
    "            score_fn=score_fn,\n",
    "            seed=rng.integers(1e9)  \n",
    "        )\n",
    "        single_sample_3d = single_sample.reshape((1, X.shape[0], 1))\n",
    "\n",
    "        if y_scaler:\n",
    "            single_sample_3d[0] = y_scaler.inverse_transform(single_sample_3d[0])\n",
    "\n",
    "        all_samples.append(single_sample_3d)\n",
    "        sample_idx += 1\n",
    "\n",
    "    if len(all_samples)==0:\n",
    "        return np.empty((0, X.shape[0], 1))\n",
    "\n",
    "    return np.concatenate(all_samples, axis=0)\n",
    "\n",
    "def sample_treeffuser_unipc(\n",
    "    X,\n",
    "    sde_dict,\n",
    "    models,\n",
    "    x_scaler,\n",
    "    y_scaler,\n",
    "    time_budget=10.0,\n",
    "    n_steps=20,\n",
    "    seed=None,\n",
    "    p=2\n",
    "):\n",
    "   \n",
    "    start_t = time.time()\n",
    "    rng = np.random.default_rng(seed)\n",
    "    all_samples = []\n",
    "\n",
    "    def score_fn(y_local, t_local):\n",
    "        return predict_noise_lightgbm_batched(models, sde_dict, y_local, X, t_local)\n",
    "    \n",
    "    sample_idx = 0\n",
    "    while True:\n",
    "        elapsed = time.time() - start_t\n",
    "        if elapsed >= time_budget:\n",
    "            break\n",
    "\n",
    "        shape = (1, X.shape[0], 1)\n",
    "        y0 = sde_dict[\"sample_prior\"](shape, seed=None)\n",
    "        y0_flat = y0.reshape(shape[0]*shape[1], shape[2])\n",
    "\n",
    "        single_sample = unipc_vpsde_integration(\n",
    "            sde_dict,\n",
    "            y0_flat,\n",
    "            n_steps=n_steps,\n",
    "            score_fn=score_fn,\n",
    "            rng=rng,\n",
    "            p=p\n",
    "        )\n",
    "        \n",
    "        single_sample_3d = single_sample.reshape(1, X.shape[0], 1)\n",
    "        if y_scaler:\n",
    "            single_sample_3d[0] = y_scaler.inverse_transform(single_sample_3d[0])\n",
    "        all_samples.append(single_sample_3d)\n",
    "        sample_idx += 1\n",
    "\n",
    "    if len(all_samples) == 0:\n",
    "        return np.empty((0, X.shape[0], 1))\n",
    "    \n",
    "    return np.concatenate(all_samples, axis=0)\n",
    "\n",
    "\n",
    "def sample_treeffuser_dpm(\n",
    "    X,\n",
    "    sde_dict,\n",
    "    models,\n",
    "    x_scaler,\n",
    "    y_scaler,\n",
    "    time_budget=10.0,\n",
    "    n_steps=20,\n",
    "    seed=None\n",
    "):\n",
    "    \n",
    "    start_t = time.time()\n",
    "    rng = np.random.default_rng(seed)\n",
    "    all_samples = []\n",
    "\n",
    "    def score_fn(y_local, t_local):\n",
    "        return predict_noise_lightgbm_batched(models, sde_dict, y_local, X, t_local)\n",
    "\n",
    "    sample_idx = 0\n",
    "    while True:\n",
    "        elapsed = time.time() - start_t\n",
    "        if elapsed >= time_budget:\n",
    "            break\n",
    "\n",
    "        shape = (1, X.shape[0], 1)\n",
    "        y0 = sde_dict[\"sample_prior\"](shape, seed=None)\n",
    "        y0_flat = y0.reshape(shape[0]*shape[1], shape[2])\n",
    "\n",
    "        single_sample = sdeint(\n",
    "            sde_dict,\n",
    "            y0_flat,\n",
    "            t0=1.0,\n",
    "            t1=0.0,\n",
    "            method=\"dpm\",\n",
    "            n_steps=n_steps,\n",
    "            score_fn=score_fn,\n",
    "            seed=rng.integers(1e9)\n",
    "        )\n",
    "        single_sample_3d = single_sample.reshape((1, X.shape[0], 1))\n",
    "        if y_scaler:\n",
    "            single_sample_3d[0] = y_scaler.inverse_transform(single_sample_3d[0])\n",
    "        all_samples.append(single_sample_3d)\n",
    "        sample_idx += 1\n",
    "\n",
    "    if len(all_samples)==0:\n",
    "        return np.empty((0, X.shape[0], 1))\n",
    "\n",
    "    return np.concatenate(all_samples, axis=0)\n",
    "\n",
    "def sample_treeffuser_dpm_stoch(\n",
    "    X,\n",
    "    sde_dict,\n",
    "    models,\n",
    "    x_scaler,\n",
    "    y_scaler,\n",
    "    time_budget=10.0,\n",
    "    n_steps=20,\n",
    "    seed=None,\n",
    "    S_churn=10.0,\n",
    "    S_min=0.05,\n",
    "    S_max=50.0,\n",
    "    S_noise=1.0,\n",
    "    schedule=\"exponential\"\n",
    "):\n",
    "   \n",
    "    start_t = time.time()\n",
    "    rng = np.random.default_rng(seed)\n",
    "    all_samples = []\n",
    "\n",
    "    def score_fn(y_local, t_local):\n",
    "        return predict_noise_lightgbm_batched(models, sde_dict, y_local, X, t_local)\n",
    "\n",
    "    sample_idx = 0\n",
    "    while True:\n",
    "        elapsed = time.time() - start_t\n",
    "        if elapsed >= time_budget:\n",
    "            break\n",
    "\n",
    "        shape = (1, X.shape[0], 1)\n",
    "        y0 = sde_dict[\"sample_prior\"](shape, seed=rng.integers(1e9))\n",
    "        y0_flat = y0.reshape(shape[0]*shape[1], shape[2])\n",
    "\n",
    "        single_sample = sdeint(\n",
    "            sde_dict,\n",
    "            y0_flat,\n",
    "            t0=1.0,\n",
    "            t1=0.0,\n",
    "            method=\"dpm_stoch\",  \n",
    "            n_steps=n_steps,\n",
    "            score_fn=score_fn,\n",
    "            seed=rng.integers(1e9),\n",
    "            S_churn=S_churn,\n",
    "            S_min=S_min,\n",
    "            S_max=S_max,\n",
    "            S_noise=S_noise,\n",
    "            schedule=schedule\n",
    "        )\n",
    "        single_sample_3d = single_sample.reshape((1, X.shape[0], 1))\n",
    "\n",
    "        if y_scaler:\n",
    "            single_sample_3d[0] = y_scaler.inverse_transform(single_sample_3d[0])\n",
    "\n",
    "        all_samples.append(single_sample_3d)\n",
    "        sample_idx += 1\n",
    "\n",
    "    if len(all_samples) == 0:\n",
    "        return np.empty((0, X.shape[0], 1))\n",
    "\n",
    "    return np.concatenate(all_samples, axis=0)\n",
    "\n",
    "def sample_treeffuser_dpm_solver2_stoch(\n",
    "    X,\n",
    "    sde_dict,\n",
    "    models,\n",
    "    x_scaler,\n",
    "    y_scaler,\n",
    "    time_budget=10.0,\n",
    "    n_steps=5,\n",
    "    seed=None,\n",
    "    S_churn=10.0, S_min=0.05, S_max=50.0, S_noise=1.0,\n",
    "    schedule = 'exponential'\n",
    "):\n",
    "    start_t = time.time()\n",
    "    rng = np.random.default_rng(seed)\n",
    "    all_samples = []\n",
    "\n",
    "    def score_fn(y_local, sigma_local):\n",
    "        return predict_noise_lightgbm_batched(models, sde_dict, y_local, X, sigma_local)\n",
    "\n",
    "    while True:\n",
    "        if time.time() - start_t >= time_budget:\n",
    "            break\n",
    "\n",
    "        shape = (1, X.shape[0], 1)\n",
    "        y0 = sde_dict[\"sample_prior\"](shape, seed=rng.integers(1e9)) \n",
    "        y0_flat = y0.reshape(-1, 1)\n",
    "\n",
    "        single_sample = sdeint(\n",
    "            sde_dict,\n",
    "            y0_flat,\n",
    "            t0 = 1.0,\n",
    "            t1 = 0.0,\n",
    "            method = \"dpm_solver_2_stoch\",\n",
    "            n_steps=n_steps,\n",
    "            score_fn=score_fn,\n",
    "            seed=rng.integers(1e9),\n",
    "            S_churn=S_churn,\n",
    "            S_min=S_min,\n",
    "            S_max=S_max,\n",
    "            S_noise=S_noise,\n",
    "            schedule=schedule\n",
    "        )\n",
    "        single_sample_3d = single_sample.reshape((1, X.shape[0], 1))\n",
    "\n",
    "        if y_scaler:\n",
    "            single_sample_3d[0] = y_scaler.inverse_transform(single_sample_3d[0])\n",
    "\n",
    "        all_samples.append(single_sample_3d)\n",
    "\n",
    "    if len(all_samples)==0:\n",
    "        return np.empty((0, X.shape[0], 1))\n",
    "    return np.concatenate(all_samples, axis=0)\n",
    "\n",
    "\n",
    "def sample_treeffuser_dpm_solver3_stoch(\n",
    "    X,\n",
    "    sde_dict,\n",
    "    models,\n",
    "    x_scaler,\n",
    "    y_scaler,\n",
    "    time_budget=10.0,\n",
    "    n_steps=5,\n",
    "    seed=None,\n",
    "    S_churn=10.0, \n",
    "    S_min=0.05, \n",
    "    S_max=50.0, \n",
    "    S_noise=1.0,\n",
    "    schedule=\"karras_linear\"\n",
    "):\n",
    "  \n",
    "    start_t = time.time()\n",
    "    rng = np.random.default_rng(seed)\n",
    "    all_samples = []\n",
    "\n",
    "    def noise_fn(y_local, sigma_local):\n",
    "        return predict_noise_lightgbm_batched(models, sde_dict, y_local, X, sigma_local)\n",
    "\n",
    "    while True:\n",
    "        if time.time() - start_t >= time_budget:\n",
    "            break\n",
    "\n",
    "        shape = (1, X.shape[0], 1)\n",
    "        y0 = sde_dict[\"sample_prior\"](shape, seed=rng.integers(1e9))\n",
    "        y0_flat = y0.reshape(-1, 1)\n",
    "\n",
    "        single_sample = vesde_reverse_finalDPM_solver3_stoch(\n",
    "            sde_dict,\n",
    "            y0_flat,\n",
    "            original_t0=1.0,\n",
    "            rev_total=1.0,\n",
    "            n_steps=n_steps,\n",
    "            rng=rng,\n",
    "            noise_fn=noise_fn,\n",
    "            S_churn=S_churn,\n",
    "            S_min=S_min,\n",
    "            S_max=S_max,\n",
    "            S_noise=S_noise,\n",
    "            schedule=schedule\n",
    "        )\n",
    "\n",
    "        single_sample_3d = single_sample.reshape((1, X.shape[0], 1))\n",
    "        if y_scaler:\n",
    "            single_sample_3d[0] = y_scaler.inverse_transform(single_sample_3d[0])\n",
    "\n",
    "        all_samples.append(single_sample_3d)\n",
    "\n",
    "    if len(all_samples) == 0:\n",
    "        return np.empty((0, X.shape[0], 1))\n",
    "    return np.concatenate(all_samples, axis=0)\n",
    "\n",
    "\n",
    "def sample_treeffuser_expint(\n",
    "    X,\n",
    "    sde_dict,\n",
    "    models,\n",
    "    x_scaler,\n",
    "    y_scaler,\n",
    "    time_budget=10.0,\n",
    "    n_steps=20,\n",
    "    seed=None\n",
    "):\n",
    "   \n",
    "    start_t = time.time()\n",
    "    rng = np.random.default_rng(seed)\n",
    "    all_samples = []\n",
    "\n",
    "    def noise_fn(y_local, t_local):\n",
    "        return predict_noise_lightgbm_batched(models, sde_dict, y_local, X, t_local)\n",
    "\n",
    "    while True:\n",
    "        if time.time() - start_t >= time_budget:\n",
    "            break\n",
    "\n",
    "        shape = (1, X.shape[0], 1)\n",
    "        y0 = sde_dict[\"sample_prior\"](shape, seed=None)\n",
    "        y0_flat = y0.reshape(shape[0]*shape[1], shape[2])\n",
    "\n",
    "        single_sample = vesde_reverse_exponential_integration_polynomial_noise(\n",
    "            sde_dict,\n",
    "            y0_flat,\n",
    "            original_t0=1.0,\n",
    "            rev_total=1.0,\n",
    "            n_steps=n_steps,\n",
    "            noise_fn=noise_fn,\n",
    "            poly_order=3,\n",
    "            seed=rng.integers(1e9)\n",
    "        )\n",
    "        single_sample_3d = single_sample.reshape((1, X.shape[0], 1))\n",
    "        if y_scaler:\n",
    "            single_sample_3d[0] = y_scaler.inverse_transform(single_sample_3d[0])\n",
    "        all_samples.append(single_sample_3d)\n",
    "\n",
    "    if len(all_samples)==0:\n",
    "        return np.empty((0, X.shape[0], 1))\n",
    "\n",
    "    return np.concatenate(all_samples, axis=0)\n",
    "\n",
    "\n",
    "def sample_treeffuser_stochsampler(\n",
    "    X,\n",
    "    sde_dict,\n",
    "    models,\n",
    "    x_scaler,\n",
    "    y_scaler,\n",
    "    time_budget=10.0,\n",
    "    n_steps=20,\n",
    "    seed=None,\n",
    "    S_churn=10.0,\n",
    "    S_min=0.05,\n",
    "    S_max=50.0,\n",
    "    S_noise=1.0\n",
    "):\n",
    "    \n",
    "    start_t = time.time()\n",
    "    rng = np.random.default_rng(seed)\n",
    "    all_samples = []\n",
    "\n",
    "    def noise_fn(y_local, t_local):\n",
    "        return predict_noise_lightgbm_batched(models, sde_dict, y_local, X, t_local)\n",
    "\n",
    "    while True:\n",
    "        if time.time() - start_t >= time_budget:\n",
    "            break\n",
    "\n",
    "        shape = (1, X.shape[0], 1)\n",
    "        y0 = sde_dict[\"sample_prior\"](shape, seed=None)\n",
    "        y0_flat = y0.reshape(shape[0]*shape[1], shape[2])\n",
    "\n",
    "        single_sample = vesde_reverse_stochsampler_once(\n",
    "            sde_dict,\n",
    "            y0,\n",
    "            n_steps=n_steps,\n",
    "            noise_fn=noise_fn,\n",
    "            S_churn=S_churn,\n",
    "            S_min=S_min,\n",
    "            S_max=S_max,\n",
    "            S_noise=S_noise\n",
    "        )\n",
    "        single_sample_3d = single_sample.reshape((1, X.shape[0], 1))\n",
    "        if y_scaler:\n",
    "            single_sample_3d[0] = y_scaler.inverse_transform(single_sample_3d[0])\n",
    "        all_samples.append(single_sample_3d)\n",
    "\n",
    "    if len(all_samples)==0:\n",
    "        return np.empty((0, X.shape[0], 1))\n",
    "\n",
    "    return np.concatenate(all_samples, axis=0)\n",
    "\n",
    "import time\n",
    "\n",
    "def pndm_vpsde_integration(\n",
    "    sde_dict,\n",
    "    y0,\n",
    "    n_steps,\n",
    "    noise_fn,  \n",
    "    rng,\n",
    "    rk4_warmup=4\n",
    "):\n",
    "    x = y0.reshape(-1, 1).copy()\n",
    "    hyper_min = sde_dict[\"hyperparam_min\"]\n",
    "    hyper_max = sde_dict[\"hyperparam_max\"]\n",
    "\n",
    "    dt = 1.0\n",
    "    t_now = float(n_steps)\n",
    "    \n",
    "    e_buffer = {}\n",
    "    \n",
    "    def do_step(x_in, noise_in, t_in, t_out):\n",
    "        return _pndm_step_vpsde(\n",
    "            x_in, noise_in, t_in / n_steps, t_out / n_steps, hyper_min, hyper_max\n",
    "        )\n",
    "    \n",
    "    warmup_steps = min(rk4_warmup, n_steps)\n",
    "\n",
    "    for _ in range(warmup_steps):\n",
    "        if t_now <= 0:\n",
    "            break\n",
    "        \n",
    "        e1 = noise_fn(x, t_now / n_steps)\n",
    "        half_t = t_now - 0.5 * dt\n",
    "        x1 = do_step(x, e1, t_now, half_t)\n",
    "\n",
    "        e2 = noise_fn(x1, half_t / n_steps)\n",
    "        x2 = do_step(x, e2, t_now, half_t)\n",
    "\n",
    "        e3 = noise_fn(x2, half_t / n_steps)\n",
    "        x3 = do_step(x, e3, t_now, t_now - dt)\n",
    "\n",
    "        e4 = noise_fn(x3, (t_now - dt) / n_steps)\n",
    "\n",
    "        e_prime = (e1 + 2 * e2 + 2 * e3 + e4) / 6.0\n",
    "\n",
    "        x_new = do_step(x, e_prime, t_now, t_now - dt)\n",
    "        x = x_new\n",
    "\n",
    "        t_now -= dt\n",
    "        e_buffer[t_now] = e_prime\n",
    "    \n",
    "    remain = int(t_now)\n",
    "    while remain > 0:\n",
    "        e_t = noise_fn(x, t_now / n_steps)\n",
    "        e_tm1 = e_buffer.get(t_now - 1, e_t)\n",
    "        e_tm2 = e_buffer.get(t_now - 2, e_t)\n",
    "        e_tm3 = e_buffer.get(t_now - 3, e_t)\n",
    "\n",
    "        e_prime = (1.0 / 24.0) * (\n",
    "            55.0 * e_t - 59.0 * e_tm1 + 37.0 * e_tm2 - 9.0 * e_tm3\n",
    "        )\n",
    "\n",
    "        x_new = do_step(x, e_prime, t_now, t_now - dt)\n",
    "        x = x_new\n",
    "        \n",
    "        t_now -= dt\n",
    "        remain -= 1\n",
    "        e_buffer[t_now] = e_prime\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def sample_treeffuser_pndm(\n",
    "    X,\n",
    "    sde_dict,\n",
    "    models,\n",
    "    x_scaler,\n",
    "    y_scaler,\n",
    "    time_budget=10.0,\n",
    "    n_steps=20,\n",
    "    seed=None,\n",
    "    rk4_warmup=4\n",
    "):\n",
    "   \n",
    "    start_t = time.time()\n",
    "    rng = np.random.default_rng(seed)\n",
    "    all_samples = []\n",
    "\n",
    "    def noise_fn(y_local, t_local):\n",
    "        if np.isscalar(t_local):  \n",
    "            t_local = np.array([[t_local]])\n",
    "        elif t_local.ndim == 1:  \n",
    "            t_local = t_local.reshape(-1, 1)\n",
    "        \n",
    "        return predict_noise_lightgbm_batched(models, sde_dict, y_local, X, t_local)\n",
    "\n",
    "    while True:\n",
    "        if time.time() - start_t >= time_budget:\n",
    "            break\n",
    "\n",
    "        shape = (1, X.shape[0], 1)\n",
    "        y0 = sde_dict[\"sample_prior\"](shape, seed=None)\n",
    "        y0_flat = y0.reshape(-1, 1)\n",
    "\n",
    "        single_sample = pndm_vpsde_integration(\n",
    "            sde_dict,\n",
    "            y0_flat,\n",
    "            n_steps=n_steps,\n",
    "            noise_fn=noise_fn,\n",
    "            rng=rng,\n",
    "            rk4_warmup=rk4_warmup\n",
    "        )\n",
    "        single_sample_3d = single_sample.reshape((1, X.shape[0], 1))\n",
    "        \n",
    "        if y_scaler is not None:\n",
    "            single_sample_3d[0] = y_scaler.inverse_transform(single_sample_3d[0])\n",
    "            \n",
    "        all_samples.append(single_sample_3d)\n",
    "\n",
    "    if len(all_samples) == 0:\n",
    "        return np.empty((0, X.shape[0], 1))\n",
    "\n",
    "    return np.concatenate(all_samples, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "def sample_treeffuser_ou_exponential(\n",
    "    X,\n",
    "    sde_dict,\n",
    "    models,\n",
    "    x_scaler,\n",
    "    y_scaler,\n",
    "    time_budget=10.0,\n",
    "    n_steps=20,\n",
    "    gamma=0.1,\n",
    "    corrector_steps=1,\n",
    "    dt_corr=0.001,\n",
    "    seed=None\n",
    "):\n",
    "    \n",
    "    start_t = time.time()\n",
    "    rng = np.random.default_rng(seed)\n",
    "    all_samples = []\n",
    "\n",
    "    def score_fn(y_local, t_local):\n",
    "        return predict_score_lightgbm(models, sde_dict, y_local, X, t_local)\n",
    "\n",
    "    while True:\n",
    "        if time.time() - start_t >= time_budget:\n",
    "            break\n",
    "\n",
    "        shape = (1, X.shape[0], 1)\n",
    "        y0 = sde_dict[\"sample_prior\"](shape, seed=None)\n",
    "        y0_flat = y0.reshape(-1, 1)\n",
    "\n",
    "        single_sample = ou_reverse_integration_once(\n",
    "            sde_dict,\n",
    "            y0_flat,\n",
    "            original_t0=1.0,\n",
    "            rev_total=1.0,\n",
    "            n_steps=n_steps,\n",
    "            rng=rng,\n",
    "            score_fn=score_fn,\n",
    "            gamma=gamma,\n",
    "            corrector_steps=corrector_steps,\n",
    "            dt_corr=dt_corr,\n",
    "            predictor_method=\"exponential\"\n",
    "        )\n",
    "        single_sample_3d = single_sample.reshape((1, X.shape[0], 1))\n",
    "        if y_scaler:\n",
    "            single_sample_3d[0] = y_scaler.inverse_transform(single_sample_3d[0])\n",
    "        all_samples.append(single_sample_3d)\n",
    "\n",
    "    if len(all_samples)==0:\n",
    "        return np.empty((0, X.shape[0], 1))\n",
    "\n",
    "    return np.concatenate(all_samples, axis=0)\n",
    "\n",
    "def sample_treeffuser_ou_midpoint(\n",
    "    X,\n",
    "    sde_dict,\n",
    "    models,\n",
    "    x_scaler,\n",
    "    y_scaler,\n",
    "    time_budget=10.0,\n",
    "    n_steps=20,\n",
    "    gamma=0.1,\n",
    "    corrector_steps=1,\n",
    "    dt_corr=0.001,\n",
    "    seed=None\n",
    "):\n",
    "    \n",
    "    start_t = time.time()\n",
    "    rng = np.random.default_rng(seed)\n",
    "    all_samples = []\n",
    "\n",
    "    def score_fn(y_local, t_local):\n",
    "        return predict_score_lightgbm(models, sde_dict, y_local, X, t_local)\n",
    "\n",
    "    while True:\n",
    "        if time.time() - start_t >= time_budget:\n",
    "            break\n",
    "\n",
    "        shape = (1, X.shape[0], 1)\n",
    "        y0 = sde_dict[\"sample_prior\"](shape, seed=None)\n",
    "        y0_flat = y0.reshape(-1, 1)\n",
    "\n",
    "        single_sample = ou_reverse_integration_once(\n",
    "            sde_dict,\n",
    "            y0_flat,\n",
    "            original_t0=1.0,\n",
    "            rev_total=1.0,\n",
    "            n_steps=n_steps,\n",
    "            rng=rng,\n",
    "            score_fn=score_fn,\n",
    "            gamma=gamma,\n",
    "            corrector_steps=corrector_steps,\n",
    "            dt_corr=dt_corr,\n",
    "            predictor_method=\"random_midpoint\"\n",
    "        )\n",
    "        single_sample_3d = single_sample.reshape((1, X.shape[0], 1))\n",
    "        if y_scaler:\n",
    "            single_sample_3d[0] = y_scaler.inverse_transform(single_sample_3d[0])\n",
    "        all_samples.append(single_sample_3d)\n",
    "\n",
    "    if len(all_samples)==0:\n",
    "        return np.empty((0, X.shape[0], 1))\n",
    "\n",
    "    return np.concatenate(all_samples, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84b0ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "################################################################################\n",
    "# 1) Import the data and basic transformations\n",
    "################################################################################\n",
    "calendar_df = pd.read_csv(\"calendar.csv\")\n",
    "sales_train_df = pd.read_csv(\"sales_train_validation.csv\")\n",
    "sell_prices_df = pd.read_csv(\"sell_prices.csv\")\n",
    "\n",
    "calendar_df[\"date\"] = pd.to_datetime(calendar_df[\"date\"])\n",
    "calendar_df[\"day\"] = calendar_df[\"date\"].dt.day\n",
    "calendar_df[\"month\"] = calendar_df[\"date\"].dt.month\n",
    "calendar_df[\"year\"] = calendar_df[\"date\"].dt.year\n",
    "\n",
    "def convert_sales_data_from_wide_to_long(sales_df_wide):\n",
    "    index_vars = [\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n",
    "    sales_df_long = pd.wide_to_long(\n",
    "        sales_df_wide.iloc[:3000, 1:], \n",
    "        i=index_vars,\n",
    "        j=\"day\",\n",
    "        stubnames=[\"d\"],\n",
    "        sep=\"_\",\n",
    "    ).reset_index()\n",
    "    sales_df_long = sales_df_long.rename(columns={\"d\": \"sales\", \"day\": \"d\"})\n",
    "    sales_df_long[\"d\"] = \"d_\" + sales_df_long[\"d\"].astype(\"str\")\n",
    "    return sales_df_long\n",
    "\n",
    "sales_train_df_long = convert_sales_data_from_wide_to_long(sales_train_df)\n",
    "print(\"Rows after converting to long format:\", sales_train_df_long.shape[0])\n",
    "\n",
    "sales_train_df_long[\"day_number\"] = sales_train_df_long[\"d\"].str.extract(\"(\\d+)\").astype(int)\n",
    "data = sales_train_df_long[sales_train_df_long[\"day_number\"] <= 365].copy()\n",
    "print(\"Rows after filtering for day_number <= 365:\", data.shape[0])\n",
    "\n",
    "data_index_vars = [\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n",
    "data.sort_values(data_index_vars + [\"day_number\"], inplace=True)\n",
    "\n",
    "n_lags = 30\n",
    "for lag in range(1, n_lags + 1):\n",
    "    data[f\"sales_lag_{lag}\"] = data.groupby(data_index_vars)[\"sales\"].shift(lag)\n",
    "\n",
    "print(\"Rows after creating lag features (before dropping NaNs):\", data.shape[0])\n",
    "data = data.dropna()\n",
    "print(\"Rows after dropping NaNs:\", data.shape[0])\n",
    "\n",
    "data = data.merge(calendar_df).merge(sell_prices_df)\n",
    "print(\"Rows after merging with calendar and prices:\", data.shape[0])\n",
    "\n",
    "categorical_columns = [\n",
    "    \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\",\n",
    "    \"d\", \"wm_yr_wk\", \"weekday\", \"event_name_1\", \"event_type_1\",\n",
    "    \"event_name_2\", \"event_type_2\", \"snap_CA\", \"snap_TX\", \"snap_WI\"\n",
    "]\n",
    "for c in categorical_columns:\n",
    "    if c in data.columns:\n",
    "        data[c] = data[c].astype(\"category\")\n",
    "\n",
    "train_cutoff = 300\n",
    "is_train = data[\"day_number\"] <= train_cutoff\n",
    "train_data = data[is_train].copy()\n",
    "test_data = data[~is_train].copy()\n",
    "\n",
    "print(\"Rows in train_data:\", train_data.shape[0])\n",
    "print(\"Rows in test_data:\", test_data.shape[0])\n",
    "\n",
    "y_name = \"sales\"\n",
    "x_names = [\n",
    "    col for col in data.columns\n",
    "    if col not in [y_name, \"day_number\", \"date\"] \n",
    "]\n",
    "\n",
    "X_train = train_data[x_names]\n",
    "y_train = train_data[y_name].values.reshape(-1, 1)\n",
    "X_test = test_data[x_names]\n",
    "y_test = test_data[y_name].values.reshape(-1, 1)\n",
    "\n",
    "print(\"Final rows in X_train:\", X_train.shape[0])\n",
    "print(\"Final rows in X_test:\", X_test.shape[0])\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "x_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "\n",
    "X_train_sc = x_scaler.fit_transform(X_train.select_dtypes(exclude=[\"category\"]))\n",
    "X_test_sc = x_scaler.transform(X_test.select_dtypes(exclude=[\"category\"]))\n",
    "\n",
    "Y_train_sc = y_scaler.fit_transform(y_train)\n",
    "Y_test_sc = y_scaler.transform(y_test)\n",
    "\n",
    "# %%\n",
    "sde_dict_vesde = create_vesde(0.1, 1.0) \n",
    "models_vesde = fit_treeffuser(\n",
    "    X_train_sc, \n",
    "    Y_train_sc, \n",
    "    sde_dict_vesde,\n",
    "    n_repeats=10,\n",
    "    eval_percent=0.1,\n",
    "    seed=42,\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=8\n",
    ")\n",
    "\n",
    "sde_dict_vpsde = create_vpsde(hyperparam_min=0.1, hyperparam_max=1.0)\n",
    "models_vpsde = fit_treeffuser(\n",
    "    X_train_sc,\n",
    "    Y_train_sc,\n",
    "    sde_dict_vpsde,\n",
    "    n_repeats=10,\n",
    "    eval_percent=0.1,\n",
    "    seed=42,\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=8\n",
    ")\n",
    "\n",
    "sde_dict_ou = create_ou_sde()\n",
    "models_ou = fit_treeffuser(\n",
    "    X_train_sc,\n",
    "    Y_train_sc,\n",
    "    sde_dict_ou,\n",
    "    n_repeats=10,\n",
    "    eval_percent=0.1,\n",
    "    seed=42,\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=8\n",
    ")\n",
    "\n",
    "\n",
    "# %%\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import properscoring as ps\n",
    "from typing import List, Dict, Callable\n",
    "import time\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "def newsvendor_utility(y_true, quantity_ordered, prices, stocking_cost):\n",
    "    \n",
    "    return prices * np.minimum(y_true, quantity_ordered) - stocking_cost * quantity_ordered\n",
    "\n",
    "def newsvendor_optimal_quantity(y_samples, prices, stocking_cost):\n",
    "  \n",
    "    alpha = (prices - stocking_cost) / prices\n",
    "    alpha = np.maximum(alpha, 0.0)\n",
    "\n",
    "    res = []\n",
    "    for i in range(y_samples.shape[1]):\n",
    "        q_i = np.quantile(y_samples[:, i], alpha[i])\n",
    "        res.append(q_i)\n",
    "    return np.array(res)\n",
    "\n",
    "def calculate_crps(y_true: np.ndarray, samples: np.ndarray) -> float:\n",
    "    \n",
    "    y_true = y_true.ravel()\n",
    "    \n",
    "    if samples.shape[0] < samples.shape[1]:  \n",
    "        samples = samples.T  \n",
    "    \n",
    "    crps_values = []\n",
    "    for i in range(len(y_true)):\n",
    "        crps_values.append(ps.crps_ensemble(y_true[i], samples[i]))\n",
    "    \n",
    "    return np.mean(crps_values)\n",
    "\n",
    "def evaluate_samples(y_true: np.ndarray, samples: np.ndarray, prices: np.ndarray, stocking_cost: np.ndarray) -> Dict[str, float]:\n",
    "   \n",
    "    y_true = y_true.ravel()\n",
    "    samples_2d = samples.squeeze(axis=2) if samples.ndim == 3 else samples\n",
    "    \n",
    "    print(\"In evaluate_samples:\")\n",
    "    print(f\"y_true shape after ravel: {y_true.shape}\")\n",
    "    print(f\"samples_2d shape: {samples_2d.shape}\")\n",
    "    \n",
    "    y_pred_mean = np.mean(samples_2d, axis=0)\n",
    "    optimal_q = newsvendor_optimal_quantity(samples_2d, prices, stocking_cost)\n",
    "    profit_arr = newsvendor_utility(y_true, optimal_q, prices, stocking_cost)\n",
    "    total_profit = profit_arr.sum()\n",
    "    \n",
    "    return {\n",
    "        'R2': r2_score(y_true, y_pred_mean),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred_mean)),\n",
    "        'CRPS': calculate_crps(y_true, samples_2d),\n",
    "        'Profit': total_profit\n",
    "    }\n",
    "\n",
    "sampling_methods = {\n",
    "    'DPM': (sample_treeffuser_dpm, 'vesde'),\n",
    "    'Euler': (sample_treeffuser, 'vesde'),\n",
    "    'PNDM': (sample_treeffuser_pndm, 'vpsde'),\n",
    "    'ExpInt': (sample_treeffuser_expint, 'vesde'),\n",
    "    'UniPC': (sample_treeffuser_unipc, 'vpsde'),\n",
    "    'StochSampler': (sample_treeffuser_stochsampler, 'vesde'),\n",
    "    'DPM_Stoch': (sample_treeffuser_dpm_stoch, 'vesde'),  \n",
    "    'OU-Exp': (sample_treeffuser_ou_exponential, 'ou'),\n",
    "    'OU-Mid': (sample_treeffuser_ou_midpoint, 'ou'),\n",
    "    'DPM_2_stoch': (sample_treeffuser_dpm_solver2_stoch, 'vesde'),\n",
    "    'DPM_3_stoch': (sample_treeffuser_dpm_solver3_stoch, 'vesde')\n",
    "}\n",
    "\n",
    "time_budgets = [5,10,15,20,25,30]\n",
    "\n",
    "results = []\n",
    "\n",
    "saved_samples = {}  \n",
    "\n",
    "prices = X_test[\"sell_price\"].fillna(1.0).values\n",
    "profit_margin = 0.5\n",
    "stocking_cost = prices / (1 + profit_margin)\n",
    "\n",
    "for budget in time_budgets:\n",
    "    print(f\"\\nRunning experiments for {budget} seconds budget...\")\n",
    "    \n",
    "    for method_name, (method_fn, sde_type) in sampling_methods.items():\n",
    "        print(f\"  Running {method_name}...\")\n",
    "        \n",
    "        if sde_type == 'vpsde':\n",
    "            sde_dict = sde_dict_vpsde\n",
    "            models = models_vpsde\n",
    "        elif sde_type == 'vesde':\n",
    "            sde_dict = sde_dict_vesde\n",
    "            models = models_vesde\n",
    "        else:  \n",
    "            sde_dict = sde_dict_ou\n",
    "            models = models_ou\n",
    "            \n",
    "        if method_name == 'Euler':\n",
    "            n_steps = 15\n",
    "        elif method_name == 'OU-Mid':\n",
    "            n_steps = 5\n",
    "        else:\n",
    "            n_steps = 3\n",
    "                \n",
    "        \n",
    "        samples = method_fn(\n",
    "            X=X_test_sc,\n",
    "            sde_dict=sde_dict,\n",
    "            models=models,\n",
    "            x_scaler=x_scaler,\n",
    "            y_scaler=y_scaler,\n",
    "            time_budget=budget,\n",
    "            n_steps=n_steps,\n",
    "            seed=42\n",
    "        )\n",
    "            \n",
    "    \n",
    "        saved_samples[(method_name, budget)] = samples\n",
    "\n",
    "        metrics = evaluate_samples(y_test, samples, prices, stocking_cost)\n",
    "    \n",
    "        results.append({\n",
    "            'Method': method_name,\n",
    "            'Time_Budget': budget,\n",
    "            **metrics\n",
    "        })\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\nResults Summary:\")\n",
    "summary_table = results_df.pivot_table(\n",
    "    index='Method',\n",
    "    columns='Time_Budget',\n",
    "    values=['R2', 'RMSE', 'CRPS', 'Profit']\n",
    ")\n",
    "print(summary_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
